<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>PFL-SRDP</title>
      <link href="/2025/08/17/PFL-SRDP/"/>
      <url>/2025/08/17/PFL-SRDP/</url>
      
        <content type="html"><![CDATA[<p><code>最近准备申请一个大创的项目，研究围绕PFL展开，这篇blog用于记录其中的一些思路</code></p><h2 id="PFL相关的基本问题"><a href="#PFL相关的基本问题" class="headerlink" title="PFL相关的基本问题"></a>PFL相关的基本问题</h2><h3 id="PFL的基本类型"><a href="#PFL的基本类型" class="headerlink" title="PFL的基本类型"></a>PFL的基本类型</h3><p>(1) 学习单一全局模型并进行微调的方法<br>这些方法首先学习一个全局共享模型，然后在每个客户端上进行本地微调。</p><h4 id="例子：FedAvg-本地微调"><a href="#例子：FedAvg-本地微调" class="headerlink" title="例子：FedAvg + 本地微调"></a>例子：FedAvg + 本地微调</h4><p>首先，使用联邦平均（Federated Averaging，FedAvg）算法进行全局模型的训练。FedAvg通过在每个客户端本地训练模型，然后将本地模型参数上传到服务器进行平均化，形成全局模型。</p><p>步骤：</p><ol><li>每个客户端在本地数据上训练模型若干轮。</li><li>将本地模型参数上传到服务器。</li><li>服务器对所有客户端上传的模型参数进行平均化，得到新的全局模型。</li><li>将新的全局模型分发给每个客户端。</li><li>每个客户端在全局模型基础上进行本地微调，使用本地数据继续训练若干轮。</li></ol><p><code>第一类要运用Nash-bargaining game来进行聚合的话，基本上与FL的差异不大，因为都是考虑对客户端训练出的参数在服务器上进行聚合。</code></p><p>(2) 学习额外个性化模型的方法<br>这些方法在全局模型的基础上，为每个客户端学习一个额外的个性化模型。</p><h4 id="例子：FedPer"><a href="#例子：FedPer" class="headerlink" title="例子：FedPer"></a>例子：FedPer</h4><p>FedPer（Federated Personalization）方法提出在全局模型的基础上，为每个客户端学习一个个性化的模型头（或最后一层）。</p><p>步骤：</p><ol><li>使用FedAvg算法训练一个共享的全局模型，但仅共享前几层的参数。</li><li>每个客户端保持自己的个性化模型头，该模型头仅在本地数据上训练。</li><li>在每轮通信中，仅共享和更新全局模型的共享层参数，不包括个性化头部参数。</li></ol><p>(3) 通过个性化（本地）聚合学习本地模型的方法<br>这些方法通过个性化聚合来进一步捕捉个性化需求，为每个客户端生成特定的模型。</p><h4 id="例子：FedProx"><a href="#例子：FedProx" class="headerlink" title="例子：FedProx"></a>例子：FedProx</h4><p>FedProx（Federated Proximal）方法通过在本地训练过程中添加一个正则项，来限制本地模型偏离全局模型过多，从而实现个性化。</p><p>步骤：</p><ol><li>每个客户端在本地数据上训练模型时，在损失函数中添加一个正则项，限制模型参数与全局模型参数的差异。</li><li>正则项形式为：loss + μ&#x2F;2 * ||w - w_global||^2，其中w是本地模型参数，w_global是全局模型参数，μ是正则化系数。</li><li>通过这种方法，使得模型在本地数据上训练时，仍然保持一定的全局模型特性。</li></ol><h2 id="设想的算法"><a href="#设想的算法" class="headerlink" title="设想的算法"></a>设想的算法</h2><p><img src="/../_images/MyAlgorithm.jpg" alt="MyAlgorithm"></p><h2 id="研究目的"><a href="#研究目的" class="headerlink" title="研究目的"></a>研究目的</h2><ol><li>有关联邦学习的内容</li><li>为什么需要个性化联邦学习（非独立同分布…）</li><li>个性化联邦学习的常见方法（各种方法对应的参考文献）</li><li>关于NashBargainingGame的一些基本情况，其与相关学习算法的结合</li><li>最终我们的研究目的</li></ol><h2 id="研究内容"><a href="#研究内容" class="headerlink" title="研究内容"></a>研究内容</h2><ol><li>实现基本的FL算法框架（FedAvg）</li><li>考虑第三类PFL，引入Nash-bargaining Game设置个性化聚合方法</li><li>考虑引入权重α，进一步改进算法</li><li>PFL实验评估（通过权威论文，明确PFL算法量化评价指标）</li></ol><h2 id="一些参考文献"><a href="#一些参考文献" class="headerlink" title="一些参考文献"></a>一些参考文献</h2><p>Advances and Open Problems in Federated Learning 联邦学习领域的权威综述，其中把个性化联邦学习作为联邦学习下一个分支，有所提及。</p><h3 id="琐碎的思路"><a href="#琐碎的思路" class="headerlink" title="琐碎的思路"></a>琐碎的思路</h3><p>如果我们不重点考虑隐私问题，或许可以采用数据中心分布式学习的背景框架。这种背景下，我们是要在一个大而扁平的数据集上训练模型，每个客户端是单个集群或数据中心中的计算结点。数据分配的特点是任何客户端都可以读取数据集任意部分。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习（联邦学习） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PFL2</title>
      <link href="/2025/08/06/PFL2/"/>
      <url>/2025/08/06/PFL2/</url>
      
        <content type="html"><![CDATA[<p><code>这篇blog用来记录我读的第二篇有关联邦学习的文献，其中也使用了nash bargaining game</code></p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="Representation-Collapse-Entanglement"><a href="#Representation-Collapse-Entanglement" class="headerlink" title="Representation Collapse Entanglement"></a>Representation Collapse Entanglement</h3><p>表示崩塌纠缠：这是指在联邦无监督学习（FUSL）过程中，由于某个本地模型的表示崩塌（即该模型的特征表示不再具有区分度），会影响到全局模型和其他本地模型的表示能力。这种崩塌会导致整个系统的表示能力下降，使得模型在处理非独立同分布（non-IID）数据时效果不佳。</p><h3 id="Flexible-Uniform-Regularizer"><a href="#Flexible-Uniform-Regularizer" class="headerlink" title="Flexible Uniform Regularizer"></a>Flexible Uniform Regularizer</h3><p>FUR：灵活均匀正则化器，这是FedU2方法中的一个组件，旨在每个客户端上避免表示崩塌。通过均匀分散样本，使得模型的表示不集中在特定区域，从而保持特征空间的多样性。通过强制模型学习到更加均匀分布的特征，FUR可以防止表示崩塌，提高模型的泛化能力。</p><h3 id="Efficient-Unified-Aggregator"><a href="#Efficient-Unified-Aggregator" class="headerlink" title="Efficient Unified Aggregator"></a>Efficient Unified Aggregator</h3><p>EUA：高效统一聚合器，这是FedU2方法中的另一个组件，部署在服务器端，用于在聚合客户端模型时促进统一的表示空间。该聚合器通过约束客户端模型的更新，确保各客户端模型在特征空间上的一致性。</p><h3 id="Inactivated-neurons"><a href="#Inactivated-neurons" class="headerlink" title="Inactivated neurons"></a>Inactivated neurons</h3><p>非活化神经元：：在人工神经网络（如深度学习模型）中，某些神经元可能在特定输入或训练阶段不被激活。激活函数（如ReLU、Sigmoid等）会根据输入值决定某个神经元是否被激活。</p><h3 id="Unbalanced-Optimal-Transport-Divergence"><a href="#Unbalanced-Optimal-Transport-Divergence" class="headerlink" title="Unbalanced Optimal Transport Divergence"></a>Unbalanced Optimal Transport Divergence</h3><p>非平衡最优传输散度：是一种测量分布之间差异的方法，特别适用于处理具有不同质量或总质量不守恒的分布。它是传统最优传输（Optimal Transport, OT）理论的扩展，传统最优传输通常假设两个分布具有相同的总质量，这样可以通过寻找最优运输计划来最小化从一个分布到另一个分布的“运输成本”。非平衡最优传输散度放宽了传统最优传输的总质量守恒假设，允许处理不同质量或存在质量损失的分布。它引入了一个正则项来惩罚质量的创建和销毁，从而可以在更广泛的应用场景中使用。</p><p>在FUR中的应用：最小化客户端数据与均匀随机样本（如来自同一球形高斯分布的样本）之间的非平衡最优传输散度。FUR强制每个客户端的数据分布更接近一个统一的参考分布，从而避免了表示崩塌并促进更均匀的特征表示。</p><p>注意：在最小化非平衡最优传输散度的过程中，通常并不会直接删除客户端的数据点，而是通过调整模型的训练过程来使数据的特征表示与统一的参考分布对齐。这是通过优化目标函数和引入正则化项来实现的，而不是通过直接修改原始数据。</p><p>具体实现方法：在训练过程中，优化目标函数时会加入非平衡最优传输散度作为正则化项。这一项会惩罚客户端数据分布与参考分布之间的差异。</p><p>公式：假设 $\mathcal{L}$ 是原始损失函数，$\text{UOT}(P, Q)$ 是非平衡最优传输散度项，那么新的优化目标可以表示为：<br>$\mathcal{L}_{\text{total}} &#x3D; \mathcal{L} + \lambda \cdot \text{UOT}(P, Q)$</p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>divergent 不同的，分歧的；（级数）发散的</p><p>suppress 抑制；封锁；压制</p><p>decorrelate 去相关</p><p>discrepant 有差异的；矛盾的</p><p>threshold 阈；门槛</p><p>deviation 偏离</p><p>dual 双重的</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习（联邦学习） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PFL</title>
      <link href="/2025/07/29/PFL/"/>
      <url>/2025/07/29/PFL/</url>
      
        <content type="html"><![CDATA[<p><code>这篇blog用于记录我阅读论文《 Improve global generalization for personalized federated learning within a Stackelberg game》过程中学习到的一些基础知识。</code></p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="PFL"><a href="#PFL" class="headerlink" title="PFL"></a>PFL</h3><p>个性化联邦学习：在联邦学习(FL)的基础上， PFL的目标是为每个客户端训练一个个性化模型，适应每个客户端的特定数据分布和需求。PFL适用于各客户端数据分布差异较大，且每个客户端需要一个定制化模型的场景。<strong>不同于FL训练一个全局共享的模型，希望是该模型在所有客户端上表现良好。</strong></p><h4 id="PFL分类"><a href="#PFL分类" class="headerlink" title="PFL分类"></a>PFL分类</h4><p>“Towards Personalized Federated Learning”一文将个性化联邦学习（PFL）分为两类：</p><ol><li>全局模型个性化（Global Model Personalization）：第一阶段，训练一个共享的全局FL模型；第二阶段，在本地的数据上进行额外的训练，达到适应个性化的目的。在这一类模型中，关注与第一阶段全局FL模型在non-IID数据上的训练能力。</li><li>学习个性化模型（Learning Personalized Model）：在训练阶段，就达到模型个性化的效果。个人理解：区别于上种二阶段的PFL，这一类方法在一阶段就实现了PFL（但这样理解的话似乎把Regularization based的方法归入architecture更合理）。</li></ol><p><a href="https://zhuanlan.zhihu.com/p/497934969">参考资料</a></p><h2 id="词汇"><a href="#词汇" class="headerlink" title="词汇"></a>词汇</h2>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习（联邦学习） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Joint Local Relational Augmentation and Global Nash Equilibrium for Federated Learning with Non-IID Data</title>
      <link href="/2025/07/16/Joint-Local-Relational-Augmentation-and-Global-Nash-Equilibrium-for-Federated-Learning-with-Non-IID-Data/"/>
      <url>/2025/07/16/Joint-Local-Relational-Augmentation-and-Global-Nash-Equilibrium-for-Federated-Learning-with-Non-IID-Data/</url>
      
        <content type="html"><![CDATA[<p><code>这篇blog用于记录我学习一篇有关运用Nash博弈解决联邦学习有关问题的论文时，学习到的相关知识</code></p><h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><h3 id="Federated-Learning"><a href="#Federated-Learning" class="headerlink" title="Federated Learning"></a>Federated Learning</h3><h4 id="基本架构"><a href="#基本架构" class="headerlink" title="基本架构"></a>基本架构</h4><p>联邦学习的基本架构是Server和Clients</p><p>Server通常没有数据，可以有一些用于评估模型的数据，但是在普通联邦学习中Server没有任何数据</p><p>Clients持有实际的训练数据，Clients的数量取决于有多少分布式的数据要参与训练。Clients会在各自的本地数据集上进行实际训练。</p><p>服务器和客户端都拥有自己的模型副本，前者的称为全局模型，后者的称为局部模型。</p><h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><ol><li>服务器初始化全局模型参数</li><li>将该参数发送给客户端</li><li>客户端在本地进行训练，训练较短时间，通常是一个周期（所以不一定达到收敛）</li><li>客户端将改进后的模型参数发送回服务器，服务器收到五个模型参数，考虑不同的权重，进行聚合</li><li>检查是否收敛，否则重复步骤2</li></ol><p>更加规范化的表述是</p><p>1） Initialization:<br>Server initializes the global model</p><p>2） Communication Round:</p><p>For each communication round:<br> Server sends the global model to participating clients</p><p> Each client receives the global model</p><p>3）Client Training and Model Update:<br>For each participating client:</p><p> Client trains the received model on its local dataset</p><p> Client sends its locally updated model to the server</p><p>4） Model Aggregation:<br>Server aggregates the updated models received from all clients using Aggregation Algorithm (for instance, FedAvg)</p><p>5） Convergence Check:<br>If convergence criteria are met, end the FL process</p><p>If not, proceed to the next communication round (step 2)</p><h3 id="non-IID-data"><a href="#non-IID-data" class="headerlink" title="non-IID data"></a>non-IID data</h3><p>在联邦学习（Federated Learning, FL）的背景下，non-IID数据是指数据不是独立同分布（Independent and Identically Distributed, IID）的。IID数据假设每一条数据都是相互独立的，并且来自相同的分布。这种假设在很多传统的机器学习方法中是成立的，但在联邦学习中通常不成立</p><h4 id="可能的原因"><a href="#可能的原因" class="headerlink" title="可能的原因"></a>可能的原因</h4><p>数据来源的异质性：在联邦学习中，数据是分布在不同的客户端（如不同的用户设备）上的。每个客户端的数据可能由于用户的行为、兴趣、地理位置、设备类型等因素不同，从而导致数据分布的差异。</p><p>分布不均衡：某些客户端可能拥有更多的数据，而其他客户端可能只有少量的数据。数据的量级和类别在不同客户端之间可能是高度不均衡的。</p><p>标签分布差异：某些客户端可能只包含特定类别的数据，而其他客户端可能包含不同类别的数据。这种情况下，每个客户端的数据标签分布也是不同的。</p><h3 id="Intra-client-inconsistencies-And-Inter-client-inconsistencies"><a href="#Intra-client-inconsistencies-And-Inter-client-inconsistencies" class="headerlink" title="Intra-client inconsistencies And Inter-client inconsistencies"></a>Intra-client inconsistencies And Inter-client inconsistencies</h3><p>Intra-client Inconsistencies客户端内部不一致性 指的是单个客户端内部的数据分布问题，主要包括以下几个方面：</p><p>数据不平衡：单个客户端内部的不同类别的数据分布可能非常不均衡。例如，某个客户端可能有大量的类别A的数据，但只有少量的类别B的数据。这种不平衡会导致模型在训练过程中对某些类别的泛化能力不足。</p><p>数据稀疏性：客户端内部可能存在数据量不足的问题，特别是对于一些罕见类别的数据，这会影响模型的训练效果和泛化能力。</p><p>数据噪声：客户端内部的数据可能包含噪声或错误标注，这会影响模型的准确性和稳定性。</p><p>Inter-client Inconsistencies客户端之间不一致性 指的是不同客户端之间的数据分布差异，包括以下几个方面：</p><p>数据分布差异：不同客户端的数据可能来自不同的分布。例如，一个客户端可能主要包含城市环境下的数据，而另一个客户端可能主要包含农村环境下的数据。这种分布差异会导致全局模型在不同客户端上的表现不一致。</p><p>标签分布差异：不同客户端的数据类别分布可能不同。例如，一个客户端可能主要关注某些特定类别，而另一个客户端可能关注完全不同的类别。这会导致全局模型难以在所有客户端上都表现良好。</p><p>数据量差异：一些客户端可能拥有大量的数据，而其他客户端可能数据量很少。这种数据量差异也会影响全局模型的训练效果。</p><h3 id="Local-Relational-Augmentation"><a href="#Local-Relational-Augmentation" class="headerlink" title="Local Relational Augmentation"></a>Local Relational Augmentation</h3><p>LRA（局部关系增强）模块的目标是解决客户端内部的不一致性问题。</p><p>数据增强：通过生成或变换现有数据，使得每个客户端的数据分布更加均衡和丰富。这有助于改进模型在少数类别数据上的表现。</p><p>关系建模：在客户端内部建立数据样本之间的关系网络，利用这些关系来增强模型的学习过程。例如，可以通过图神经网络（Graph Neural Networks, GNN）来捕捉数据样本之间的相似性和相关性，从而提升模型的泛化能力。</p><p>摘一段原文的内容：</p><p>LRA first computes the similarity among a batch of data samples, and finds the neighbors of data samples based on the similarity.</p><p>Then LRA enhances the data feature representation via attentive message passing among the neighbors of data samples.</p><p>Besides, LRA conducts contrastive discrimination to maintain the representations correspondence before and after augmentation, for the same sample.</p><h3 id="Global-Nash-Equilibrium"><a href="#Global-Nash-Equilibrium" class="headerlink" title="Global Nash Equilibrium"></a>Global Nash Equilibrium</h3><p>GNE模块的目标是解决客户端之间的不一致性问题。纳什均衡在博弈论中是指在某种策略组合下，没有任何参与者能够通过单方面改变自己的策略来获得更好的结果。</p><p>摘一段原文的内容</p><p>Specifically, GNE collects the updating deviations from different clients to server.</p><p>Then GNE not only seeks a global optimization direction that maximizes the consistency among discrepant local model deviations, but also maintains clients’ optimizations towards their local optimums.</p><h4 id="在联邦学习中，全局纳什均衡的常见作用"><a href="#在联邦学习中，全局纳什均衡的常见作用" class="headerlink" title="在联邦学习中，全局纳什均衡的常见作用"></a>在联邦学习中，全局纳什均衡的常见作用</h4><p>平衡客户端贡献：在模型更新过程中，使各个客户端的贡献达到一种平衡状态，即没有任何一个客户端的更新会对全局模型产生过度的偏差。</p><p>优化全局模型：通过博弈论的方法，找到一种策略组合，使得全局模型在各个客户端的数据分布上都能表现良好。这可能涉及到权重调整、梯度校正等技术。</p><h3 id="representation"><a href="#representation" class="headerlink" title="representation"></a>representation</h3><p>“representation” 通常指的是数据在模型内部某一层次上的表达方式或特征表示。</p><p>特征表示（Feature Representation）：在深度学习模型中，输入数据（如图像、文本、音频等）经过多个层的变换后，每一层都会生成不同的特征表示。这些特征表示是原始数据在模型内部的抽象和高维度表示，能够捕捉到数据的关键特征和模式。</p><p>隐层表示（Hidden Layer Representation）：在神经网络中，隐层（即非输入层和非输出层）会生成中间表示，这些表示是原始输入数据通过网络层级传递和变换后的结果。这些隐层表示在分类、聚类或其他任务中具有重要作用。</p><p>嵌入表示（Embedding Representation）：在自然语言处理（NLP）等领域，词嵌入（如Word2Vec、GloVe）是常见的表示形式，它们将高维的稀疏数据（如单词的一个热编码）映射到低维的稠密向量空间中，从而捕捉词与词之间的语义关系。</p><h3 id="SLIM-method"><a href="#SLIM-method" class="headerlink" title="SLIM method"></a>SLIM method</h3><p>Sparse Linear Methods (SLIM) 是一种在推荐系统中广泛应用的技术，旨在通过稀疏线性模型来挖掘项目与项目之间的关系。SLIM 的核心思想是通过学习一个稀疏的线性权重矩阵来捕捉项目之间的相似性，从而提高推荐的准确性和效率。</p><h4 id="相关基本概念"><a href="#相关基本概念" class="headerlink" title="相关基本概念"></a>相关基本概念</h4><p>稀疏性（Sparsity）：SLIM 假设推荐系统中的大多数项目之间并没有直接的关联，只有少数项目之间存在显著的相似性。因此，SLIM 通过稀疏矩阵来表示这种稀疏性，从而减少计算复杂度和存储需求。</p><p>线性模型（Linear Model）：SLIM 使用线性模型来表示项目与项目之间的关系。具体来说，它通过一个线性组合来预测用户对一个项目的评分，该组合是基于用户对其他相关项目的评分加权得到的。</p><p>低秩性（Low-Rankness）：SLIM 还利用了数据的低秩特性，假设项目之间的关系可以用一个低秩矩阵来近似。这种低秩性有助于捕捉数据中的潜在结构和模式。</p><h4 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h4><p>构建相似度矩阵：首先，SLIM 构建一个项目与项目之间的相似度矩阵，可以基于统计相似性（如皮尔逊相关系数）来计算。</p><p>学习稀疏线性权重：然后，SLIM 通过优化算法学习一个稀疏的线性权重矩阵。这个矩阵的每个元素表示一个项目对另一个项目的线性关系权重。</p><p>预测评分：最后，使用学习到的权重矩阵和用户的历史评分数据，SLIM 可以预测用户对未评分项目的评分。</p><h3 id="Pearson-Correlation-Matrix"><a href="#Pearson-Correlation-Matrix" class="headerlink" title="Pearson Correlation Matrix"></a>Pearson Correlation Matrix</h3><p>皮尔逊相关矩阵（Pearson Correlation Matrix） 是一种用于度量数据集中各个变量之间线性关系强度的矩阵。它通过皮尔逊相关系数来衡量不同变量（或数据样本）之间的相关性。相关系数的值范围从 -1 到 +1，其中：</p><p>+1 表示完全正相关，即两个变量的变化方向完全一致。</p><p>0 表示没有线性相关性，即两个变量之间没有任何线性关系。</p><p>-1 表示完全负相关，即两个变量的变化方向完全相反。</p><h4 id="皮尔逊相关系数的计算公式"><a href="#皮尔逊相关系数的计算公式" class="headerlink" title="皮尔逊相关系数的计算公式"></a>皮尔逊相关系数的计算公式</h4><p>皮尔逊相关系数（$r$）的计算公式为：</p><p>$r &#x3D; \frac{\sum (X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum (X_i - \bar{X})^2 \sum (Y_i - \bar{Y})^2}}$</p><p>其中：</p><p>$X_i$和 $Y_i$分别是样本 $X$ 和 $Y$ 的第 $i$ 个观测值；<br>$\bar{X}$和 $\bar{Y}$分别是样本 $X$ 和 $Y$ 的均值。</p><h4 id="皮尔逊相关矩阵的应用"><a href="#皮尔逊相关矩阵的应用" class="headerlink" title="皮尔逊相关矩阵的应用"></a>皮尔逊相关矩阵的应用</h4><p>特征选择：在机器学习中，相关矩阵可以帮助识别高度相关的特征，进而去除冗余的特征，从而提升模型的性能。</p><h3 id="Frobenius-norm"><a href="#Frobenius-norm" class="headerlink" title="Frobenius norm"></a>Frobenius norm</h3><p>弗罗贝尼乌斯范数：一种用于衡量矩阵大小的范数，计算方法是将矩阵中所有元素的平方和开平方。</p><h3 id="KKT-conditions"><a href="#KKT-conditions" class="headerlink" title="KKT conditions"></a>KKT conditions</h3><p>KKT条件（Karush-Kuhn-Tucker Conditions）是非线性规划问题的一组必要条件，用于找到约束优化问题的最优解。</p><p>对于一个优化问题：</p><p>$\begin{aligned}<br>&amp; \min f(\mathbf{x}), \<br>&amp; \text{subject to} \ g_i(\mathbf{x}) \leq 0, \ i &#x3D; 1, \ldots, m, \<br>&amp; \ \ \ \ \ \ \ \ \ \ \ \ \ h_j(\mathbf{x}) &#x3D; 0, \ j &#x3D; 1, \ldots, p,<br>\end{aligned}$</p><p>KKT条件包括以下几个部分：</p><p>拉格朗日函数：</p><p>构造拉格朗日函数 $L(\mathbf{x}, \mathbf{\lambda}, \mathbf{\mu})$：<br>$L(\mathbf{x}, \mathbf{\lambda}, \mathbf{\mu}) &#x3D; f(\mathbf{x}) + \sum_{i&#x3D;1}^{m} \lambda_i g_i(\mathbf{x}) + \sum_{j&#x3D;1}^{p} \mu_j h_j(\mathbf{x})$</p><p>Stationarity（驻点条件）：</p><p>$\frac{\partial L(\mathbf{x}, \mathbf{\lambda}, \mathbf{\mu})}{\partial \mathbf{x}} &#x3D; 0$</p><p>Primal Feasibility（原始可行性）：</p><p>$g_i(\mathbf{x}) \leq 0, \quad i &#x3D; 1, \ldots, m$</p><p>$h_j(\mathbf{x}) &#x3D; 0, \quad j &#x3D; 1, \ldots, p$</p><p>Dual Feasibility（对偶可行性）：</p><p>$\lambda_i \geq 0, \quad i &#x3D; 1, \ldots, m$</p><p>Complementary Slackness（互补松弛性）：</p><p>$\lambda_i g_i(\mathbf{x}) &#x3D; 0, \quad i &#x3D; 1, \ldots, m$</p><h2 id="词汇"><a href="#词汇" class="headerlink" title="词汇"></a>词汇</h2><p>decentralized：分散管理的</p><p>discrepant：有差异的；矛盾的</p><p>deviation：偏离；偏差</p><p>distributed：分布式的</p><p>paradigm：典范</p><p>unified：一致的</p><p>simultaneously：同时的</p><p>hinder：阻碍</p><p>variance：分歧，不一致；方差（统计学）</p><p>empirical：经验主义的</p><p>conventionally：照惯例</p><p>distinguishable：可辨识的</p><p>sparse：稀少的</p><p>refine：精炼；改善</p><p>contamination：污染</p><p>alleviate：减轻，缓和</p><p>contrastive：对比的</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习（联邦学习） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reforcement learning入门</title>
      <link href="/2025/07/13/Reforcement-learning%E5%85%A5%E9%97%A8/"/>
      <url>/2025/07/13/Reforcement-learning%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<p><code>这篇blog用于记录我进行强化学习入门时学习到的基础知识</code></p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>一些基本要素：</p><ol><li>Agent：进行“学习”的主体，会通过学习到的经验与环境交互，并在与环境交互的过程中进一步学习。</li><li>Environment：Agent交互的对象，客观存在，例如智能驾驶捕捉到的一张图片</li><li>State：Agent观察当前自己所处环境，获取到的局部环境信息</li><li>Action：Agent根据State结合自己以往的经验做出的行动，行动会改变Environment（Action可能是离散值，也可能是连续值，处理方法是不同的）</li><li>Reward：Agent执行Action改变Environment后，Environment给到Agent的反馈，可以用于反映Action的好坏，同时为Agent累积“经验”</li></ol><p>进一步，关于Reward的作用：<br>例如将Agent当作一个neural network，其经验就是权重参数θ，坏的Action应该产生一个Reward将θ向“反向”调节，好的Action应该产生一个Reward将θ向“正向”调节。</p><h3 id="基本的工作流程"><a href="#基本的工作流程" class="headerlink" title="基本的工作流程"></a>基本的工作流程</h3><p>强化学习算法的基本工作流程就是，根据Environment产生State输入Agent，Agent做出Action，改变Environment，获得Reward，反馈调整Agent，不断重复。直到满足退出条件</p><h3 id="关于计算机眼中的环境"><a href="#关于计算机眼中的环境" class="headerlink" title="关于计算机眼中的环境"></a>关于计算机眼中的环境</h3><p>通常情况下，计算机眼中的环境是一张图像。</p><p>例如，环境就是各种可能的画面，State就是当前捕捉到的画面，State输入给Agent(NN)，然后由Agent(NN的参数与结构)计算产生Action(决策，或是各种决策的概率)；进一步，也可以理解成State是由画面经过GNN处理后的特征信息…</p><h3 id="强化学习的一个关键问题"><a href="#强化学习的一个关键问题" class="headerlink" title="强化学习的一个关键问题"></a>强化学习的一个关键问题</h3><p>通过前面的基本概念，看似只需要训练一个输入为State，输出为Action的神经网络就可以了，但是事实并非如此。</p><p>因为本质上，这个神经网络不是一个纯粹的有监督学习，并非用于分类与回归的网络，处理的数据没有标签，对于每一个State处理后得到的Action无法立刻得知True or False（类比分类），没有办法直接地对神经网络的参数进行更新，取而代之的是产生一个Good or Bad的Reward，我们最终的目标总是要去最大化这个Reward。（当然，以一个游戏做类比，在进行一系列Action后，我们最终能够得知游戏的结果是胜利或失败，对应True or False，所以说不是纯粹的有监督学习）。</p><p>所以强化学习算法的关键是要有对于每种Action给出合理Reward的方案，并且要有根据Reward去更新模型参数的方案。</p><p>关于<strong>强化学习对比深度学习</strong>例如给出老虎的图片做分类</p><p>深度学习可以用CNN去提取特征，然后根据当前模型参数计算出分类标签，根据估计的标签和真实标签的差异情况，计算损失，根据损失更新模型参数，就完成了一次学习。</p><p>而强化学习对于给出的图片是先由Agent，得出Action，然后有Action与Reward，再去想办法更新模型参数。</p><p>所以，<strong>如何训练一个网络，可以根据Action和Reward进行更新</strong>是强化学习最关键的问题。</p><h2 id="PPO-Proximal-Policy-Optimization-算法"><a href="#PPO-Proximal-Policy-Optimization-算法" class="headerlink" title="PPO(Proximal Policy Optimization)算法"></a>PPO(Proximal Policy Optimization)算法</h2><h3 id="介绍背景"><a href="#介绍背景" class="headerlink" title="介绍背景"></a>介绍背景</h3><p>此部分根据一个飞船降落的小游戏，介绍PPO算法，这个小游戏是一个下降的飞船，玩家应该根据left or right去操作飞船的左右行动，最终让飞船落地时在两个小旗帜中间，即降落在正确的位置。</p><h3 id="再次从深度学习的视角看"><a href="#再次从深度学习的视角看" class="headerlink" title="再次从深度学习的视角看"></a>再次从深度学习的视角看</h3><p>关于神经网络NN（θ）</p><ol><li>输入：使用NN神经网络，输入是当前游戏的图片</li><li>输出：两个Action，左或右</li></ol><p>如果我们使用深度学习，看起来这是一个二分类的问题，很简单。如果说θ作为权重参数会控制，对于每一张图片的分类结果，左或右，代表应该怎样玩。那么我们最终的目标应该是求出一个合适的θ，可以对每一张图片给出合理的决策，其中可以用到梯度下降等等一些常见的方法。</p><p>但是，我们不得不面临一些非常困难的问题，标签（tag）、梯度（gradient）、损失函数（loss function）应该怎样去确定，这是强化学习需要考虑的问题。</p><p>因为不是每一步Action都有一个对应的“正确的”tag，告诉我们的决策是否正确，我们每轮训练，只有一系列的Action，以及最终的一个结果，胜利或失败（或许可以称为tag），那么我们应该如何去定义损失函数？<strong>这就是PPO算法考虑的关键问题，即目标函数的定义方法&amp;如何进行求解</strong></p><h3 id="PPO算法"><a href="#PPO算法" class="headerlink" title="PPO算法"></a>PPO算法</h3><h4 id="episod"><a href="#episod" class="headerlink" title="episod"></a>episod</h4><p>一个完整的过程，其中具有许多的State，Agent根据每个State做出Action，改变环境在此处即根据Action进入下一个State，直到停止的退出条件即完成了一个episod。</p><h4 id="常见的退出条件"><a href="#常见的退出条件" class="headerlink" title="常见的退出条件"></a>常见的退出条件</h4><p>max_step，最大迭代次数，Agent最多只能进行max_step次Action的产生，完成后退出episod；其它退出条件，对于具体的游戏可能需要设置不同的退出条件，例如此处就是飞机落到地面上，episod就完成了。</p><h4 id="整个生命周期的奖励"><a href="#整个生命周期的奖励" class="headerlink" title="整个生命周期的奖励"></a>整个生命周期的奖励</h4><p>$R &#x3D; \sum^{T}_{t&#x3D;1} r_t$，如果考虑退出条件为设置max_step &#x3D; 1000，那么对于Action（$a_1, a_2, \dots, a_1000$），有Reward（$r_1, r_2, \dots, r_1000$），求和后即整个episod的Reward(R)。</p><p>奖励可以类比于标签（用于衡量每次Action的好坏），在一些简单的实验中我们可以使用openAI的工具包gym，其中按照一些经典的小游戏的游戏规则，会<strong>给定好每种Action的Reward</strong>。</p><p>奖励是由当前一步的Action与State共同决定的，预先确定好后，作为游戏规则是不能改变的</p><p>（当然奖励的设置仍然是强化学习的关键，只是在这一部分，对于PPO算法的学习，我们更加关注于<strong>神经网络的设置、目标函数的定义、参数更新的方法</strong>）</p><h4 id="一次游戏的记录结果"><a href="#一次游戏的记录结果" class="headerlink" title="一次游戏的记录结果"></a>一次游戏的记录结果</h4><p>即每一步的状态与行动（trajectory），$\tau &#x3D; {s_1,a_1,s_2,a_2,\dots,s_T,a_T}$（在此基础上，我们需要考虑如何给出每一步的行动，让总的奖励达到最大，即训练一个网络模型$\pi_\theta(a_t|s_t)$，其输入是状态State，输出是Action，由所有的模型参数来根据输入决定输出，训练参数的目标是要让奖励尽可能的大）</p><p>游戏记录的表达式是：</p><p>$p_theta(s_1,a_1,\dots,s_T,a_T) &#x3D; p(s_1)\prod^{T}<em>{t&#x3D;1}\pi_\theta(a_t|s_t)p(s</em>{t+1}|s_t,a_t)$</p><p>含义是得到游戏记录，初始状态是$s_1$执行action，$a_1$，切换到状态，$s_2$执行action，$a_2$，…切换到状态$s_T$执行action，$a_T$的概率是：初始状态是$s_1$的概率连乘上模型在该状态下给出对应action的概率和根据游戏规则，在当前状态下执行相应action后切换到下一对应状态的概率</p><h4 id="Action的产生与作用"><a href="#Action的产生与作用" class="headerlink" title="Action的产生与作用"></a>Action的产生与作用</h4><p>关于Action的产生$p_\theta(a_t|s_t)$，由当前状态确定某一行为的概率，对应Action产生的概率，是模型输出的结果，同时是我们应该关注的问题</p><p>关于Action的作用，$p(s_{t+1}|s_t,a_t)$在某一状态下做出某Action，会切换到下一个状态，这是根据游戏的规则确定的，与我们的模型无关</p><h4 id="一般的强化学习算法的目标与训练过程"><a href="#一般的强化学习算法的目标与训练过程" class="headerlink" title="一般的强化学习算法的目标与训练过程"></a>一般的强化学习算法的目标与训练过程</h4><p>我们的目标是得到一个合适的模型参数 $\theta^{*} &#x3D; argmax_\theta E_{\tau \sim p_\theta(\tau)}[\sum_t r(s_t,a_t)]$</p><p><em>相关理解：</em></p><p>其中$p_\theta(\tau)$表示在策略$\pi_\theta$下，产生轨迹$\tau$的概率分布，即$p_\theta(\tau)$是轨迹 $\tau$ 的概率，其依赖于策略的参数$\theta$。</p><p>在优化目标 $\theta^{*} &#x3D; \arg\max_\theta E_{\tau \sim p_\theta(\tau)}\left[\sum_t r(s_t, a_t)\right]$中，$E_{\tau \sim p_\theta(\tau)}$表示在策略 $\pi_\theta$下，对所有可能的轨迹 $\tau$ 进行期望（期望值的计算）。具体来说，$\tau \sim p_\theta(\tau)$这个下标的含义是：轨迹 $\tau$ 是根据策略 $\pi_\theta$产生的。也就是说，我们考虑的是在当前策略 $\pi_\theta$下，每个轨迹 $\tau$ 出现的概率，并对其累计奖励 $\sum_t r(s_t, a_t)$进行加权平均（即期望值）。</p><p>进一步，根据大数定律</p><p>$E_{\tau \sim p_\theta(\tau)} \approx \frac{1}{N}\sum_{i}\sum_{t}r(s_{i,t},a_{i,t})$其中$N$趋近于$∞$，我们将该期望记作$\mathcal{J}_\theta$</p><p>或者是，期望的展开</p><p>这里直接将$\pi_\theta$记作$\tau$的分布概率，将$\tau$对应的奖励，即刚才的$\sum_t r(s_t, a_t)$记作$r(\tau)$，对期望展开如下:</p><p>$\mathcal{J}(\theta) &#x3D; E_{\tau \sim \pi_\theta(\tau)}[r(\tau)] &#x3D; \int \pi_\theta(\tau)r(\tau)d\tau$</p><p>在期望展开的基础上，我们计算对于参数的梯度</p><p>$\nabla_\theta \mathcal{J}<em>\theta &#x3D; \int \nabla_\theta \pi_\theta (\tau)r(\tau)d\tau &#x3D; \int \pi_\theta(\tau)\nabla_\theta log\pi_\theta(\tau)r(\tau)d\tau &#x3D; E</em>{\tau \sim \pi_\theta(\tau)}[\nabla_\theta log \pi_{\theta}(\tau)r(\tau)]$</p><p>最后，再使用大数定律展开</p><p>$\nabla_\theta \mathcal{J}(\theta) \approx \frac{1}{N}\sum^{N}<em>{i&#x3D;1}(\sum^{T}</em>{t&#x3D;1}\nabla_\theta log \pi_{\theta}(a_{i,t}|s_{i,t}))(\sum^T_{t&#x3D;1}r(s_{i,t},a_{i,t}))$</p><p>注意，这里把得到轨迹$\tau$的概率展开了，把其对应的奖励也展开了</p><p>我们训练的过程就是使用<strong>梯度上升</strong>更新参数，对应为：</p><p>$\theta$ &lt;- $\theta + \alpha \nabla_\theta \mathcal{J}(\theta)$</p><h2 id="蒙特卡洛方法"><a href="#蒙特卡洛方法" class="headerlink" title="蒙特卡洛方法"></a>蒙特卡洛方法</h2><h3 id="Motivating-example"><a href="#Motivating-example" class="headerlink" title="Motivating example"></a>Motivating example</h3><p>Q：我们如何能够不依赖于模型，进行一些预测<br>A：最简单的思想就是，Monte Carlo estimation</p><h4 id="一个例子——抛掷硬币"><a href="#一个例子——抛掷硬币" class="headerlink" title="一个例子——抛掷硬币"></a>一个例子——抛掷硬币</h4><p>定义一个变量$X$，如果正面朝上则$X &#x3D; 1$，否则$X &#x3D; -1$，我们现在要考虑的是如何计算期望$E(X)$</p><p>显然，当我们拥有一个模型（概率分布）的时候，即$X \sim p(X), p(X&#x3D;1)&#x3D;0.5, P(X&#x3D;-1)&#x3D;0.5$，那么我们可以根据期望的定义$E(X) &#x3D; \sum_x xp(x)$快速地求出均值。</p><p>但是当我们没有这个模型的时候呢？即Model-free的情况</p><p>Monte Carlo estimation：进行多次实验，用平均数近似期望</p><p>$E(X) &#x3D; \frac{1}{n}\sum^n_{i&#x3D;1}x_i$</p><p>数学上的支撑就是我们的大数定律（Law of Large Numbers）</p><p>我们知道state value、action value的定义本身就是期望expectations，所以后续我们会用Mente Carlo在Model free的条件下去求state value、action value，并得出相应的策略</p><h3 id="MC-Basic"><a href="#MC-Basic" class="headerlink" title="MC Basic"></a>MC Basic</h3><p>Key quesition：How to convert the policy iteration algorithm to be model-free</p><h4 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h4><p>策略迭代（Policy Iteration）是强化学习中的一种经典算法，用于解决马尔可夫决策过程（MDP）问题。其目标是找到一个最优策略，使得在该策略下的长期累积奖励最大化。策略迭代由两个主要步骤组成：策略评估（Policy Evaluation）和策略提升（Policy Improvement）。</p><p>策略迭代的步骤</p><p>初始化</p><p>初始化一个任意策略 $\pi$。</p><p>策略评估（Policy Evaluation）</p><p>在策略评估阶段，我们计算当前策略 $\pi$ 的状态价值函数 $V^{\pi}(s)$<br>，即在策略 $\pi$ 下，从状态 $s$ 开始的期望累积奖励。</p><p>通过贝尔曼期望方程迭代地更新 $V^{\pi}(s)$<br>：<br>$  V^{\pi}(s) &#x3D; \sum_{a} \pi(a|s) \sum_{s’} P(s’|s,a) [R(s,a,s’) + \gamma V^{\pi}(s’)]$</p><p>其中，$P(s’|s,a)$是从状态 $s$ 执行动作 $a$ 转移到状态 $s’$的概率，$R(s,a,s’)$是从状态 $s$ 执行动作 $a$ 并转移到状态 $s’$所得到的奖励， $\gamma$ 是折扣因子。</p><p>策略提升（Policy Improvement）</p><p>在策略提升阶段，我们使用当前价值函数 $V^{\pi}(s)$来改进策略 $\pi$。<br>通过贪心策略提升来生成一个新的策略 $\pi’$，使得在每个状态下选择使得期望累积奖励最大的动作：</p><p>$  \pi’(s) &#x3D; \arg\max_{a} \sum_{s’} P(s’|s,a) [R(s,a,s’) + \gamma V^{\pi}(s’)]<br> $</p><p>检查收敛<br>如果新的策略 $\pi’$等于旧的策略 $\pi$（即策略不再变化），则策略迭代过程结束，当前策略即为最优策略。</p><p>否则，更新策略 $\pi \ne \pi’$，继续迭代。</p><h4 id="key-point"><a href="#key-point" class="headerlink" title="key point"></a>key point</h4><p>在Policy Iteration的策略提升过程中，实际上我们不止考虑一个针对当前state的最优action，而是会考虑一系列action，即最优的策略，可以写作：</p><p>$\pi’(s) &#x3D; argmax_\pi \sum_a \pi(a|s) [\sum_r p(r|s,a)r + \gamma \sum_{s’}p(s’|s,a)v_{\pi_k}(s’)] &#x3D; argmax_\pi \sum_a \pi(a|s)q_{\pi_k}(s,a), s \in S$</p><p>问题的关键就在于$q_{\pi_k}(s,a)$这个状态动作值函数，而$\pi(a|s)$为状态s下采取动作a的概率，这在mento carlo考虑的场景下是已知的</p><p>我们回到状态值函数的定义</p><p>$q_{\pi_k}(s,a) &#x3D; E[G_t|S_t &#x3D; s,At &#x3D; a]$</p><p>其中$G_t$是状态s下做出动作a后的累积的奖励；可以看到其本质是一个期望，所以可以用mento carlo的方法来估计它</p><h4 id="具体求解"><a href="#具体求解" class="headerlink" title="具体求解"></a>具体求解</h4><p>1）从任意状态$(s,a)$出发，根据当前策略$\pi_k$，生成一个episode</p><p>2）返回episode的累积奖励$g(s,a)$，这里$g(s,a)$本质上就是状态值函数期望函数中的变量$G_t$的一个采样（sample）</p><p>3）进行多轮采样，令$q_{\pi_k}(s,a) &#x3D; \frac{1}{N}\sum^{N}_{i&#x3D;1}g^{(i)}(s,a)$</p><p>得到了$q_{\pi_k}(s,a)$之后，我们就可以和policy iteration中的第二步一样，去根据这个状态值函数，更新我们的最优策略。<strong>所以mento carlo与policy iteration两者最大的不同就在于值函数的求解，policy iteration不是model-free的，相关的概率是已知的，可以直接根据期望的定义求解，而mento carlo做不到。</strong></p><h2 id="奖励设置"><a href="#奖励设置" class="headerlink" title="奖励设置"></a>奖励设置</h2><p><code>这一部分介绍有关栅格迷宫问题可以使用的奖励设置</code></p><h3 id="目标导向"><a href="#目标导向" class="headerlink" title="目标导向"></a>目标导向</h3><p>当智能体成功到达目标位置时，给予一个较大的正奖励；反之，每一步移动没有到达目标点，可以给予一个较小的负奖励。</p><p>从而让智能体更快地到达目标位置。</p><h3 id="常见的惩罚"><a href="#常见的惩罚" class="headerlink" title="常见的惩罚"></a>常见的惩罚</h3><ol><li>智能体试图移动到不可到达的位置，给予负奖励，防止撞墙</li><li>智能体重复访问同一位置的时候，给予负奖励，防止重复访问</li></ol><h3 id="其它设置"><a href="#其它设置" class="headerlink" title="其它设置"></a>其它设置</h3><p>距离奖励，每步靠近目标给予正奖励；远离目标给予负奖励<br>探索奖励，进入没有探索过的区域给予正奖励，但是逐渐收敛</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习（强化学习） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Fairness-Aware Meta-Learning via Nash Bargaining</title>
      <link href="/2025/07/06/Fairness-Aware-Meta-Learning-via-Nash-Bargaining/"/>
      <url>/2025/07/06/Fairness-Aware-Meta-Learning-via-Nash-Bargaining/</url>
      
        <content type="html"><![CDATA[<p><code>这篇blog用于记录我阅读的一篇将传统的Game中的方法应用到学习中的论文</code></p><h2 id="论文理解"><a href="#论文理解" class="headerlink" title="论文理解"></a>论文理解</h2><p>思路：</p><ol><li>经典的Meta-learning的框架</li><li>Meta-learning在learning with fairness中的运用（框架、典型的方法）</li><li>经典方法中存在的问题 &amp; 使用 NBS的改进</li></ol><h3 id="Meta-learning的框架"><a href="#Meta-learning的框架" class="headerlink" title="Meta-learning的框架"></a>Meta-learning的框架</h3><p>在解决机器学习的过程中对于不同group的公平性问题的时候，会使用sensitive-attributed validation set来训练调整模型的参数，这个过程与常规的训练过程相结合通常被套入一个meta-learning framework中。</p><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p><code>用于记录阅读过程中，遇到的新的、不熟悉的概念</code></p><h3 id="Group-level-fairness"><a href="#Group-level-fairness" class="headerlink" title="Group-level fairness"></a>Group-level fairness</h3><p>群体级公平性： 群体级公平性是指在机器学习模型中确保不同群体（如性别、种族、年龄等）在预测结果上受到公平对待。具体来说，这意味着模型的性能（如准确率、误报率等）在不同群体之间应该尽可能一致，避免某些群体受到系统性的偏见或歧视。例如，在招聘系统中，不同性别的候选人应该有相似的通过率，而不是系统性地偏向某一性别。</p><h3 id="Fairness-objectives"><a href="#Fairness-objectives" class="headerlink" title="Fairness objectives"></a>Fairness objectives</h3><p>公平性目标：公平性目标是指在模型训练和评估过程中设定的具体指标，用来衡量和改进模型的公平性。</p><p>常见的公平性目标有：</p><ol><li>Demographic parity：模型的预测结果应该在不同群体之间均匀分布。</li><li>Equalized odds：在不同群体中，模型的假正率和假负率应该相同。</li><li>Equal opportunity：对于实际正类样本，不同群体的真正率应该相同。</li></ol><p>补充：<br>Demographically balanced validation set（人口统计学平衡的验证集）：指在机器学习模型的验证过程中，确保验证集中的数据在人口统计学特征（例如：性别、年龄、种族、收入水平、地理位置等）上具有均衡性。这种平衡的目标是使得模型能够在不同群体之间表现一致，从而避免模型在某些群体上产生偏差或不公平的表现。</p><h3 id="Sensitive-attributed-validation-set"><a href="#Sensitive-attributed-validation-set" class="headerlink" title="Sensitive attributed validation set"></a>Sensitive attributed validation set</h3><p>敏感属性验证集：敏感属性验证集是指包含敏感属性（如性别、种族、年龄等）的数据集，用于评估模型在这些属性上的表现和公平性。通过在验证集上测试模型的表现，可以确定模型是否对某些群体存在偏见，并据此调整模型参数以提升公平性。例如，如果发现模型在不同种族上的准确率差异较大，可以通过调整模型来减少这种差异。</p><h3 id="Meta-learning-framework"><a href="#Meta-learning-framework" class="headerlink" title="Meta-learning framework"></a>Meta-learning framework</h3><p>元学习：指的是“学习如何学习”，即通过学习算法在多个任务上的表现，来调整和优化学习过程本身。在机器学习中，元学习框架通常用于设计模型，帮助它们更好地适应新任务，或者从不同任务中学习出更泛化的知识。</p><p>元学习框架：通过一个高层的学习过程，动态地调整模型的参数，以便满足公平性目标。这种框架让机器学习模型不仅仅是对一个固定任务进行学习，还能调整自己的学习策略，以实现更好的公平性目标。</p><h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p>为什么需要Meta-learning</p><p>神经网络在进行猫、狗的图像识别的时候，可能需要数以千计的图片用于训练模型的参数，最终对于新给出的图片神经网络才能给出正确的结果。类比人类的小孩子，在没有见过任何动物的情况下，可能也需要见过许多猫猫狗狗才能正确区分这两种动物。</p><p>在此基础上，出现了一只驴子，人类小孩可以结合以前对于识别猫狗的经验，来观察驴子的特征，通过这一只驴子，在未来就可能正确的识别出新的驴子；但是对于神经网络而言无法做到。</p><p>Meta-learning的目的就是想赋予神经网络这样的能力——基于过去的学习经验，对于新的学习任务，进行一个快速的学习。</p><h4 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h4><p>基本机器学习算法流程：</p><ol><li>Data1，例如(x, y)，x是特征，y是label，将它们输入模型</li><li>Traning1，对Loss求gradient，迭代</li><li>Model1，直到收敛，得到一组权重，对应就是Model1</li></ol><p><em>算法描述：</em></p><p>For all $\mathcal{T}_i$ do<br>Evaluate $\nabla_\theta \mathcal{L}(\mathcal{T}_i, f_\theta)$ with respect to $K$ examples.<br>Compute adapted parameters with gradient descent:<br>$\theta’_i &#x3D; \theta - \alpha \nabla_\theta \mathcal{L}(\mathcal{T}_i, f_\theta)$<br>end for</p><p>实际上这就是Meta learning的inner part，在此基础上加上outer part就是完整的元学习框架。</p><p><em>算法描述：</em></p><p>Require：$p(\mathcal{T})$: distribution over tasks<br>Require: $\alpha, \beta$: step size hyperparameters</p><ol><li>randomly initialize $\theta$</li><li>while not done do</li><li>Sample batch of tasks $\mathcal{T}_i$~$p(\mathcal{T})$</li><li>for all $\mathcal{T}_i do$</li><li>Evaluate $\nabla_\theta \mathcal{L}(\mathcal{T}_i, f_\theta)$ with respect to $K$ examples.  </li><li>Compute adapted parameters with gradient descent:<br>$\theta’_i &#x3D; \theta - \alpha \nabla_\theta \mathcal{L}(\mathcal{T}_i, f_\theta)$  </li><li>end for</li><li>Update $\theta$&lt;-$\theta - \beta \nabla_{\theta} \sum_{\mathcal{T}<em>i ~ p(\mathcal{T})}\mathcal{L}</em>{\mathcal{T}<em>i}(f</em>{\theta’_{i}})$</li><li>end while</li></ol><p>元学习有两个loop，inner loop对应for，outer loop对应while</p><p>inner loop会从很多个模型挑出几个进行训练，$\theta_i$就对应第i个模型（训练器）</p><p>$p(\mathcal{T})$就是所有的Task<br>$\alpha, \beta$是学习率，前者是对每个模型学习的学习率，后者是元学习的学习率</p><p>Update $\theta$&lt;-$\theta - \beta \nabla_{\theta} \sum_{\mathcal{T}<em>i ~ p(\mathcal{T})}\mathcal{L}</em>{\mathcal{T}<em>i}(f</em>{\theta’_{i}})$是最关键的一步，结合inner loop中的所有loss，定义新的loss，再求梯度，用来更新$\theta$。其体现出的是元学习模型是要学习各种小模型的平均能力。</p><p>其好处是你最终得到的$\theta$（对应的各种weights），可以用于作为未来你要训练的用于一个新的任务的小模型时$\theta_{new}$的初始值（新的任务与过去的各种小模型对应的任务相似），这样由于初始的权重天然对应各种小模型的平均能力，其在学习的过程中可以很快地收敛，加快训练速度。</p><h3 id="Hypergradient"><a href="#Hypergradient" class="headerlink" title="Hypergradient"></a>Hypergradient</h3><p>超梯度： 在元学习中，<strong>超梯度（hypergradient）</strong>是指对学习过程本身的梯度进行计算。简单来说，元学习需要优化的目标不仅仅是模型的参数（如权重），还包括学习规则或算法本身的参数（例如学习率）。</p><p>超梯度冲突：在元学习过程中，当不同的子群体（如不同性别或种族的群体）需要不同的调整来实现公平性目标时，这些调整可能会产生冲突。例如，为了在某个群体上实现某个公平性目标，可能需要对模型的某个参数进行特定的调整，但对另一个群体却可能会产生不利影响，导致不同的公平性目标之间无法兼容。这样的冲突会导致模型优化过程的不稳定，甚至可能使得模型的性能和公平性都受到影响。</p><h3 id="validation-loss"><a href="#validation-loss" class="headerlink" title="validation loss"></a>validation loss</h3><p>验证损失：在验证集上评估模型性能时计算得到的损失值。它衡量了模型在验证集上的预测误差。通常来说，训练过程中，我们希望看到训练损失（training loss）和验证损失（validation loss）都逐渐降低，这表明模型在不断学习和提高性能。</p><p>损失（loss）：是模型预测结果与实际标签之间差异的度量。它通常表示为一个数值，表示模型在进行预测时的“错误程度”。常见的损失函数有均方误差（MSE）、交叉熵损失（Cross-Entropy Loss）等。</p><p>验证集：一个与训练集和测试集不同的数据集，通常用于在训练过程中对模型进行评估和调优。验证集用于检查模型的泛化能力——即模型能否在没有见过的数据上表现良好。</p><p>注：<strong>如果验证损失开始增加，而训练损失继续降低，可能表明模型在训练数据上过拟合，即模型学习了训练数据中的噪声和不相关的细节，而不是学到了一般性的规律。</strong></p><h3 id="social-context-of-a-learning-system"><a href="#social-context-of-a-learning-system" class="headerlink" title="social context of a learning system"></a>social context of a learning system</h3><p>Environment： 在传统的机器学习中，环境通常指的是模型与之交互并从中获取数据的外部系统或空间。例如，在强化学习中，智能体（agent）通过与环境交互来学习和优化其决策策略。在这个语境下，“环境”通常被视为一个相对抽象的、无差别的对象，包含了所有外部因素，模型仅通过这些因素进行训练。</p><p>Scarcity：稀缺性指的是资源（如时间、金钱、机会等）在社会中是有限的。在机器学习系统的社交背景下，稀缺性涉及到社会资源的分配问题，如何在有限的资源中做出选择，尤其是在多个利益相关方或群体之间进行权衡。</p><p>Conflict：冲突指的是在社会环境中，由于利益、目标或观点的差异，可能会出现对立或争执。在机器学习中，这个概念可以指不同利益相关者之间（如不同用户、群体、公司等）在使用技术时可能产生的矛盾与对立。例如，算法可能会在某些群体之间造成不平等，从而引发冲突。</p><p>Social norms：社会规范是指在某一社会群体中广泛接受和遵守的行为标准和价值观。它们定义了个体之间的行为预期。例如，公平性和诚信可能是许多社会群体的核心规范。机器学习系统在部署时需要考虑这些社会规范，以避免做出违反社会价值观的决策。</p><p>Communication：在机器学习的社会背景中，沟通通常指的是人与人、人与机器之间的信息交换。在机器学习系统中，沟通可能涉及模型与用户或开发者之间的反馈机制，以及如何有效地传达模型的意图、预测和决策。</p><p>Trust： 信任是指个体或群体对系统、技术或他人的可靠性和诚实性的信念。在机器学习系统中，信任至关重要，尤其是在人们需要依赖算法做出决策时。例如，如果用户不信任推荐系统或自动驾驶车辆的决策，那么这些系统的使用将受到限制。信任的缺失可能导致模型的抵制或不使用。</p><p>Fairness：公平性是指在决策或资源分配过程中，各方是否受到平等对待。机器学习中的公平性问题通常与算法可能对特定群体或个体产生不公正的偏见有关。例如，性别、种族、年龄等因素可能影响模型的预测结果。解决机器学习中的公平性问题，通常需要确保不同群体在模型中的待遇是平等的，避免不必要的偏见和歧视。</p><h3 id="Bi-level-optimization"><a href="#Bi-level-optimization" class="headerlink" title="Bi-level optimization"></a>Bi-level optimization</h3><p>双层优化：一种优化问题，其中的优化过程分为两个层级：上层优化和下层优化。每一层都有自己的优化目标和约束条件，而下层优化的解通常会影响上层优化的目标函数。</p><p>上层优化：这是优化问题的“外层”或“主层”，目标是优化一个总体目标，这通常是由下层优化问题的解所决定的。</p><p>下层优化：这是优化问题的“内层”或“子问题”，其目标是最小化或最大化一个局部目标。这个问题通常是通过上层优化问题中的参数来定义的，或者说下层优化问题的解是上层优化问题的约束之一。</p><p>在元学习中的应用：假设我们有一个双层优化问题，其中上层优化的目标是选择最优的模型参数，而下层优化则通过训练模型来优化模型的性能。这种结构常见于元学习（Meta-learning）和模型调优等问题中。例如，在元学习中，上层优化可能是学习一个优化策略，而下层优化则是针对特定任务的参数优化。</p><p>形式化：<br>双层优化问题通常可以用以下数学表达式来表示：</p><p>上层问题（Outer problem）：<br>$  \min_{\theta} , F(\theta, \mathbf{z}^*(\theta))$</p><p>其中，$\theta$ 是上层优化的决策变量，$\mathbf{z}^*(\theta)$<br>  是下层优化问题的最优解，它依赖于$\theta$。</p><p>下层问题（Inner problem）：<br>$  \min_{\mathbf{z}} , G(\mathbf{z}, \theta)$</p><p>其中，$\mathbf{z}$ 是下层优化的决策变量，$G(\mathbf{z}, \theta)$ 是下层优化问题的目标函数，$\theta$ 是从上层优化传递下来的参数。</p><h3 id="minibatch"><a href="#minibatch" class="headerlink" title="minibatch"></a>minibatch</h3><p>在机器学习和深度学习中，一种将大型数据集分割成较小的子集进行训练的方法。这种方法可以加速训练过程，同时减少计算资源的需求。</p><h2 id="词汇"><a href="#词汇" class="headerlink" title="词汇"></a>词汇</h2><p>aggregation：聚合</p><p>monotonic：单调的</p><p>steer：引导</p><p>navigate the issue：解决</p><p>validation：验证</p><p>emerging applications：新兴应用</p><p>lump：整合、混淆</p><p>deployment：部署</p><p>amplification：扩大、引申</p><p>clarity：清晰</p><p>align with：保持一致</p><p>address：解决</p><p>integrate：整合、合并</p><p>disparity：不一致</p><p>demographic：具有某种特征的群体；人口的</p><p>epochs：迭代次数</p><p>prevalence：流行、普遍存在</p><p>intrinsic：固有的</p><p>alignment issues：不一致问题</p><p>be derived by：由…推导出</p><p>untenable：站不住脚的</p><p>circumvent：回避</p><p>consensus：共识</p><p>intermediate：居中的、中等程度的</p><p>feasible：可行的、很可能会发生的</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习（元学习） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>拜占庭将军问题与计算机系统的一致性</title>
      <link href="/2025/05/25/%E6%8B%9C%E5%8D%A0%E5%BA%AD%E5%B0%86%E5%86%9B%E9%97%AE%E9%A2%98%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7/"/>
      <url>/2025/05/25/%E6%8B%9C%E5%8D%A0%E5%BA%AD%E5%B0%86%E5%86%9B%E9%97%AE%E9%A2%98%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7/</url>
      
        <content type="html"><![CDATA[<h2 id="拜占庭将军问题是什么（它阐述了什么）？"><a href="#拜占庭将军问题是什么（它阐述了什么）？" class="headerlink" title="拜占庭将军问题是什么（它阐述了什么）？"></a>拜占庭将军问题是什么（它阐述了什么）？</h2><p>对于n个将军，将一个视为指挥将军，其它n-1个视作它的副将，指挥将军会向所有副将发送命令。在此基础上，拜占庭将军问题就是考虑一种算法，确保1）所有忠诚的副将将遵守相同的命令；2）如果指挥将军是忠诚的，则每个忠诚的副将都必须遵循他发送的命令。</p><h2 id="如何确保忠诚的将军能够达成一致的行动计划？"><a href="#如何确保忠诚的将军能够达成一致的行动计划？" class="headerlink" title="如何确保忠诚的将军能够达成一致的行动计划？"></a>如何确保忠诚的将军能够达成一致的行动计划？</h2><p>对于只能使用口头消息的时候，在叛徒数少于总数的三分之一的时候，忠诚的将军能够按照下面的方法达成一致的行动：我们将第一个发送决策值的将军看错指挥官；在0个叛徒时，指挥官将决策值传递给副将，副将根据接收到的消息，或默认的撤退，直接做出对应的决策；在m个叛徒时，副将们递归地充当新的指挥官，向其它副将传递决策的值，每递归一次认为叛徒数减少一个；所有递归完成后，最终每位将军都会从其它将军哪里收到一系列决策，从这些决策中采取占多数的作为自己的决策即可。</p><p>对于使用带签名消息时可以解决任意叛徒数的情况。将军之间每一次发送信息的时候都附带上自己的签名，这样的话只要出现了篡改信息（B收到A签名的Attack，收到A签名+C签名的Rtreat，C是叛徒）或发送误导信息（B收到C签名的Attack，A收到C签名的Rtreat，C是叛徒）的情况，可以直接判断出谁是判断。排除或减少一定数量的叛徒之后，就可以进一步协商达成一致的行动。</p><h2 id="拜占庭将军问题的解决方案能够应用于哪些计算机领域？"><a href="#拜占庭将军问题的解决方案能够应用于哪些计算机领域？" class="headerlink" title="拜占庭将军问题的解决方案能够应用于哪些计算机领域？"></a>拜占庭将军问题的解决方案能够应用于哪些计算机领域？</h2><p>其解决方案主要应用于确保计算机系统的可靠性。尤其是存在故障组件，向系统的不同部分发送冲突信息的情况。具体的相关领域，可以是我们课堂上了解到的，用于确保分布式系统的一致性，用于保障区块链和加密货币中交易的有效性和账本的一致性等等的要求高可靠性的场景。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 计算机基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式的原子性与一致性</title>
      <link href="/2025/05/25/%E5%88%86%E5%B8%83%E5%BC%8F%E7%9A%84%E5%8E%9F%E5%AD%90%E6%80%A7%E4%B8%8E%E4%B8%80%E8%87%B4%E6%80%A7/"/>
      <url>/2025/05/25/%E5%88%86%E5%B8%83%E5%BC%8F%E7%9A%84%E5%8E%9F%E5%AD%90%E6%80%A7%E4%B8%8E%E4%B8%80%E8%87%B4%E6%80%A7/</url>
      
        <content type="html"><![CDATA[<p><code>一次计算机系统工程导论的习题</code></p><h2 id="问题一"><a href="#问题一" class="headerlink" title="问题一"></a>问题一</h2><p>选择：A</p><p>因为小明的客户端到S3之间的网络经常停止工作，每次几分钟，而根据伪代码，在更新S3服务器上的信息的时候，while循环会一直尝试直到rpc_OK被置为true为止。所以CLENTWRITE通常需要花费几分钟（S3导致的）或更长的时间（可能不排除S1、S2也有出问题的可能性），去更新服务器。</p><h2 id="问题二"><a href="#问题二" class="headerlink" title="问题二"></a>问题二</h2><p>返回：Breakfast</p><p>因为在系统没有故障的时候，该分布式系统的一致性是有得到保障的，根据题目中串行执行的代码，读取到的内容会是最后一次写入的内容，对应为Breakfast。</p><h2 id="问题三"><a href="#问题三" class="headerlink" title="问题三"></a>问题三</h2><p>选择：AB</p><p>对于A，如果客户端计算机是在已经完成最后一次CLENTWRITE操作中对部分服务器的更新，那么当使用CLIENTREAD的时候就可能出现A的结果。</p><p>对于B，如果是在CLIENTWRITE (0,11, “Talk to Frans at 11”)刚刚完成的时候，发生了重启，那么使用CLIENTREAD就可能出现B的结果。</p><p>对于C，Breakfast at 10意味着最后一次更新服务器已经部分完成，这与 Free at 11是矛盾的。</p><p>对于D，与C同理，存在矛盾。</p><h2 id="问题四"><a href="#问题四" class="headerlink" title="问题四"></a>问题四</h2><p>选择：ABD</p><p>对于A，如果所有服务器顺利更新，对应为A的结果</p><p>对于B，如果客户端重启前已经顺利执行完CLIENTWRITE (0,10,”Talk to Frans at 10”)，对应为B的结果。</p><p>对于C，CLIENTWRITE (0,10, “Breakfast at 10”)至少已经对部分服务器完成了更新，这意味着CLIENTWRITE (0,11, “Talk to Frans at 11”)已经对服务器完成了更新，那么不应该出现Free at 11，所以C错误</p><p>对于D，如果CLIENTWRITE (0,10,”Talk to Frans at 10”)只是对部分服务器完成更新例如S1、S2，那么S3中对应保存的仍然是Free at 10。当执行CLIENTREAD的时候，如果第一次从S1或S2顺利读取了结果，但是第二次S1、S2的网络出现了问题，从S3读取了结果，则可能出现这样的情况。</p><h2 id="问题五"><a href="#问题五" class="headerlink" title="问题五"></a>问题五</h2><p>选择：ACD</p><p>对于A，如果所有更新与读取都正常，那么得到的会是A的结果。</p><p>对于B，在第二次读取的时候还没有尝试过“Z”的写入，不会读取到“Z”，故错误。</p><p>对于C，如果第二次更新尝试没有一个服务器成功更新，其它操作均正常，则可能得到C的结果。</p><p>对于D，如果第一次更新均正常，第二次更新部分正常例如S1被更新，第三次没有成功更新任何服务器。第一次读取正常，第二次读取从S1读取到了结果，第三次读取从S2或S3读取到结果，则可能出现这样的情况。</p><h2 id="问题六"><a href="#问题六" class="headerlink" title="问题六"></a>问题六</h2><p>选择：A</p><p>因为小明原本的系统在CLIENTWRITE的时候对每个服务器使用了while循环来确保rpc_OK&#x3D;true，这意味着要么对服务器完成更新，要么客户端一直执行CLIENTWRITE。所以如果顺利读取到三个结果，只能是正确的结果，即A对应的结果，其它情况都不可能出现。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 计算机基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>毛概复习</title>
      <link href="/2025/05/18/%E6%AF%9B%E6%A6%82%E5%A4%8D%E4%B9%A0/"/>
      <url>/2025/05/18/%E6%AF%9B%E6%A6%82%E5%A4%8D%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p><code>用于记录我线上练习时遇到的知识点，以供日后复习的时候对照书本查看</code><br><code>毛概与习概的复习思路都是先线上练习，记录知识点，期末的时候对照书本查看小题，同时背诵大题</code><br><code>复习的时候要注意可能的题目变形</code></p><h2 id="第二阶段题库"><a href="#第二阶段题库" class="headerlink" title="第二阶段题库"></a>第二阶段题库</h2><h3 id="二十届三中全会"><a href="#二十届三中全会" class="headerlink" title="二十届三中全会"></a>二十届三中全会</h3><p>1</p><blockquote><p>2024年7月15至18日，二十届三中全会在北京举行。全会指出，到<strong>2029</strong>年，完成《决定》提出的改革任务。</p></blockquote><p>2</p><blockquote><p>二十届三中全会提出，要健全因地制宜发展新质生产力体制机制，推动<strong>技术革命性突破</strong>，<strong>生产要素创新性配置</strong>和<strong>产业深度转型升级</strong>。<br>（没有实体经济数字化，可以想想，确实实体经济难道一定要数字化吗？有的经济可能本身就不适合数字化…）</p></blockquote><p>3</p><blockquote><p>党的二十届三中全会指出，<strong>坚持系统观念</strong>处理好经济和社会、政府和市场、效率和公平、活力和秩序、发展和安全等重大关系，增强改革系统性、整体性、协同性。<br>（坚持系统观念可以帮助处理一系列重大关系）</p></blockquote><p>4</p><blockquote><p>二十届三中全会指出，推动生产关系和生产力、上层建筑和经济基础、治理体系和治理能力更好相适应，为中国式现代化提供强大动力和制度<br>保证。（<strong>错误</strong>）</p></blockquote><p>5</p><blockquote><p>中国式现代化的鲜明标志是开放（<strong>正确</strong>）</p></blockquote><h3 id="第五章"><a href="#第五章" class="headerlink" title="第五章"></a>第五章</h3><p>1</p><blockquote><p>下列属于邓小平南方谈话终提出的重要论断的有<br>1）要提倡科学，靠科学才有希望<br>2）革命是解放生产力，改革也是解放生产力<br>3）“三个有利于”标准<br>4）社会主义可以搞市场经济</p></blockquote><p>2</p><blockquote><p>2000年的宪法修正案正式将邓小平理论载入宪法（<strong>错误</strong>）</p></blockquote><p>3</p><blockquote><p>党的十七大把科学发展观确立为党必须长期坚持的指导思想（<strong>错误</strong>）</p></blockquote><p>4</p><blockquote><p>邓小平在<strong>党的十二大</strong>上提出“走自己的道路，建设有中国特色的社会主义”的论断，中国特色社会主义成为了党的全部理论和实践创新主题。</p></blockquote><p>5</p><blockquote><p>新世纪新阶段，我国进入<strong>发展关键期、改革攻坚期和矛盾凸显期</strong>，经济社会发展呈现一系列新的阶段性特征。<br>（没有开放活跃期）</p></blockquote><p>6</p><blockquote><p>中国特色社会主义理论体系形成时期的时代主题是<strong>和平与发展</strong></p></blockquote><h3 id="第六章"><a href="#第六章" class="headerlink" title="第六章"></a>第六章</h3><p>1</p><blockquote><p>邓小平指出社会主义的本质是<br>1）解放生产力，发展生产力<br>2）消灭剥削，消除两极分化<br>3）最终达到共同富裕<br><strong>没有以经济建设为中心</strong></p></blockquote><p>2</p><blockquote><p>1978年12月召开的党的十一届三中全会，做出了以下重大决策<br>1）重新确立了解放思想、实事求是的思想路线<br>2）批评了“两个凡是”的错误方针<br>3）确定把党和国家的工作重点转移到社会主义现代化建设上来的战略决策<br><strong>因为是“决策”，所以没有“提高了对科学社会主义的认识”</strong></p></blockquote><p>3</p><blockquote><p>发展社会主义市场经济是当代中国的鲜明标志和活力源泉，是决定中国命运的关键一招（<strong>错误</strong>）</p></blockquote><p>4</p><blockquote><p>邓小平认为和平是东西问题，发展是南北问题（<strong>正确</strong>）</p></blockquote><p>5</p><blockquote><p>1997年党的十五大指出不发达的社会生产力水平是中国最大的实际（<strong>错</strong>）</p></blockquote><p>6</p><blockquote><p>党的十四大把<strong>建立社会主义市场经济体制</strong>作为我国经济体制改革的目标</p></blockquote><p>7</p><blockquote><p>在社会主义的依靠力量上，<strong>人民群众</strong>是我们党的力量源泉和胜利之本</p></blockquote><p>8</p><blockquote><p>为了更好实现“三步走”的现代化发展战略，邓小平提出了以重点带动全局的思想。1982年他提出了三个战略重点，包括了<strong>农业、能源和交通、教育和科学</strong><br>（没有文化与技术、工业与金融）</p></blockquote><p>9</p><blockquote><p>以经济建设为中心是改变我国不发达现状的需要，也体现了社会主义奋斗精神。（<strong>错误</strong>）<br>（我国的现状不是不发达，人民日益增长的…）</p></blockquote><h3 id="第七章"><a href="#第七章" class="headerlink" title="第七章"></a>第七章</h3><p>1</p><blockquote><p>贯彻“三个代表”重要思想，关键在<strong>坚持与时俱进</strong><br>(或许核心在执政为民)</p></blockquote><p>2</p><blockquote><p>坚持中国共产党的领导，就是要<br>1）2）3）4）</p></blockquote><p>3</p><blockquote><p>社会主义的根本任务是<strong>发展社会生产力</strong><br>（不是实现共同富裕）</p></blockquote><p>4</p><blockquote><p>建设社会主义政治文明，最根本的就是要<strong>坚持党的领导、人民当家作主和依法治国的有机统一。</strong></p></blockquote><p>5</p><blockquote><p>江泽民强调，我们想做事，做工作，想得对不对，做得好不好，根本的衡量尺度就是<br>1）2）3）4） 人民…</p></blockquote><p>6</p><blockquote><p>党的领导是人民当家做主和依法治国的根本保证（<strong>正确</strong>）</p></blockquote><p>7</p><blockquote><p>推进党的建设新的伟大工程，重点是加强<strong>党的执政能力建设</strong></p></blockquote><p>8</p><blockquote><p>始终做到“三个代表”，是我们党的<strong>立党之本、执政之基和力量之源</strong><br>（没有兴国之要）</p></blockquote><h3 id="第八章"><a href="#第八章" class="headerlink" title="第八章"></a>第八章</h3><p>1</p><blockquote><p><strong>科学技术</strong>是先进生产力的集中体现和主要标志</p></blockquote><p>2</p><blockquote><p>以<strong>科学发展</strong>为主题，是时代的要求，关系改革开放和现代化建设全局</p></blockquote><p>3</p><blockquote><p><strong>自主创新</strong>是科技发展的灵魂，是一个民族发展的不竭动力，是支撑国家崛起的筋骨</p></blockquote><p>4</p><blockquote><p>坚持统筹兼顾，妥善处理社会主义事业重大关系，包括<br>1）统筹城乡发展<br>2）统筹区域发展<br>3）统筹经济社会发展<br>4）统筹国内发展和对外开放<br>5）统筹人与自然和谐发展</p></blockquote><p>5</p><blockquote><p><strong>扩大内需</strong>是我国经济社会发展的基本立足点和长期战略方针，也是调整经济结构的首要任务。</p></blockquote><p>6</p><blockquote><p><strong>建设生态文明</strong>关系人民福祉，关乎民族未来的长远大计</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> 大学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nash-MTL（代码实现）</title>
      <link href="/2025/05/16/Nash-MTL/"/>
      <url>/2025/05/16/Nash-MTL/</url>
      
        <content type="html"><![CDATA[<p><code>这篇blog用于记录我阅读Nash-MTL算法基于python的implemention，其中存在大量使用pytorch、tensorflow的地方，这些是我之前几乎没有接触过的东西，所以同时我也会进行相关的记录，进行学习。</code></p><h2 id="Readme"><a href="#Readme" class="headerlink" title="Readme"></a>Readme</h2><p><code>整篇blog从Nash-MTL开源项目的Readme文档开始，可以帮助我了解整个项目的文件组织结构。</code></p><h3 id="conda"><a href="#conda" class="headerlink" title="conda"></a>conda</h3><p>在Python中，conda 是一个开源的包管理器和环境管理器，它主要用于安装和管理Python包以及创建和管理虚拟环境。</p><h4 id="什么是包管理器和环境管理器"><a href="#什么是包管理器和环境管理器" class="headerlink" title="什么是包管理器和环境管理器"></a>什么是包管理器和环境管理器</h4><p>通过查阅资料，我现在通俗的理解是：</p><ol><li>包&#x2F;库，项目开发过程中用到的封装好的<strong>编程语言（或第三方）预置代码</strong></li><li>环境，当前项目所基于的所有包&#x2F;库的总称，是<strong>项目的外部依赖</strong></li></ol><h4 id="一个困惑"><a href="#一个困惑" class="headerlink" title="一个困惑"></a>一个困惑</h4><p>我之前一直有这样的困惑：为什么Py的各种包不能像C++的库一样，一次下载，可以在任何项目文件中使用；就例如numpy而言，每次我都需要在新的项目目录中通过pip重新下载，然后再导入到文件。</p><p>这次我查阅了相关资料，了解到：</p><ol><li><p>C++和Python本身对包管理“如何依赖”的设计就不同，C++ 没有官方统一的包管理工具，依赖通常由系统包管理器（如 apt、brew）或手动安装到系统目录。这种设计<strong>适合系统级工具</strong>，但容易导致版本冲突；Python 通过虚拟环境（如 venv、conda）为每个项目创建独立的依赖环境，避免全局污染。这是为了解决 Python 生态中<strong>广泛存在</strong>的依赖版本冲突问题。</p></li><li><p>C++的标准库（如STL）是语言规范的一部分，与编译器深度绑定。因此具有特权地位，它们的实现（如 libstdc++ 或 libc++）随编译器安装，因此无需额外管理。但是第三方的库（如OpenCV）仍需手动管理，其使用方式更接近 Python 的第三方包。所以并非C++不需要额外的包管理就可以一次安装随意使用，而是我目前的学识并没有接触到需要包管理的地方。</p></li><li><p>Python 标准库与第三方包的平等性，如 os、sys虽然随解释器预装，但第三方包（如 numpy）在导入机制上与标准库完全平等。</p></li><li><p>Python的虚拟环境使得包不能直接全局复用，但是这并非强制的，如果不使用虚拟环境，第三方包会被安装到全局 site-packages，所有项目共享。但这会导致：不同项目依赖的版本冲突；无权限修改全局环境（例如在服务器上）。因此，<strong>虚拟环境是 Python 社区的最佳实践</strong>，而非技术限制。</p></li></ol><h3 id="项目的文件组织"><a href="#项目的文件组织" class="headerlink" title="项目的文件组织"></a>项目的文件组织</h3><p>整个nash-mtl下包含两个主要dir</p><ol><li>experiments，其中包含了3个MTL任务文件相关代码</li><li>methods，其中包含了可以用于处理MTL任务的算法代码</li></ol><p>二者的关系，experiments中<code>trainer.py</code>是任务入口，对于每一个任务，我们可以指定一个methods中的算法来解决。</p><p>于是，<strong>接下来我打算从toy任务入手，先阅读任务文件代码，再阅读methods中Nash-MTL算法部分的代码</strong></p><h2 id="packages：torch"><a href="#packages：torch" class="headerlink" title="packages：torch"></a>packages：torch</h2><h3 id="clamp"><a href="#clamp" class="headerlink" title="clamp"></a>clamp</h3><p>在 PyTorch 中，torch.clamp 是一个非常实用的函数，主要用于对张量中的元素进行截断（clamping），将其限制在一个指定的区间范围内。</p><p>原型</p><blockquote><p>torch.clamp(input, min&#x3D;None, max&#x3D;None) → Tensor</p></blockquote><p>input<br>类型：Tensor<br>需要进行截断操作的输入张量。</p><p>min<br>类型：float 或 None（默认值）<br>指定张量中元素的最小值。小于 min 的元素会被截断为 min 值。<br>如果设置为 None，则表示不限制最小值。</p><p>max<br>类型：float 或 None（默认值）<br>指定张量中元素的最大值。大于 max 的元素会被截断为 max 值。<br>如果设置为 None，则表示不限制最大值。</p><p>返回值<br>返回一个新的张量，其中元素已经被限制在 ([min, max]) 的范围内。<br>原张量不会被修改（函数是非原地操作），除非使用 torch.clamp_ 以进行原地操作。</p><p>参考资料来源：<br><a href="https://blog.csdn.net/shizheng_Li/article/details/144432030">CSDN</a></p><h3 id="stack"><a href="#stack" class="headerlink" title="stack"></a>stack</h3><p>torch.stack() 是一个非常有用的函数，用于沿着指定的维度（dim）将多个张量（Tensor）合并成一个新的张量。它可以把一个包含多个张量的序列（比如一个列表或元组）按照给定的维度拼接成一个新的张量。</p><p>原型</p><blockquote><p>torch.stack(tensors, dim&#x3D;0, out&#x3D;None)→ Tensor</p></blockquote><p>tensors: 一个包含多个张量的序列（例如列表或元组）。这些张量必须具有相同的形状（即，除了要拼接的维度外，其他所有维度的大小必须相同）。<br>dim: 沿着哪个维度进行拼接。dim 参数决定了新张量的形状以及拼接方向。这个参数的值可以是负数，表示从最后一个维度开始倒数。<br>out: 可选，指定输出张量。如果没有提供，函数会自动生成一个新的张量。</p><p>补充：关于dim，假设原始的tensor.size() &#x3D; ([x,y,z])，参与拼接的tensor共有a个，dim&#x3D;0，1，2，3的结果分别为：</p><ol><li>tensor.size() &#x3D; ([a,x,y,z])</li><li>tensor.size() &#x3D; ([x,a,y,z])</li><li>tensor.size() &#x3D; ([x,y,a,z])</li><li>tensor.size() &#x3D; ([x,y,z,a])</li></ol><p>相应的tensor的“形状”也会有所改变，但是其中包含的信息却是没有改变的，而dim &#x3D; 0是比较容易理解也是比较常用的</p><p><a href="https://blog.csdn.net/qq_40507857/article/details/119854085">CSDN</a></p><h3 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h3><p>在给定维度上拼接输入序列 tensors 中的张量。所有张量除了拼接维度外必须具有相同的形状，或者是一个大小为 (0,) 的一维空张量。</p><p>原型</p><blockquote><p>torch.cat(tensors, dim&#x3D;0, *, out&#x3D;None) → Tensor</p></blockquote><p>tensors (Tensor 序列) – 提供的非空张量除了拼接维度外必须具有相同的形状。</p><p>dim (int, 可选) – 拼接张量的维度</p><p>out (Tensor, 可选) – 输出张量。</p><p>示例</p><pre><code>&gt;&gt;&gt; x = torch.randn(2, 3)&gt;&gt;&gt; xtensor([[ 0.6580, -1.0969, -0.4614],    [-0.1034, -0.5790,  0.1497]])&gt;&gt;&gt; torch.cat((x, x, x), 0)tensor([[ 0.6580, -1.0969, -0.4614],    [-0.1034, -0.5790,  0.1497],    [ 0.6580, -1.0969, -0.4614],    [-0.1034, -0.5790,  0.1497],    [ 0.6580, -1.0969, -0.4614],    [-0.1034, -0.5790,  0.1497]])&gt;&gt;&gt; torch.cat((x, x, x), 1)tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,     -1.0969, -0.4614],    [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,     -0.5790,  0.1497]])</code></pre><p>注意其与stack的区别，它不会增加新的维度，而是在指定的维度上将所有的tensor拼接到一起</p><h3 id="torch-autograd-grad"><a href="#torch-autograd-grad" class="headerlink" title="torch.autograd.grad()"></a>torch.autograd.grad()</h3><p><a href="https://zhuanlan.zhihu.com/p/279758736">知乎</a></p><h2 id="Experiment：toy"><a href="#Experiment：toy" class="headerlink" title="Experiment：toy"></a>Experiment：toy</h2><h3 id="train-py"><a href="#train-py" class="headerlink" title="train.py"></a>train.py</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> wandb</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> ArgumentParser</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> experiments.toy.problem <span class="keyword">import</span> Toy</span><br><span class="line"><span class="keyword">from</span> experiments.toy.utils <span class="keyword">import</span> plot_2d_pareto</span><br><span class="line"><span class="keyword">from</span> experiments.utils <span class="keyword">import</span> (</span><br><span class="line">    common_parser,</span><br><span class="line">    extract_weight_method_parameters_from_args,</span><br><span class="line">    set_logger,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> methods.weight_methods <span class="keyword">import</span> WeightMethods</span><br><span class="line"></span><br><span class="line">set_logger()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">method_type, device, n_iter, scale</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选定MTL参数θ更新方法</span></span><br><span class="line">    weight_methods_parameters = extract_weight_method_parameters_from_args(args)</span><br><span class="line">    n_tasks = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 确定任务函数</span></span><br><span class="line">    F = Toy(scale=scale)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用于描述优化过程的损失函数变化的轨迹</span></span><br><span class="line">    all_traj = <span class="built_in">dict</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the initial positions（初始参数）</span></span><br><span class="line">    inits = [</span><br><span class="line">        torch.Tensor([-<span class="number">8.5</span>, <span class="number">7.5</span>]),</span><br><span class="line">        torch.Tensor([<span class="number">0.0</span>, <span class="number">0.0</span>]),</span><br><span class="line">        torch.Tensor([<span class="number">9.0</span>, <span class="number">9.0</span>]),</span><br><span class="line">        torch.Tensor([-<span class="number">7.5</span>, -<span class="number">0.5</span>]),</span><br><span class="line">        torch.Tensor([<span class="number">9</span>, -<span class="number">1.0</span>]),</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, init <span class="keyword">in</span> <span class="built_in">enumerate</span>(inits):</span><br><span class="line">        <span class="comment"># init对应每个初始参数，i用于索引轨迹存储在all_traj中的位置</span></span><br><span class="line">        traj = []</span><br><span class="line">        x = init.clone()</span><br><span class="line">        x.requires_grad = <span class="literal">True</span></span><br><span class="line">        x = x.to(device)</span><br><span class="line"></span><br><span class="line">        method = WeightMethods(</span><br><span class="line">            method=method_type,</span><br><span class="line">            device=device,</span><br><span class="line">            n_tasks=n_tasks,</span><br><span class="line">            **weight_methods_parameters[method_type],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 优化器初始化，确定了其要同时优化x（任务的参数）、具体method的参数；lr是学习率</span></span><br><span class="line">        optimizer = torch.optim.Adam(</span><br><span class="line">            [</span><br><span class="line">                <span class="built_in">dict</span>(params=[x], lr=<span class="number">1e-3</span>),</span><br><span class="line">                <span class="built_in">dict</span>(params=method.parameters(), lr=args.method_params_lr),</span><br><span class="line">            ],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(n_iter)):</span><br><span class="line">            traj.append(x.cpu().detach().numpy().copy())</span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            f = F(x, <span class="literal">False</span>) <span class="comment"># 计算损失函数</span></span><br><span class="line">            <span class="comment"># 根据选定方法，“反向传播”更新参数，我在复现时用的nashmtl</span></span><br><span class="line">            _ = method.backward( </span><br><span class="line">                losses=f,</span><br><span class="line">                shared_parameters=(x,),</span><br><span class="line">                task_specific_parameters=<span class="literal">None</span>,</span><br><span class="line">                last_shared_parameters=<span class="literal">None</span>,</span><br><span class="line">                representation=<span class="literal">None</span>,</span><br><span class="line">            )</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">        all_traj[i] = <span class="built_in">dict</span>(init=init.cpu().detach().numpy().copy(), traj=np.array(traj))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> all_traj</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    parser = ArgumentParser(</span><br><span class="line">        <span class="string">&quot;Toy example (modification of the one in CAGrad)&quot;</span>, parents=[common_parser]</span><br><span class="line">    )</span><br><span class="line">    parser.set_defaults(n_epochs=<span class="number">35000</span>, method=<span class="string">&quot;nashmtl&quot;</span>, data_path=<span class="literal">None</span>)</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--scale&quot;</span>, default=<span class="number">1e-1</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, <span class="built_in">help</span>=<span class="string">&quot;scale for first loss&quot;</span></span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--out-path&quot;</span>, default=<span class="string">&quot;outputs&quot;</span>, <span class="built_in">type</span>=Path, <span class="built_in">help</span>=<span class="string">&quot;output path&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--wandb_project&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="literal">None</span>, <span class="built_in">help</span>=<span class="string">&quot;Name of Weights &amp; Biases Project.&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--wandb_entity&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="literal">None</span>, <span class="built_in">help</span>=<span class="string">&quot;Name of Weights &amp; Biases Entity.&quot;</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.wandb_project <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        wandb.init(project=args.wandb_project, entity=args.wandb_entity, config=args)</span><br><span class="line"></span><br><span class="line">    out_path = args.out_path</span><br><span class="line">    out_path.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    logging.info(<span class="string">f&quot;Logs and plots are saved in: <span class="subst">&#123;out_path.as_posix()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    all_traj = main(</span><br><span class="line">        method_type=args.method, device=device, n_iter=args.n_epochs, scale=args.scale</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot</span></span><br><span class="line">    ax, fig, legend = plot_2d_pareto(trajectories=all_traj, scale=args.scale)</span><br><span class="line"></span><br><span class="line">    title_map = &#123;</span><br><span class="line">        <span class="string">&quot;nashmtl&quot;</span>: <span class="string">&quot;Nash-MTL(Ours)&quot;</span>,</span><br><span class="line">        <span class="string">&quot;cagrad&quot;</span>: <span class="string">&quot;CAGrad&quot;</span>,</span><br><span class="line">        <span class="string">&quot;mgda&quot;</span>: <span class="string">&quot;MGDA&quot;</span>,</span><br><span class="line">        <span class="string">&quot;pcgrad&quot;</span>: <span class="string">&quot;PCGrad&quot;</span>,</span><br><span class="line">        <span class="string">&quot;ls&quot;</span>: <span class="string">&quot;LS&quot;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">    ax.set_title(title_map[args.method], fontsize=<span class="number">25</span>)</span><br><span class="line">    plt.savefig(</span><br><span class="line">        out_path / <span class="string">f&quot;<span class="subst">&#123;args.method&#125;</span>.png&quot;</span>,</span><br><span class="line">        bbox_extra_artists=(legend,),</span><br><span class="line">        bbox_inches=<span class="string">&quot;tight&quot;</span>,</span><br><span class="line">        facecolor=<span class="string">&quot;white&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    plt.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> wandb.run <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        wandb.log(&#123;<span class="string">&quot;Pareto Front&quot;</span>: wandb.Image((out_path / <span class="string">f&quot;<span class="subst">&#123;args.method&#125;</span>.png&quot;</span>).as_posix())&#125;)</span><br><span class="line"></span><br><span class="line">        wandb.finish()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="problem-py"><a href="#problem-py" class="headerlink" title="problem.py"></a>problem.py</h3><p>该模块代码如下</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#这个模型定义了两个损失函数（或目标函数） f1 和 f2，它们是输入变量 $x = [x_1, x_2]$的非线性函数，并允许计算梯度信息。</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="comment">#nn 是 PyTorch 中用于构建神经网络的模块，包括 Module、常用的层（如 Linear、Conv2d 等）和常用的损失函数等。</span></span><br><span class="line">LOWER = <span class="number">0.000005</span></span><br><span class="line"><span class="comment">#定义一个常量 LOWER，用于在计算中对一些值进行下限裁剪。这个常量用于避免对数计算中的负值或零（log(0)），从而防止梯度爆炸或 NaN 问题。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Toy</span>(nn.Module):</span><br><span class="line">    <span class="comment">#定义了一个名为 Toy 的类，继承自 PyTorch 中的 nn.Module。nn.Module 是所有神经网络模块的基类，提供了很多通用功能，如参数注册、反向传播等。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, scale=<span class="number">1.0</span>, scale_both_losses=<span class="number">1.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Toy, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.centers = torch.Tensor([[-<span class="number">3.0</span>, <span class="number">0</span>], [<span class="number">3.0</span>, <span class="number">0</span>]])</span><br><span class="line">        <span class="variable language_">self</span>.scale = scale</span><br><span class="line">        <span class="variable language_">self</span>.scale_both_losses = scale_both_losses</span><br><span class="line">    <span class="comment"># centers 属性，是一个 2x2 的张量，代表两个中心点的坐标 [-3.0, 0] 和 [3.0, 0]。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, compute_grad=<span class="literal">False</span></span>):</span><br><span class="line">        x1 = x[<span class="number">0</span>]</span><br><span class="line">        x2 = x[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        f1 = torch.clamp((<span class="number">0.5</span> * (-x1 - <span class="number">7</span>) - torch.tanh(-x2)).<span class="built_in">abs</span>(), LOWER).log() + <span class="number">6</span></span><br><span class="line">        f2 = torch.clamp((<span class="number">0.5</span> * (-x1 + <span class="number">3</span>) + torch.tanh(-x2) + <span class="number">2</span>).<span class="built_in">abs</span>(), LOWER).log() + <span class="number">6</span></span><br><span class="line">        c1 = torch.clamp(torch.tanh(x2 * <span class="number">0.5</span>), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        f1_sq = ((-x1 + <span class="number">7</span>).<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.1</span> * (-x2 - <span class="number">8</span>).<span class="built_in">pow</span>(<span class="number">2</span>)) / <span class="number">10</span> - <span class="number">20</span></span><br><span class="line">        f2_sq = ((-x1 - <span class="number">7</span>).<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.1</span> * (-x2 - <span class="number">8</span>).<span class="built_in">pow</span>(<span class="number">2</span>)) / <span class="number">10</span> - <span class="number">20</span></span><br><span class="line">        c2 = torch.clamp(torch.tanh(-x2 * <span class="number">0.5</span>), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        f1 = f1 * c1 + f1_sq * c2</span><br><span class="line">        f1 *= <span class="variable language_">self</span>.scale</span><br><span class="line">        f2 = f2 * c1 + f2_sq * c2</span><br><span class="line"></span><br><span class="line">        f = torch.stack([f1, f2]) * <span class="variable language_">self</span>.scale_both_losses</span><br><span class="line">        <span class="keyword">if</span> compute_grad:</span><br><span class="line">            g11 = torch.autograd.grad(f1, x1, retain_graph=<span class="literal">True</span>)[<span class="number">0</span>].item()</span><br><span class="line">            g12 = torch.autograd.grad(f1, x2, retain_graph=<span class="literal">True</span>)[<span class="number">0</span>].item()</span><br><span class="line">            g21 = torch.autograd.grad(f2, x1, retain_graph=<span class="literal">True</span>)[<span class="number">0</span>].item()</span><br><span class="line">            g22 = torch.autograd.grad(f2, x2, retain_graph=<span class="literal">True</span>)[<span class="number">0</span>].item()</span><br><span class="line">            g = torch.Tensor([[g11, g21], [g12, g22]])</span><br><span class="line">            <span class="keyword">return</span> f, g</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> f</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">batch_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x1 = x[:, <span class="number">0</span>]</span><br><span class="line">        x2 = x[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        f1 = torch.clamp((<span class="number">0.5</span> * (-x1 - <span class="number">7</span>) - torch.tanh(-x2)).<span class="built_in">abs</span>(), LOWER).log() + <span class="number">6</span></span><br><span class="line">        f2 = torch.clamp((<span class="number">0.5</span> * (-x1 + <span class="number">3</span>) + torch.tanh(-x2) + <span class="number">2</span>).<span class="built_in">abs</span>(), LOWER).log() + <span class="number">6</span></span><br><span class="line">        c1 = torch.clamp(torch.tanh(x2 * <span class="number">0.5</span>), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        f1_sq = ((-x1 + <span class="number">7</span>).<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.1</span> * (-x2 - <span class="number">8</span>).<span class="built_in">pow</span>(<span class="number">2</span>)) / <span class="number">10</span> - <span class="number">20</span></span><br><span class="line">        f2_sq = ((-x1 - <span class="number">7</span>).<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.1</span> * (-x2 - <span class="number">8</span>).<span class="built_in">pow</span>(<span class="number">2</span>)) / <span class="number">10</span> - <span class="number">20</span></span><br><span class="line">        c2 = torch.clamp(torch.tanh(-x2 * <span class="number">0.5</span>), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        f1 = f1 * c1 + f1_sq * c2</span><br><span class="line">        f1 *= <span class="variable language_">self</span>.scale</span><br><span class="line">        f2 = f2 * c1 + f2_sq * c2</span><br><span class="line"></span><br><span class="line">        f = torch.cat([f1.view(-<span class="number">1</span>, <span class="number">1</span>), f2.view(-<span class="number">1</span>, <span class="number">1</span>)], -<span class="number">1</span>) * <span class="variable language_">self</span>.scale_both_losses</span><br><span class="line">        <span class="keyword">return</span> f</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Nash-MTL"><a href="#Nash-MTL" class="headerlink" title="Nash-MTL"></a>Nash-MTL</h3><p>该类的目标是通过一个优化问题来计算每个任务的权重（alpha），从而实现加权多任务学习。具体来说，该方法使用的是一个基于纳什均衡的算法来计算任务之间的加权关系。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NashMTL</span>(<span class="title class_ inherited__">WeightMethod</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        n_tasks: <span class="built_in">int</span>, <span class="comment">#任务数量</span></span></span><br><span class="line"><span class="params">        device: torch.device, <span class="comment">#设备类型</span></span></span><br><span class="line"><span class="params">        max_norm: <span class="built_in">float</span> = <span class="number">1.0</span>, <span class="comment">#最大范数，用于梯度裁剪</span></span></span><br><span class="line"><span class="params">        update_weights_every: <span class="built_in">int</span> = <span class="number">1</span>, <span class="comment">#更新任务权重的频率</span></span></span><br><span class="line"><span class="params">        optim_niter=<span class="number">20</span>, <span class="comment">#优化器的迭代次数</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(NashMTL, <span class="variable language_">self</span>).__init__(</span><br><span class="line">            n_tasks=n_tasks,</span><br><span class="line">            device=device,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这是规范的Py成员变量的定义方法（不需要显示声明，直接在__init__中用self.variable赋值即可）</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.optim_niter = optim_niter</span><br><span class="line">        <span class="variable language_">self</span>.update_weights_every = update_weights_every</span><br><span class="line">        <span class="variable language_">self</span>.max_norm = max_norm</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.prvs_alpha_param = <span class="literal">None</span> <span class="comment"># 上次更新的权重参数，赋值为None供后续使用</span></span><br><span class="line">        <span class="variable language_">self</span>.normalization_factor = np.ones((<span class="number">1</span>,)) <span class="comment"># 单一元素为1的素质，标准化处理用</span></span><br><span class="line">        <span class="variable language_">self</span>.init_gtg = <span class="variable language_">self</span>.init_gtg = np.eye(<span class="variable language_">self</span>.n_tasks) <span class="comment"># 对应后面的GTG，这里初始化为了单位矩阵</span></span><br><span class="line">        <span class="variable language_">self</span>.step = <span class="number">0.0</span></span><br><span class="line">        <span class="variable language_">self</span>.prvs_alpha = np.ones(<span class="variable language_">self</span>.n_tasks, dtype=np.float32) <span class="comment"># 上次更新的权重，初始化为全1的数组，初始权重相同</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>此方法用于判断是否满足停止条件。具体而言，它检查以下几种情况：</p><ol><li>alpha_param 为 None。</li><li>当前的 gtg 与 alpha_t 计算结果的范数小于某个阈值。</li><li>当前的 alpha_param 与 prvs_alpha_param 之间的范数小于某个阈值。</li></ol><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_stop_criteria</span>(<span class="params">self, gtg, alpha_t</span>):</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        (<span class="variable language_">self</span>.alpha_param.value <span class="keyword">is</span> <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">or</span> (np.linalg.norm(gtg @ alpha_t - <span class="number">1</span> / (alpha_t + <span class="number">1e-10</span>)) &lt; <span class="number">1e-3</span>)</span><br><span class="line">        <span class="keyword">or</span> (</span><br><span class="line">            np.linalg.norm(<span class="variable language_">self</span>.alpha_param.value - <span class="variable language_">self</span>.prvs_alpha_param.value)</span><br><span class="line">            &lt; <span class="number">1e-6</span></span><br><span class="line">        )</span><br><span class="line">    )  </span><br></pre></td></tr></table></figure><p>这个方法是优化的核心，目标是求解 alpha_t，即各个任务的权重。优化过程使用了一个迭代的方法，每次通过求解优化问题来更新 alpha_t，直到满足停止条件为止。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">solve_optimization</span>(<span class="params">self, gtg: np.array</span>):</span><br><span class="line">    <span class="variable language_">self</span>.G_param.value = gtg <span class="comment"># </span></span><br><span class="line">    <span class="variable language_">self</span>.normalization_factor_param.value = <span class="variable language_">self</span>.normalization_factor <span class="comment"># 对应使用的标准化因子，方便计算机处理</span></span><br><span class="line"></span><br><span class="line">    alpha_t = <span class="variable language_">self</span>.prvs_alpha</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里使用了迭代求解优化，warm_start=True让求解器利用前一次迭代的解来加速收敛，是一种求解器的使用方法，区别于论文中CCP方法迭代使用alpha，CCP的迭代是接触prvs_*参数来完成的</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.optim_niter):</span><br><span class="line">        <span class="variable language_">self</span>.alpha_param.value = alpha_t</span><br><span class="line">        <span class="variable language_">self</span>.prvs_alpha_param.value = alpha_t</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>: <span class="comment"># 前面在_init_optim_problem中定义了优化问题，这里进行求解</span></span><br><span class="line">            <span class="variable language_">self</span>.prob.solve(solver=cp.ECOS, warm_start=<span class="literal">True</span>, max_iters=<span class="number">100</span>)</span><br><span class="line">        <span class="keyword">except</span>: <span class="comment"># 求解器更新参数发生溢出时的异常处理</span></span><br><span class="line">            <span class="variable language_">self</span>.alpha_param.value = <span class="variable language_">self</span>.prvs_alpha_param.value</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>._stop_criteria(gtg, alpha_t):</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        alpha_t = <span class="variable language_">self</span>.alpha_param.value</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> alpha_t <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="variable language_">self</span>.prvs_alpha = alpha_t</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.prvs_alpha</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>此方法计算线性化的 phi_alpha，用于优化问题的目标函数中。具体的计算过程涉及到 G_param 和 alpha_param 的线性运算。</p><p>这里 prvs_phi_tag 对应的是在点 $\alpha^{(\tau)}$<br>  处的梯度 $\nabla \phi(\alpha^{(\tau)})$<br> ，其中：</p><p>$\frac{1}{\alpha^{(\tau)}}$是对 $\log(\alpha_i)$的导数</p><p>$\frac{1}{G \alpha^{(\tau)}} G$是对 $\log(\beta_i)$的导数</p><p>实际上在这里$\tilde{\phi_\tau}(\alpha) &#x3D; \phi(\alpha^{(\tau)})+\nabla \phi(\alpha^{(\tau)})^T(\alpha-\alpha^{(\tau)})$，只考虑了$\nabla \phi(\alpha^{(\tau)})^T(\alpha-\alpha^{(\tau)})$，因为前面是一个常数项，不影响优化的结果。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_calc_phi_alpha_linearization</span>(<span class="params">self</span>):</span><br><span class="line">    G_prvs_alpha = <span class="variable language_">self</span>.G_param @ <span class="variable language_">self</span>.prvs_alpha_param</span><br><span class="line">    prvs_phi_tag = <span class="number">1</span> / <span class="variable language_">self</span>.prvs_alpha_param + (<span class="number">1</span> / G_prvs_alpha) @ <span class="variable language_">self</span>.G_param</span><br><span class="line">    phi_alpha = prvs_phi_tag @ (<span class="variable language_">self</span>.alpha_param - <span class="variable language_">self</span>.prvs_alpha_param)</span><br><span class="line">    <span class="keyword">return</span> phi_alpha</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>此方法初始化优化问题，具体地说，使用 cvxpy 来定义一个凸优化问题。优化目标是最小化一个由加权任务损失和线性化项构成的目标函数。</p><p>参考论文中的公式：</p><p>$G^TG\alpha &#x3D; 1&#x2F;\alpha$</p><p>$\beta_i(\alpha) &#x3D; g_i^TG\alpha$</p><p>$\phi_i(\alpha) &#x3D; log(\alpha_i) + log(\beta_i),\phi(\alpha) &#x3D; \sum_i \phi_i(\alpha)$</p><p>$min_\alpha \sum_i\beta_i(\alpha) + \phi(\alpha),s.t. \forall i,-\phi_i(\alpha)&lt;0,\alpha_i&gt;0$</p><p>$\tilde{\phi_\tau}(\alpha) &#x3D; \phi(\alpha^{(\tau)})+\nabla \phi(\alpha^{(\tau)})^T(\alpha-\alpha^{(\tau)})$</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_init_optim_problem</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment"># 创建一个形状为 (self.n_tasks,) 的非负变量 alpha_param，代表各任务的权重。</span></span><br><span class="line">    <span class="variable language_">self</span>.alpha_param = cp.Variable(shape=(<span class="variable language_">self</span>.n_tasks,), nonneg=<span class="literal">True</span>)</span><br><span class="line">    <span class="variable language_">self</span>.prvs_alpha_param = cp.Parameter(</span><br><span class="line">        shape=(<span class="variable language_">self</span>.n_tasks,), value=<span class="variable language_">self</span>.prvs_alpha</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 创建一个形状为 (self.n_tasks, self.n_tasks) 的参数 G_param，初始化为梯度协方差矩阵 self.init_gtg。</span></span><br><span class="line">    <span class="variable language_">self</span>.G_param = cp.Parameter(</span><br><span class="line">        shape=(<span class="variable language_">self</span>.n_tasks, <span class="variable language_">self</span>.n_tasks), value=<span class="variable language_">self</span>.init_gtg</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 标准化用，但是从数学意义上来说没有什么意义，或许与计算机的数据处理有关（例如增强代码的鲁棒性、稳定性和可维护性...）</span></span><br><span class="line">    <span class="variable language_">self</span>.normalization_factor_param = cp.Parameter(</span><br><span class="line">        shape=(<span class="number">1</span>,), value=np.array([<span class="number">1.0</span>])</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 线性化项</span></span><br><span class="line">    <span class="variable language_">self</span>.phi_alpha = <span class="variable language_">self</span>._calc_phi_alpha_linearization()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个列向量对应论文中的βi，这里将其组成的矩阵，后续使用sum对应就是目标中的所有βi求和</span></span><br><span class="line">    G_alpha = <span class="variable language_">self</span>.G_param @ <span class="variable language_">self</span>.alpha_param <span class="comment"># gtg alpha</span></span><br><span class="line">    constraint = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.n_tasks):</span><br><span class="line">        constraint.append(</span><br><span class="line">            -cp.log(<span class="variable language_">self</span>.alpha_param[i] * <span class="variable language_">self</span>.normalization_factor_param)</span><br><span class="line">            - cp.log(G_alpha[i])</span><br><span class="line">            &lt;= <span class="number">0</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 优化目标</span></span><br><span class="line">    obj = cp.Minimize(</span><br><span class="line">        cp.<span class="built_in">sum</span>(G_alpha) + <span class="variable language_">self</span>.phi_alpha / <span class="variable language_">self</span>.normalization_factor_param</span><br><span class="line">    )</span><br><span class="line">    <span class="variable language_">self</span>.prob = cp.Problem(obj, constraint)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>此方法计算加权损失。首先计算每个任务的梯度（即 grads），然后计算梯度的协方差矩阵 GTG。使用 solve_optimization 方法求解最优的任务权重 alpha。最终返回加权损失和任务权重。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_weighted_loss</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    losses,</span></span><br><span class="line"><span class="params">    shared_parameters,</span></span><br><span class="line"><span class="params">    **kwargs,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    losses :</span></span><br><span class="line"><span class="string">    shared_parameters : shared parameters</span></span><br><span class="line"><span class="string">    kwargs :</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    extra_outputs = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.step == <span class="number">0</span>: <span class="comment"># 步数为0，初始化优化任务</span></span><br><span class="line">        <span class="variable language_">self</span>._init_optim_problem()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="variable language_">self</span>.step % <span class="variable language_">self</span>.update_weights_every) == <span class="number">0</span>: <span class="comment"># 记录更新步数</span></span><br><span class="line">        <span class="variable language_">self</span>.step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#  对每个任务的损失计算相对于共享参数的梯度，并将梯度展平成一维向量后存储在 grads 字典中。</span></span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i, loss <span class="keyword">in</span> <span class="built_in">enumerate</span>(losses):</span><br><span class="line">            g = <span class="built_in">list</span>(</span><br><span class="line">                torch.autograd.grad(</span><br><span class="line">                    loss,</span><br><span class="line">                    shared_parameters,</span><br><span class="line">                    retain_graph=<span class="literal">True</span>,</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">            grad = torch.cat([torch.flatten(grad) <span class="keyword">for</span> grad <span class="keyword">in</span> g])</span><br><span class="line">            grads[i] = grad</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度矩阵 G: 将所有任务的梯度堆叠成一个矩阵。</span></span><br><span class="line">        G = torch.stack(<span class="built_in">tuple</span>(v <span class="keyword">for</span> v <span class="keyword">in</span> grads.values()))</span><br><span class="line">        <span class="comment"># 梯度协方差矩阵 GTG: 计算梯度矩阵的自乘，得到梯度协方差矩阵。</span></span><br><span class="line">        GTG = torch.mm(G, G.t())</span><br><span class="line">        <span class="comment"># 标准化因子: 计算 GTG 的范数，并用其标准化 GTG。</span></span><br><span class="line">        <span class="variable language_">self</span>.normalization_factor = (</span><br><span class="line">            torch.norm(GTG).detach().cpu().numpy().reshape((<span class="number">1</span>,))</span><br><span class="line">        )</span><br><span class="line">        GTG = GTG / <span class="variable language_">self</span>.normalization_factor.item()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 采用优化的方法，在GTG的基础上求解各个损失函数的权重alpha</span></span><br><span class="line">        alpha = <span class="variable language_">self</span>.solve_optimization(GTG.cpu().detach().numpy())</span><br><span class="line">        alpha = torch.from_numpy(alpha)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="variable language_">self</span>.step += <span class="number">1</span></span><br><span class="line">        alpha = <span class="variable language_">self</span>.prvs_alpha</span><br><span class="line">    <span class="comment"># 这里我猜测是作者考虑到为了加快模型的训练可以，简单地通过在某些步数的时候，不按照正常的流程求权重，而是直接使用上次的权重；但是实际上没有这样做，因为update_weights_every是被设置为1的，无论step为多少，总是会按正常步骤更新alpha</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按照得到的权重alpha计算加权损失函数，供backward()使用</span></span><br><span class="line">    weighted_loss = <span class="built_in">sum</span>([losses[i] * alpha[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(alpha))])</span><br><span class="line">    extra_outputs[<span class="string">&quot;weights&quot;</span>] = alpha</span><br><span class="line">    <span class="keyword">return</span> weighted_loss, extra_outputs</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        losses: torch.Tensor,</span></span><br><span class="line"><span class="params">        shared_parameters: <span class="type">Union</span>[</span></span><br><span class="line"><span class="params">            <span class="type">List</span>[torch.nn.parameter.Parameter], torch.Tensor</span></span><br><span class="line"><span class="params">        ] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        task_specific_parameters: <span class="type">Union</span>[</span></span><br><span class="line"><span class="params">            <span class="type">List</span>[torch.nn.parameter.Parameter], torch.Tensor</span></span><br><span class="line"><span class="params">        ] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        last_shared_parameters: <span class="type">Union</span>[</span></span><br><span class="line"><span class="params">            <span class="type">List</span>[torch.nn.parameter.Parameter], torch.Tensor</span></span><br><span class="line"><span class="params">        ] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        representation: <span class="type">Union</span>[<span class="type">List</span>[torch.nn.parameter.Parameter], torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        **kwargs,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">Tuple</span>[<span class="type">Union</span>[torch.Tensor, <span class="literal">None</span>], <span class="type">Union</span>[<span class="type">Dict</span>, <span class="literal">None</span>]]:</span><br><span class="line">        loss, extra_outputs = <span class="variable language_">self</span>.get_weighted_loss(</span><br><span class="line">            losses=losses, <span class="comment"># 当前各损失函数</span></span><br><span class="line">            shared_parameters=shared_parameters, <span class="comment"># 确定待求梯度的损失函数的变量</span></span><br><span class="line">            **kwargs,</span><br><span class="line">        ) <span class="comment"># 得到加权后的损失函数，额外参数是各损失函数的权重信息</span></span><br><span class="line">        loss.backward() <span class="comment"># 对加权损失函数求梯度，从而确定更新方向，以供优化器更新参数使用（optimize.step()）</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># make sure the solution for shared params has norm &lt;= self.eps</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.max_norm &gt; <span class="number">0</span>:</span><br><span class="line">            torch.nn.utils.clip_grad_norm_(shared_parameters, <span class="variable language_">self</span>.max_norm)</span><br><span class="line"></span><br><span class="line"><span class="comment">#在训练过程中，backward 方法用于执行反向传播。它调用 get_weighted_loss 来计算加权损失，并使用 loss.backward() 进行梯度计算。如果 max_norm 大于 0，则会对梯度进行裁剪。</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, extra_outputs</span><br></pre></td></tr></table></figure><h3 id="result"><a href="#result" class="headerlink" title="result"></a>result</h3><p><img src="/../_images/result.png" alt="Nash-MTL" title="Nash-MTL"></p><h3 id="My-Test"><a href="#My-Test" class="headerlink" title="My_Test"></a>My_Test</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleLinearModel</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, output_dim</span>):</span><br><span class="line">        <span class="variable language_">self</span>.W = np.random.randn(input_dim, output_dim)</span><br><span class="line">        <span class="variable language_">self</span>.b = np.random.randn(output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> X @ <span class="variable language_">self</span>.W + <span class="variable language_">self</span>.b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">task_loss</span>(<span class="params">model, X, y_true</span>):</span><br><span class="line">    y_pred = model.predict(X)</span><br><span class="line">    <span class="keyword">return</span> np.mean((y_pred - y_true) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义两个任务的数据，这里作为简单的测试，我使用的随机生成</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X = np.random.randn(<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">y_A = X @ np.array([<span class="number">2</span>, -<span class="number">3</span>]) + <span class="number">1</span> + <span class="number">0.1</span> * np.random.randn(<span class="number">100</span>)</span><br><span class="line">y_B = X @ np.array([-<span class="number">1</span>, <span class="number">4</span>]) - <span class="number">2</span> + <span class="number">0.1</span> * np.random.randn(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型</span></span><br><span class="line">model = SimpleLinearModel(input_dim=<span class="number">2</span>, output_dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算每个任务的梯度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradients</span>(<span class="params">model, X, y_A, y_B</span>):</span><br><span class="line">    epsilon = <span class="number">1e-5</span></span><br><span class="line">    initial_loss_A = task_loss(model, X, y_A)</span><br><span class="line">    initial_loss_B = task_loss(model, X, y_B)</span><br><span class="line"></span><br><span class="line">    grad_W_A = np.zeros_like(model.W)</span><br><span class="line">    grad_b_A = np.zeros_like(model.b)</span><br><span class="line">    grad_W_B = np.zeros_like(model.W)</span><br><span class="line">    grad_b_B = np.zeros_like(model.b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(model.W.size):</span><br><span class="line">        model.W.flat[i] += epsilon</span><br><span class="line">        grad_W_A.flat[i] = (task_loss(model, X, y_A) - initial_loss_A) / epsilon</span><br><span class="line">        grad_W_B.flat[i] = (task_loss(model, X, y_B) - initial_loss_B) / epsilon</span><br><span class="line">        model.W.flat[i] -= epsilon</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(model.b.size):</span><br><span class="line">        model.b.flat[i] += epsilon</span><br><span class="line">        grad_b_A.flat[i] = (task_loss(model, X, y_A) - initial_loss_A) / epsilon</span><br><span class="line">        grad_b_B.flat[i] = (task_loss(model, X, y_B) - initial_loss_B) / epsilon</span><br><span class="line">        model.b.flat[i] -= epsilon</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (grad_W_A, grad_b_A), (grad_W_B, grad_b_B)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算Nash Bargaining Solution</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nash_bargaining_solution</span>(<span class="params">grad_A, grad_B</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">objective</span>(<span class="params">x</span>):</span><br><span class="line">        grad_W = x[<span class="number">0</span>] * grad_A[<span class="number">0</span>] + x[<span class="number">1</span>] * grad_B[<span class="number">0</span>]</span><br><span class="line">        grad_b = x[<span class="number">0</span>] * grad_A[<span class="number">1</span>] + x[<span class="number">1</span>] * grad_B[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> - np.<span class="built_in">sum</span>(grad_W ** <span class="number">2</span> + grad_b ** <span class="number">2</span>)  <span class="comment"># Minimize the negative of the combined gradient norms</span></span><br><span class="line"></span><br><span class="line">    constraints = &#123;</span><br><span class="line">        <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;eq&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;fun&#x27;</span>: <span class="keyword">lambda</span> x: np.<span class="built_in">sum</span>(x) - <span class="number">1</span>  <span class="comment"># x_A + x_B = 1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    bounds = [(<span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">1</span>)]</span><br><span class="line">    x0 = [<span class="number">0.5</span>, <span class="number">0.5</span>]</span><br><span class="line"></span><br><span class="line">    result = minimize(objective, x0, bounds=bounds, constraints=constraints)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> result.success:</span><br><span class="line">        <span class="keyword">return</span> result.x</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Optimization failed&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行一次更新</span></span><br><span class="line">(grad_W_A, grad_b_A), (grad_W_B, grad_b_B) = compute_gradients(model, X, y_A, y_B)</span><br><span class="line">nash_weights = nash_bargaining_solution((grad_W_A, grad_b_A), (grad_W_B, grad_b_B))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新模型参数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">model.W -= learning_rate * (nash_weights[<span class="number">0</span>] * grad_W_A + nash_weights[<span class="number">1</span>] * grad_W_B)</span><br><span class="line">model.b -= learning_rate * (nash_weights[<span class="number">0</span>] * grad_b_A + nash_weights[<span class="number">1</span>] * grad_b_B)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Updated W: <span class="subst">&#123;model.W&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Updated b: <span class="subst">&#123;model.b&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Experiment-quantum-chemistry"><a href="#Experiment-quantum-chemistry" class="headerlink" title="Experiment: quantum_chemistry"></a>Experiment: quantum_chemistry</h2><p>整个实验是基于QM9数据集中的原子-分子数据，训练了一个神经网络，用来根据分子的特征（原子个数、原子特征、原子之间的化学键连接情况、化学键的性质特征…），来对分子的性质进行一个预测。</p><h3 id="QM9"><a href="#QM9" class="headerlink" title="QM9"></a>QM9</h3><p>QM9数据集：一个广泛使用的化学分子数据集，主要用于机器学习和深度学习领域的化学和材料科学研究。它包含了约 134,000 个小分子的详细信息，每个分子包含的属性和标签数量丰富，是研究分子性质和行为的宝贵资源。</p><p>实验中的数据使用PyTorch Geometric框架来对原始数据进行预处理，转换为图格式的数据，得到数据集供我们使用。</p><h4 id="数据对象"><a href="#数据对象" class="headerlink" title="数据对象"></a>数据对象</h4><p>在 PyTorch Geometric 中，每个数据对象是 torch_geometric.data.Data 类型的实例</p><p>包含以下属性：</p><ol><li>x：节点特征矩阵（形状为 [num_nodes, num_node_features]）。</li><li>edge_index：边索引矩阵（形状为 [2, num_edges]），其中每列表示一条边的起始节点和终止节点。</li><li>edge_attr：边特征矩阵（形状为 [num_edges, num_edge_features]，可选）。</li><li>y：目标属性或标签（形状为 [num_targets]，可选）。</li><li>pos：节点的三维坐标（形状为 [num_nodes, 3]，可选）。</li><li>z：节点的原子序数（形状为 [num_nodes]）。</li><li>idx：分子索引。</li><li>name：分子名称（数据集中的标识）。</li></ol><p><strong>test的输出信息：Data(x&#x3D;[5, 11], edge_index&#x3D;[2, 8], edge_attr&#x3D;[8, 4], y&#x3D;[1, 11], pos&#x3D;[5, 3], idx&#x3D;[1], name&#x3D;’gdb_1’, z&#x3D;[5])</strong></p><h3 id="train-py-1"><a href="#train-py-1" class="headerlink" title="train.py"></a>train.py</h3><h4 id="main"><a href="#main" class="headerlink" title="main"></a>main</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params"></span></span><br><span class="line"><span class="params">    data_path: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">    batch_size: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    device: torch.device,</span></span><br><span class="line"><span class="params">    method: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">    weight_method_params: <span class="built_in">dict</span>,</span></span><br><span class="line"><span class="params">    lr: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">    method_params_lr: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">    n_epochs: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    targets: <span class="built_in">list</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    scale_target: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    main_task: <span class="built_in">int</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="comment"># 模型初始化，训练的是一个网络，其中任务的数量n_tasks对应输出头的个数；num_features是输入数据的特征维度，这里设置为11对应quantum_chemistry数据集中一个分子的相关性质的量化指标有11个；dim确定了隐层的维度；（相关细节在model.py）</span></span><br><span class="line">    <span class="comment"># to(device)可以将模型移动到指定的计算设备，保存相关数据并执行计算操作</span></span><br><span class="line">    dim = <span class="number">64</span></span><br><span class="line">    model = Net(n_tasks=<span class="built_in">len</span>(targets), num_features=<span class="number">11</span>, dim=dim).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据转换和加载（相关细节在utils.py）</span></span><br><span class="line">    transform = T.Compose([MyTransform(targets), Complete(), T.Distance(norm=<span class="literal">False</span>)])</span><br><span class="line">    dataset = QM9(data_path, transform=transform).shuffle()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 拆分数据集</span></span><br><span class="line">    test_dataset = dataset[:<span class="number">10000</span>]</span><br><span class="line">    val_dataset = dataset[<span class="number">10000</span>:<span class="number">20000</span>]</span><br><span class="line">    train_dataset = dataset[<span class="number">20000</span>:]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果需要，进行目标值的归一化</span></span><br><span class="line">    std = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> scale_target:</span><br><span class="line">        mean = train_dataset.data.y[:, targets].mean(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = train_dataset.data.y[:, targets].std(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        dataset.data.y[:, targets] = (dataset.data.y[:, targets] - mean) / std</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建数据加载器</span></span><br><span class="line">    test_loader = DataLoader(</span><br><span class="line">        test_dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line">    val_loader = DataLoader(</span><br><span class="line">        val_dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line">    train_loader = DataLoader(</span><br><span class="line">        train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化多任务学习的权重方法</span></span><br><span class="line">    weight_method = WeightMethods(</span><br><span class="line">        method,</span><br><span class="line">        n_tasks=<span class="built_in">len</span>(targets),</span><br><span class="line">        device=device,</span><br><span class="line">        **weight_method_params[method],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置优化器，并为模型和权重方法参数设置不同的学习率</span></span><br><span class="line">    optimizer = torch.optim.Adam(</span><br><span class="line">        [</span><br><span class="line">            <span class="built_in">dict</span>(params=model.parameters(), lr=lr),</span><br><span class="line">            <span class="built_in">dict</span>(params=weight_method.parameters(), lr=method_params_lr),</span><br><span class="line">        ],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习率调度器</span></span><br><span class="line">    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(</span><br><span class="line">        optimizer, mode=<span class="string">&quot;min&quot;</span>, factor=<span class="number">0.7</span>, patience=<span class="number">5</span>, min_lr=<span class="number">0.00001</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练循环</span></span><br><span class="line">    epoch_iterator = trange(n_epochs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用于保存最佳验证损失、最佳测试损失及其变化量的变量，初始值设为正无穷大。</span></span><br><span class="line">    best_val = np.inf</span><br><span class="line">    best_test = np.inf</span><br><span class="line">    best_test_delta = np.inf</span><br><span class="line">    best_val_delta = np.inf</span><br><span class="line">    <span class="comment"># 用于保存最佳测试结果的变量</span></span><br><span class="line">    best_test_results = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> epoch_iterator:</span><br><span class="line">        lr = scheduler.optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>] <span class="comment"># 从学习率调度器（scheduler）中获取当前学习率</span></span><br><span class="line">        <span class="keyword">for</span> j, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):  <span class="comment"># 遍历训练数据加载器（train_loader）</span></span><br><span class="line">            model.train() </span><br><span class="line"></span><br><span class="line">            data = data.to(device)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">            out, features = model(data, return_representation=<span class="literal">True</span>) <span class="comment"># 模型前向传播，进行估计</span></span><br><span class="line"></span><br><span class="line">            losses = F.mse_loss(out, data.y, reduction=<span class="string">&quot;none&quot;</span>).mean(<span class="number">0</span>) <span class="comment"># 估计结果与真实结果的平方误差作为损失函数</span></span><br><span class="line"></span><br><span class="line">            loss, extra_outputs = weight_method.backward(</span><br><span class="line">                losses=losses,</span><br><span class="line">                shared_parameters=<span class="built_in">list</span>(model.shared_parameters()),</span><br><span class="line">                task_specific_parameters=<span class="built_in">list</span>(model.task_specific_parameters()),</span><br><span class="line">                last_shared_parameters=<span class="built_in">list</span>(model.last_shared_parameters()),</span><br><span class="line">                representation=features,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 在验证集和测试集上评估模型（细节在def evaluate中）</span></span><br><span class="line">        val_loss_dict = evaluate(model, val_loader, std=std, scale_target=scale_target)</span><br><span class="line">        test_loss_dict = evaluate(</span><br><span class="line">            model, test_loader, std=std, scale_target=scale_target</span><br><span class="line">        )</span><br><span class="line">        val_loss = val_loss_dict[<span class="string">&quot;avg_loss&quot;</span>]</span><br><span class="line">        val_delta = val_loss_dict[<span class="string">&quot;delta_m&quot;</span>]</span><br><span class="line">        test_loss = test_loss_dict[<span class="string">&quot;avg_loss&quot;</span>]</span><br><span class="line">        test_delta = test_loss_dict[<span class="string">&quot;delta_m&quot;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据验证集性能更新最佳结果</span></span><br><span class="line">        <span class="keyword">if</span> method == <span class="string">&quot;stl&quot;</span>: <span class="comment"># 单任务学习</span></span><br><span class="line">            best_val_criteria = val_loss_dict[<span class="string">&quot;avg_task_losses&quot;</span>][main_task] &lt;= best_val</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 多任务学习</span></span><br><span class="line">            best_val_criteria = val_delta &lt;= best_val_delta <span class="comment"># 判断是否需要更新最佳结果，验证集上delta_m是否小于最好的delta_m</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> best_val_criteria:</span><br><span class="line">            best_val = val_loss</span><br><span class="line">            best_test = test_loss</span><br><span class="line">            best_test_results = test_loss_dict</span><br><span class="line">            best_val_delta = val_delta</span><br><span class="line">            best_test_delta = test_delta</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每个epoch记录日志</span></span><br><span class="line">        epoch_iterator.set_description(</span><br><span class="line">            <span class="string">f&quot;epoch <span class="subst">&#123;epoch&#125;</span> | lr=<span class="subst">&#123;lr&#125;</span> | train loss <span class="subst">&#123;losses.mean().item():<span class="number">.3</span>f&#125;</span> | val loss: <span class="subst">&#123;val_loss:<span class="number">.3</span>f&#125;</span> | &quot;</span></span><br><span class="line">            <span class="string">f&quot;test loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | best test loss <span class="subst">&#123;best_test:<span class="number">.3</span>f&#125;</span> | best_test_delta <span class="subst">&#123;best_test_delta:<span class="number">.3</span>f&#125;</span>&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> wandb.run <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            wandb.log(&#123;<span class="string">&quot;Learning Rate&quot;</span>: lr&#125;, step=epoch)</span><br><span class="line">            wandb.log(&#123;<span class="string">&quot;Train Loss&quot;</span>: losses.mean().item()&#125;, step=epoch)</span><br><span class="line">            wandb.log(&#123;<span class="string">&quot;Val Loss&quot;</span>: val_loss&#125;, step=epoch)</span><br><span class="line">            wandb.log(&#123;<span class="string">&quot;Val Delta&quot;</span>: val_delta&#125;, step=epoch)</span><br><span class="line">            wandb.log(&#123;<span class="string">&quot;Test Loss&quot;</span>: test_loss&#125;, step=epoch)</span><br><span class="line">            wandb.log(&#123;<span class="string">&quot;Test Delta&quot;</span>: test_delta&#125;, step=epoch)</span><br><span class="line">            wandb.log(&#123;<span class="string">&quot;Best Test Loss&quot;</span>: best_test&#125;, step=epoch)</span><br><span class="line">            wandb.log(&#123;<span class="string">&quot;Best Test Delta&quot;</span>: best_test_delta&#125;, step=epoch)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据验证集性能调整学习率</span></span><br><span class="line">        scheduler.step(</span><br><span class="line">            val_loss_dict[<span class="string">&quot;avg_task_losses&quot;</span>][main_task]</span><br><span class="line">            <span class="keyword">if</span> method == <span class="string">&quot;stl&quot;</span></span><br><span class="line">            <span class="keyword">else</span> val_delta</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 脚本入口点</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 参数解析器设置</span></span><br><span class="line">    parser = ArgumentParser(<span class="string">&quot;QM9&quot;</span>, parents=[common_parser])</span><br><span class="line">    parser.set_defaults(</span><br><span class="line">        data_path=<span class="string">&quot;dataset&quot;</span>,</span><br><span class="line">        lr=<span class="number">1e-3</span>,</span><br><span class="line">        n_epochs=<span class="number">300</span>,</span><br><span class="line">        batch_size=<span class="number">120</span>,</span><br><span class="line">        method=<span class="string">&quot;nashmtl&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--scale-y&quot;</span>, default=<span class="literal">True</span>, <span class="built_in">type</span>=str2bool)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--wandb_project&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="literal">None</span>, <span class="built_in">help</span>=<span class="string">&quot;Name of Weights &amp; Biases Project.&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--wandb_entity&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="literal">None</span>, <span class="built_in">help</span>=<span class="string">&quot;Name of Weights &amp; Biases Entity.&quot;</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置随机种子以确保可重复性</span></span><br><span class="line">    set_seed(args.seed)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果指定了项目，则初始化Weights &amp; Biases</span></span><br><span class="line">    <span class="keyword">if</span> args.wandb_project <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        wandb.init(project=args.wandb_project, entity=args.wandb_entity, config=args)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取权重方法的参数</span></span><br><span class="line">    weight_method_params = extract_weight_method_parameters_from_args(args)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取计算设备（CPU或GPU）</span></span><br><span class="line">    device = get_device(gpus=args.gpu)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 运行主训练和评估函数</span></span><br><span class="line">    main(</span><br><span class="line">        data_path=args.data_path,</span><br><span class="line">        batch_size=args.batch_size,</span><br><span class="line">        device=device,</span><br><span class="line">        method=args.method,</span><br><span class="line">        weight_method_params=weight_method_params,</span><br><span class="line">        lr=args.lr,</span><br><span class="line">        method_params_lr=args.method_params_lr,</span><br><span class="line">        n_epochs=args.n_epochs,</span><br><span class="line">        targets=targets,</span><br><span class="line">        scale_target=args.scale_y,</span><br><span class="line">        main_task=args.main_task,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 完成Weights &amp; Biases运行</span></span><br><span class="line">    <span class="keyword">if</span> wandb.run <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        wandb.finish()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="evaluate"><a href="#evaluate" class="headerlink" title="evaluate"></a>evaluate</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">model, loader, std, scale_target</span>): <span class="comment"># 评估模型；数据加载其；标准差；bool值</span></span><br><span class="line">    model.<span class="built_in">eval</span>() <span class="comment"># 模型切换为评估模式</span></span><br><span class="line">    data_size = <span class="number">0.0</span></span><br><span class="line">    task_losses = <span class="number">0.0</span> <span class="comment"># 用来累计各个任务的总损失</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):</span><br><span class="line">        data = data.to(device)</span><br><span class="line">        out = model(data)</span><br><span class="line">        <span class="keyword">if</span> scale_target:</span><br><span class="line">            task_losses += F.l1_loss(</span><br><span class="line">                out * std.to(device), data.y * std.to(device), reduction=<span class="string">&quot;none&quot;</span></span><br><span class="line">            ).<span class="built_in">sum</span>(</span><br><span class="line">                <span class="number">0</span></span><br><span class="line">            )  <span class="comment"># 平均绝对误差（MAE），考虑了先按标准差进行放缩</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            task_losses += F.l1_loss(out, data.y, reduction=<span class="string">&quot;none&quot;</span>).<span class="built_in">sum</span>(<span class="number">0</span>)  <span class="comment"># 平均绝对误差（MAE）</span></span><br><span class="line">        data_size += <span class="built_in">len</span>(data.y)</span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    avg_task_losses = task_losses / data_size <span class="comment"># 平均任务损失</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将误差从电子伏特（eV）转换为毫电子伏特（meV）</span></span><br><span class="line">    avg_task_losses = avg_task_losses.detach().cpu().numpy()</span><br><span class="line">    avg_task_losses[multiply_indx] *= <span class="number">1000</span> <span class="comment"># multiply_indx定义于utils中，或许是索引了单位是eV的物理化学量</span></span><br><span class="line"></span><br><span class="line">    delta_m = delta_fn(avg_task_losses)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">        avg_loss=avg_task_losses.mean(), <span class="comment"># 所有任务平均损失</span></span><br><span class="line">        avg_task_losses=avg_task_losses, <span class="comment"># 每个任务的平均损失</span></span><br><span class="line">        delta_m=delta_m, <span class="comment"># MTL模型在各个任务上的平均相对误差百分比，基于STL模型的基准表现。这个百分比表示MTL模型相较于STL基准的整体性能提升或退步。</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure><h3 id="model-py"><a href="#model-py" class="headerlink" title="model.py"></a>model.py</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 这个类定义了一个用于量子化学任务的神经网络模型，并使用了图神经网络（GNN）的相关模块来处理图结构数据。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_tasks, num_features=<span class="number">11</span>, dim=<span class="number">64</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.n_tasks = n_tasks</span><br><span class="line">        <span class="variable language_">self</span>.dim = dim</span><br><span class="line">        <span class="variable language_">self</span>.lin0 = torch.nn.Linear(num_features, dim) <span class="comment"># 初始化了一个线性层，用来把输入数据维度从num_features转换为dim</span></span><br><span class="line"></span><br><span class="line">        nn = Sequential(Linear(<span class="number">5</span>, <span class="number">128</span>), ReLU(), Linear(<span class="number">128</span>, dim * dim)) <span class="comment"># 序列模块，用来生成卷积核的权重（使用到一个有序容器，其中包含两个线性层和一个ReLU层）</span></span><br><span class="line">        <span class="comment"># 在 GNN 中，卷积核用于在图结构上进行卷积操作。它们会在图的每个节点及其邻居节点之间滑动，聚合和更新节点特征。卷积核在 GNN 中的作用类似于在图像处理中提取局部特征，但其操作对象是节点及其邻居。其形状可以是多维的，如d×d，其中d是节点特征的维度。</span></span><br><span class="line">        <span class="comment"># 卷积核的权重（Weights of Convolutional Kernel）是卷积核中的元素值。它们是模型训练过程中需要学习的参数。卷积核的权重直接影响卷积操作的结果，从而影响特征提取的效果。</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.conv = NNConv(dim, dim, nn, aggr=<span class="string">&quot;mean&quot;</span>) <span class="comment"># NNConv卷积层，输入输出维度都是dim；使用nn生成的权重进行卷积操作；使用平均聚合</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># GRU（门控循环单元）处理图卷积后的数据，qa其是一种循环神经网络（RNN），用于处理序列数据。GRU 可以捕捉长程依赖，避免梯度消失问题。</span></span><br><span class="line">        <span class="variable language_">self</span>.gru = GRU(dim, dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set2Set 是一种图池化方法，用于将图节点的表示聚合成固定大小的图表示。</span></span><br><span class="line">        <span class="variable language_">self</span>.set2set = Set2Set(dim, processing_steps=<span class="number">3</span>)</span><br><span class="line">        <span class="comment"># 这个线性层将 Set2Set 的输出转换为 dim 维度。</span></span><br><span class="line">        <span class="variable language_">self</span>.lin1 = torch.nn.Linear(<span class="number">2</span> * dim, dim)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>._init_task_heads() <span class="comment"># 动态创建与任务数量对应的输出层</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 动态创建多个任务头（输出头），每个任务头是一个线性层，将 dim 维度转换为1维输出，对应的是所预测化学分子的相关性质，具体在utils.py中有定义。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_task_heads</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.n_tasks):</span><br><span class="line">            <span class="built_in">setattr</span>(<span class="variable language_">self</span>, <span class="string">f&quot;head_<span class="subst">&#123;i&#125;</span>&quot;</span>, torch.nn.Linear(<span class="variable language_">self</span>.dim, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># task_specific是一个 ModuleList，包含所有任务头</span></span><br><span class="line">        <span class="variable language_">self</span>.task_specific = torch.nn.ModuleList(</span><br><span class="line">            [<span class="built_in">getattr</span>(<span class="variable language_">self</span>, <span class="string">f&quot;head_<span class="subst">&#123;i&#125;</span>&quot;</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.n_tasks)]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播方法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, data, return_representation=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="comment"># 第一层线性变换和激活，输入是结点特征data.x矩阵</span></span><br><span class="line">        out = F.relu(<span class="variable language_">self</span>.lin0(data.x))</span><br><span class="line">        h = out.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 图卷积和GRU层</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            m = F.relu(<span class="variable language_">self</span>.conv(out, data.edge_index, data.edge_attr))</span><br><span class="line">            out, h = <span class="variable language_">self</span>.gru(m.unsqueeze(<span class="number">0</span>), h)</span><br><span class="line">            out = out.squeeze(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># data.edge_index是边索引矩阵（通常是2×edges_num的张量），决定了图中哪些点之间有信息的传递；data.edgea_attr是边特征矩阵（通常是edges_num×edge_features_num的张量），边的特征在信息传递的过程中可能会被使用</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set2Set池化</span></span><br><span class="line">        out = <span class="variable language_">self</span>.set2set(out, data.batch)</span><br><span class="line">        features = F.relu(<span class="variable language_">self</span>.lin1(out))</span><br><span class="line">        logits = torch.cat(</span><br><span class="line">            [<span class="built_in">getattr</span>(<span class="variable language_">self</span>, <span class="string">f&quot;head_<span class="subst">&#123;i&#125;</span>&quot;</span>)(features) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.n_tasks)], dim=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> return_representation:</span><br><span class="line">            <span class="keyword">return</span> logits, features</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回模型中共享层的参数迭代器</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">shared_parameters</span>(<span class="params">self</span>) -&gt; Iterator[torch.nn.parameter.Parameter]:</span><br><span class="line">        <span class="keyword">return</span> chain(</span><br><span class="line">            <span class="variable language_">self</span>.lin0.parameters(),</span><br><span class="line">            <span class="variable language_">self</span>.conv.parameters(),</span><br><span class="line">            <span class="variable language_">self</span>.gru.parameters(),</span><br><span class="line">            <span class="variable language_">self</span>.set2set.parameters(),</span><br><span class="line">            <span class="variable language_">self</span>.lin1.parameters(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回任务头（最后输出的线性层）的参数迭代器</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">task_specific_parameters</span>(<span class="params">self</span>) -&gt; Iterator[torch.nn.parameter.Parameter]:</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.task_specific.parameters()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回最后一层共享层的参数迭代器</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">last_shared_parameters</span>(<span class="params">self</span>) -&gt; Iterator[torch.nn.parameter.Parameter]:</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.lin1.parameters()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="utils-py"><a href="#utils-py" class="headerlink" title="utils.py"></a>utils.py</h3><h4 id="target"><a href="#target" class="headerlink" title="target"></a>target</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">qm9_target_dict = &#123;</span><br><span class="line">    <span class="number">0</span>: <span class="string">&quot;mu&quot;</span>,</span><br><span class="line">    <span class="number">1</span>: <span class="string">&quot;alpha&quot;</span>,</span><br><span class="line">    <span class="number">2</span>: <span class="string">&quot;homo&quot;</span>,</span><br><span class="line">    <span class="number">3</span>: <span class="string">&quot;lumo&quot;</span>,</span><br><span class="line">    <span class="number">5</span>: <span class="string">&quot;r2&quot;</span>,</span><br><span class="line">    <span class="number">6</span>: <span class="string">&quot;zpve&quot;</span>,</span><br><span class="line">    <span class="number">7</span>: <span class="string">&quot;U0&quot;</span>,</span><br><span class="line">    <span class="number">8</span>: <span class="string">&quot;U&quot;</span>,</span><br><span class="line">    <span class="number">9</span>: <span class="string">&quot;H&quot;</span>,</span><br><span class="line">    <span class="number">10</span>: <span class="string">&quot;G&quot;</span>,</span><br><span class="line">    <span class="number">11</span>: <span class="string">&quot;Cv&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># for \Delta_m calculations</span></span><br><span class="line"><span class="comment"># -------------------------</span></span><br><span class="line"><span class="comment"># DimeNet uses the atomization energy for targets U0, U, H, and G.</span></span><br><span class="line">target_idx = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">11</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Report meV instead of eV.</span></span><br><span class="line">multiply_indx = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line"></span><br><span class="line">n_tasks = <span class="built_in">len</span>(target_idx)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="MyTransform"><a href="#MyTransform" class="headerlink" title="MyTransform"></a>MyTransform</h4><p>MyTransform的主要作用是在加载 QM9 数据集时选择并提取感兴趣的目标属性（y）</p><p>其用作QM9类，torch_geometric.datasets.QM9(root, transform&#x3D;None, pre_transform&#x3D;None, pre_filter&#x3D;None)，其中的transform参数，应用于每个数据对象的变换函数或变换序列。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyTransform</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, target: <span class="built_in">list</span> = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> target <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            target = torch.tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">11</span>])  <span class="comment"># removing 4</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            target = torch.tensor(target)</span><br><span class="line">        <span class="variable language_">self</span>.target = target</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, data</span>):</span><br><span class="line">        <span class="comment"># Specify target.</span></span><br><span class="line">        data.y = data.y[:, <span class="variable language_">self</span>.target]</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="Compelete"><a href="#Compelete" class="headerlink" title="Compelete"></a>Compelete</h4><p>Complete 类的主要作用是将原始的图数据转换为完全图，以便更好地捕获全局信息和特征。通过添加所有可能的边，并保留原有的边特征，这种转换为某些图神经网络模型的训练和评估提供了便利。</p><p>其用作QM9类，torch_geometric.datasets.QM9(root, transform&#x3D;None, pre_transform&#x3D;None, pre_filter&#x3D;None)，其中的pre_transform参数，应用于预处理数据对象的变换函数，仅在数据集下载和处理时运行一次。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Complete</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, data</span>):</span><br><span class="line">        device = data.edge_index.device</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建所有节点的组合（即完整连接），通过将图补全为完全图，每个节点都与其他所有节点相连。这有助于捕获全局的结构信息，特别是当你需要计算节点之间的距离或集成全局特征时。</span></span><br><span class="line">        row = torch.arange(data.num_nodes, dtype=torch.long, device=device)</span><br><span class="line">        col = torch.arange(data.num_nodes, dtype=torch.long, device=device)</span><br><span class="line"></span><br><span class="line">        row = row.view(-<span class="number">1</span>, <span class="number">1</span>).repeat(<span class="number">1</span>, data.num_nodes).view(-<span class="number">1</span>)</span><br><span class="line">        col = col.repeat(data.num_nodes)</span><br><span class="line">        edge_index = torch.stack([row, col], dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        edge_attr = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> data.edge_attr <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 复制现有边特征到新的完全图中，保留原有的边特征，并将其复制到新的完全图中，确保节点之间的连接信息不丢失。</span></span><br><span class="line">            idx = data.edge_index[<span class="number">0</span>] * data.num_nodes + data.edge_index[<span class="number">1</span>]</span><br><span class="line">            size = <span class="built_in">list</span>(data.edge_attr.size())</span><br><span class="line">            size[<span class="number">0</span>] = data.num_nodes * data.num_nodes</span><br><span class="line">            edge_attr = data.edge_attr.new_zeros(size)</span><br><span class="line">            edge_attr[idx] = data.edge_attr</span><br><span class="line">        <span class="comment"># 移除自环，移除自环以减少噪声，有助于某些模型的训练和性能提升。</span></span><br><span class="line">        edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)</span><br><span class="line">        data.edge_attr = edge_attr</span><br><span class="line">        data.edge_index = edge_index</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="delta-fn"><a href="#delta-fn" class="headerlink" title="delta_fn"></a>delta_fn</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># stl results</span></span><br><span class="line">BASE = np.array(</span><br><span class="line">    [</span><br><span class="line">        <span class="number">0.0671</span>,</span><br><span class="line">        <span class="number">0.1814</span>,</span><br><span class="line">        <span class="number">60.576</span>,</span><br><span class="line">        <span class="number">53.915</span>,</span><br><span class="line">        <span class="number">0.5027</span>,</span><br><span class="line">        <span class="number">4.539</span>,</span><br><span class="line">        <span class="number">58.838</span>,</span><br><span class="line">        <span class="number">64.244</span>,</span><br><span class="line">        <span class="number">63.852</span>,</span><br><span class="line">        <span class="number">66.223</span>,</span><br><span class="line">        <span class="number">0.07212</span>,</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">SIGN = np.array([<span class="number">0</span>] * n_tasks)</span><br><span class="line">KK = np.ones(n_tasks) * -<span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">delta_fn</span>(<span class="params">a</span>):</span><br><span class="line">    <span class="keyword">return</span> (KK ** SIGN * (a - BASE) / BASE).mean() * <span class="number">100.0</span>  <span class="comment"># *100 for percentage</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h3><ol><li>MR的计算？</li><li>训练的结束？（论文中提到了根据验证集的评估情况，提前结束训练）</li><li>$\Delta_m$%的含义？（MTL 模型在各个任务上的平均相对误差百分比，基于 STL 模型的基准表现。这个百分比表示 MTL 模型相较于 STL 基准的整体性能提升或退步。）</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习（MTL） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式计算框架MapReduce</title>
      <link href="/2025/05/15/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6MapReduce/"/>
      <url>/2025/05/15/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6MapReduce/</url>
      
        <content type="html"><![CDATA[<p><code>这篇blog用于记录我在学习计算机系统工程导论时所做的一次实验。我阅读了一篇论文，其介绍了一种名为MapReduce的模型，它通过键值对来拆解任务同时并行地处理子任务，达到了提高数据吞吐率从而降低时延提高性能的效果。</code></p><h2 id="实验目的"><a href="#实验目的" class="headerlink" title="实验目的"></a>实验目的</h2><p>主要实验目的如下：</p><ol><li>通过实验，理解MapReduce框架的基本原理，掌握MapReduce框架的Map阶段和Reduce阶段的执行流程，以及数据的划分、传输和聚合过程，具备使用MapReduce进行简单的分布式数据处理，并能通过进一步自学进行更为复杂的分布式数据处理，从而掌握使用MapReduce进行大数据处理的基本系统设计能力。</li><li>通过程序验证，理解性能优化中的并发技术对时延和吞吐率的影响，掌握用并发技术提升系统性能的设计思想。</li><li>通过观察分析，理解系统的可并行性与不可并行性的差异，理解工程实践中的并行不完备问题，通过思考该问题，训练理解和处理工程实践冲突和设计实现差异的思维能力。</li><li>通过实验准备和作业提交过程中的论文查阅（MapReduce2004），<br>逐步掌握外文专业文献的检索、学习和应用能力。</li></ol><h2 id="实验过程与习题"><a href="#实验过程与习题" class="headerlink" title="实验过程与习题"></a>实验过程与习题</h2><h3 id="2-1-WordCount"><a href="#2-1-WordCount" class="headerlink" title="2.1 WordCount"></a>2.1 WordCount</h3><h4 id="问题1：WordCount的-init-方法的参数是maptask和reducetask，简单地解释一下这两个变量都控制了什么？"><a href="#问题1：WordCount的-init-方法的参数是maptask和reducetask，简单地解释一下这两个变量都控制了什么？" class="headerlink" title="问题1：WordCount的__init__方法的参数是maptask和reducetask，简单地解释一下这两个变量都控制了什么？"></a>问题1：WordCount的__init__方法的参数是maptask和reducetask，简单地解释一下这两个变量都控制了什么？</h4><p>maptask指定了Map阶段的并行任务数（原始数据分块数），在MapReduce模型中会将数据先进行分块处理，然后在每一块上运行一个Map任务，所以这个参数也间接决定了每一个Map任务需要处理的数据量大小。</p><p>reducetask指定了Reduce阶段的并行任务数（中间数据分区数），在模型中会先对Map任务处理完成的中间键值对数据进行分区，在每一个区上运行一个Reduce任务，这个参数也间接决定了每个Reduce任务需要处理的数据量大小。</p><p>由run方法中线程池使用了这两个参数来启动线程可知，如下图所示。</p><p>如图1所示</p><h4 id="问题2：简要解释调用run如何触发对WordCount实例的map和reduce方法的调用"><a href="#问题2：简要解释调用run如何触发对WordCount实例的map和reduce方法的调用" class="headerlink" title="问题2：简要解释调用run如何触发对WordCount实例的map和reduce方法的调用"></a>问题2：简要解释调用run如何触发对WordCount实例的map和reduce方法的调用</h4><p>WordCount类继承了MapReduce类，使用<code>wc.run()</code>调用了run方法；</p><p>在run方法中使用<code>Pool</code>创建了进程池，并添加了相应的进程数量；</p><p>根据maptask、reducetask启动了相应数量的Map和Reduce任务，对应为<code>doMap()</code>和<code>doReduce()</code>函数；（这两个函数相当于MapReduce模型中map和reduce工作结点在调用用户的Map、Reduce函数前所进行的预处理操作）</p><p>在doMap函数中调用了<code>Map()</code>，在doReduce函数中调用了<code>Reduce()</code>即WordCount实例的map和reduce方法；</p><p>综上，通过run触发了WordCount实例的map和reduce方法的调用。</p><p>doMap、doReduce调用Map、Reduce的位置如下图所示。</p><p>如图2所示</p><h3 id="Map和Reduce"><a href="#Map和Reduce" class="headerlink" title="Map和Reduce"></a>Map和Reduce</h3><h4 id="问题3：WordCount中map方法的参数keyvalue和value代表什么？"><a href="#问题3：WordCount中map方法的参数keyvalue和value代表什么？" class="headerlink" title="问题3：WordCount中map方法的参数keyvalue和value代表什么？"></a>问题3：WordCount中map方法的参数keyvalue和value代表什么？</h4><p>keyvalue代表了当前处理的数据块对应的文本起始位置在整个文本文件中的偏移量。</p><p>因为<code>keyvalue = f.readline()</code>，而前面进行Split处理的时候，将整个文本文件按块分割为了多个中间文件，中间文件的起始行由<code>f.write(str(i) + &quot;\n&quot;)</code>记录了当前的偏移<code>i</code>。</p><p>value代表了当前处理数据块对应的文本数据。</p><p>因为<code>value = f.read()</code>，读取了相应的文本数据赋值给value。</p><p>keyvalue、value的赋值情况如下图所示</p><p>如图3所示</p><h4 id="问题4：WordCount中reduce方法的参数key和keyvalues代表什么？"><a href="#问题4：WordCount中reduce方法的参数key和keyvalues代表什么？" class="headerlink" title="问题4：WordCount中reduce方法的参数key和keyvalues代表什么？"></a>问题4：WordCount中reduce方法的参数key和keyvalues代表什么？</h4><p>每个reduce任务会将自己处理的数据区的键值对按照键聚集起来形成集合，并进行排序。</p><p>key代表了当前处理的是第几个键值对集合，keyvalues代表了当前处理的键值对集合中的各个键值对。</p><p>注：这里使用“集合”只是形象的表述，由于单词计数的任务特征，实际上同一键值对集合中的各个“元素”都是完全相同的键值对，<code>(str,1)</code>，所以这里的“集合”并不具有“互异性”</p><p>key、keyvalues的赋值情况如下图所示。</p><p>如图4所示</p><h3 id="Map和Reduce的并行"><a href="#Map和Reduce的并行" class="headerlink" title="Map和Reduce的并行"></a>Map和Reduce的并行</h3><h4 id="问题5：doMap有多少调用，doReduce有多少调用？为什么？"><a href="#问题5：doMap有多少调用，doReduce有多少调用？为什么？" class="headerlink" title="问题5：doMap有多少调用，doReduce有多少调用？为什么？"></a>问题5：doMap有多少调用，doReduce有多少调用？为什么？</h4><p>doMap有maptask次调用，doReduce有reducetask次调用。</p><p>因为在run方法中，通过线程池的map函数分别按照<code>range(0,self.maptask)</code>和<code>range(0,self.reducetask)</code>分发了maptask个doMap调用、reducetask个doReduce调用。</p><h4 id="问题6：假设有足够的内核，哪些调用是并行运行的？"><a href="#问题6：假设有足够的内核，哪些调用是并行运行的？" class="headerlink" title="问题6：假设有足够的内核，哪些调用是并行运行的？"></a>问题6：假设有足够的内核，哪些调用是并行运行的？</h4><p>假设内核是足够的，所有的doMap调用是并行的、所有的doReduce调用是并行的；doMap与doReduce两种调用之间是串行的。</p><p>因为doMap由线程池管理的线程一一调用，doReduce同理；而在所有doMap完成之后才开始由线程池管理调用。</p><h4 id="问题7：对于maptask和reducetask参数的值，哪一个影响到了程序的运行时间？为什么有的参数不会对程序的运行时间产生影响？（可以通过在代码中创建开始时间和结束时间来计算程序运行时间）"><a href="#问题7：对于maptask和reducetask参数的值，哪一个影响到了程序的运行时间？为什么有的参数不会对程序的运行时间产生影响？（可以通过在代码中创建开始时间和结束时间来计算程序运行时间）" class="headerlink" title="问题7：对于maptask和reducetask参数的值，哪一个影响到了程序的运行时间？为什么有的参数不会对程序的运行时间产生影响？（可以通过在代码中创建开始时间和结束时间来计算程序运行时间）"></a>问题7：对于maptask和reducetask参数的值，哪一个影响到了程序的运行时间？为什么有的参数不会对程序的运行时间产生影响？（可以通过在代码中创建开始时间和结束时间来计算程序运行时间）</h4><p>maptask和reducetask的值理论上都会影响到程序运行的时间。如果有其中之一不会对程序的运行时间产生影响，我认为原因是当前这个参数并非程序运行时间的瓶颈所在。例如当reducetask决定Reduce阶段才是任务执行的瓶颈的时候，增加maptask的数量并不会显著加快程序运行的时间，例如下图中的<strong>2个maptask，2个reducetask和4个maptask，2个reducetask</strong>情况下的对照，可以说明这个问题。</p><p>图5（2个maptask，2个reducetask运行情况）</p><p>图6（4个maptask，2个reducetask运行情况）</p><p>图7（2个maptask，4个reducetask运行情况）</p><p>图8（4个maptask，4个reducetask运行情况）</p><p>图9（8个maptask，8个reducetask运行情况）</p><h2 id="遇到的问题及解决办法"><a href="#遇到的问题及解决办法" class="headerlink" title="遇到的问题及解决办法"></a>遇到的问题及解决办法</h2><p>在进行本次实验的过程中，我主要遇到了以下几个问题：</p><ol><li><p>在探究WordCount中Map方法的参数keyvalue的时候，我不太理解这个参数存在的意义。虽然通过Split分割文本时，记录了相应的偏移，再在doMap阶段将偏移读入了keyvalue，最终在调用用户的Map的时候将keyvalue传递给了Map方法，但是Map方法并没有使用到keyvalue。所以起初，我认为这个参数没必要存在。后来，我与同学进行了一些讨论，我们发现在MapReduce的文献中，描述该模型的工作规范时，有类似这样的陈述“被分配 map 任务的工作节点读取对应输入分片的内容。它从输入数据中解析出键&#x2F;值对，并将每个对传递给用户定义的 Map 函数。Map 函数产生的中间键&#x2F;值对被缓存在内存中。”于是，我们认为，此处的keyvalue、value对应的正是map任务工作结点解析产生的键值对，后续Map函数将在value中进一步解析产生中间键值对。于是，我们认为此处虽然没有用到keyvalue，但是这是<strong>模型的思想所在</strong>，应当予以规范地保留。</p></li><li><p>在探究问题7的时候，对于maptask、reducetask的大小和任务执行的快慢，似乎有一些合不上，因为起初我认为应该两个任务分配得越多，并行越多，执行越快。但是可以看到在8个maptask，8个reducetask的情况下，执行时间却是最慢的。于是我查阅相关资料，了解到，这可能有以下几种原因：1）与我的内核数有关，如果系统内核数小于进程数，则会带来频繁地上下文切换，进一步增大时延（这或许就是为什么分配任务数最多的时候，反而最慢的原因）2）Map阶段的输出需要传递给Reduce阶段，如果并行的任务数增加，可能带来更加频繁地数据交换，从而造成I&#x2F;O瓶颈，增大时延（这可能是我单独增大reducetask时，运行变慢的原因）3）最后这个原因比较有趣，我开始完全没有考虑过，即任务之间的负载均衡问题，如果分配数量过多，可能导致有的任务几乎没有数据处理，但是仍然占用了相关的计算机资源，在资源有限的情况下，实际工作的任务分配到的资源减少，同样也会造成执行时间变慢。</p></li></ol><h2 id="课后实验与思考（选做）"><a href="#课后实验与思考（选做）" class="headerlink" title="课后实验与思考（选做）"></a>课后实验与思考（选做）</h2><p>我在原始代码的基础上，新增了一个ReverseIndex类用于替代原来的WordCount类，并在其中添加了类似于先前的Map、Reduce方法。Map方法的主要更改是形成的键值对是(word,offset)，而不再是用于计数的(word,1)，在Reduce中会对中间键值对进行处理，返回的是一个(word,sorted(list))，list中是同一word的各个offset，并且按照从小到大进行了排序。最后再简单处理一下结果，按照word的字典序（a~z），输出前二十组(word,sorted(list))。得到如下图所示的结果。</p><p>结果如图10所示</p><h2 id="实验总结"><a href="#实验总结" class="headerlink" title="实验总结"></a>实验总结</h2><p>通过本次实验，在阅读MapReduce文献的基础上，我进一步地理解了这个模型的工作方式，其通过Map、Reduce来便捷地实现任务的并行是模型思想的核心所在，尤其是理解了先前阅读文献时不太理解的map工作阶段、reduce工作阶段和用户的Map、Reduce函数之间的关系以及具体是怎样工作的。此外我还学习到了一些有关Python的知识，例如线程池管理下的简单的多线程编程，还有有关Python类之间继承关系与函数复用的知识。最后我通过课后实验与思考，增强了自己的Python编码能力，同时进一步熟悉了MapReduce的工作方式，收获颇丰。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 分布式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce</title>
      <link href="/2025/05/11/MapReduce/"/>
      <url>/2025/05/11/MapReduce/</url>
      
        <content type="html"><![CDATA[<p><code>这篇blog用于记录，我在学习计算机系统工程导论，有关性能的章节时，阅读的一篇叫做MapReduce的论文</code></p><h2 id="工程师提出MapReduce的编程模型和实现，他们的性能目标是什么？"><a href="#工程师提出MapReduce的编程模型和实现，他们的性能目标是什么？" class="headerlink" title="工程师提出MapReduce的编程模型和实现，他们的性能目标是什么？"></a>工程师提出MapReduce的编程模型和实现，他们的性能目标是什么？</h2><p>他们的性能目标是通过MapReduce实现大规模数据专用计算的自动并行化，使得缺少并行与分布式系统经验的程序员可以轻松利用大规模分布式系统资源，从而突破数据处理的时延这一性能瓶颈。</p><h2 id="Google是怎么通过实现去满足这些目标的？"><a href="#Google是怎么通过实现去满足这些目标的？" class="headerlink" title="Google是怎么通过实现去满足这些目标的？"></a>Google是怎么通过实现去满足这些目标的？</h2><p>MapReduce程序主要有用户部分和库部分，前者根据用户的业务逻辑需要在后者的基础上进行编写，而相关的并行化、容错、本地优化和负载均衡的细节被隐藏在后者中。整个系统的工作流程是：MapReduce库分割输入文件为较小块，并启动程序运行于机器集群之上；主结点根据块的分割情况，将map、reduce任务分配给各个机器处理；map任务通过用户的MAP函数建立相应的键值对；reduce任务将map任务处理的结果进行排序从而组合相同键，再通过用户的Reduce进行处理。这样的程序系统可以运行于廉价的PC集群之上，可以显著减少时延。</p><p>基于此，Google使用MapReduce程序来进行自己业务要求的实现：重写了生成Google网页搜索服务所需数据结构的生产索引、生成流行查询报告的数据提取、用于新实验和产品的网页属性提取等等。</p><h2 id="MapReduce为什么选择这样实现，而没有走其它技术道路？"><a href="#MapReduce为什么选择这样实现，而没有走其它技术道路？" class="headerlink" title="MapReduce为什么选择这样实现，而没有走其它技术道路？"></a>MapReduce为什么选择这样实现，而没有走其它技术道路？</h2><p>因为许多类型的问题都可以轻松地用MapReduce这种模型进行表示，从而被并行化地计算，所以其泛用性很好；基于这种模型的处理方式具有很强的可拓展性，考虑增加相应的工作结点，就可以用较低的代价换取较高的性能提升，通过合适的Map、Reduce函数也可以带来更加个性化的工作方式；这样的实现充分考虑了故障的可能，以及潜在的风险，并设计了相对应的处理方式，使得其可靠性较强，这一点从文献中提供的相关数据也可以看出。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 分布式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读：基于博弈论的复任务学习</title>
      <link href="/2025/05/08/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%EF%BC%9A%E5%9F%BA%E4%BA%8E%E5%8D%9A%E5%BC%88%E8%AE%BA%E7%9A%84%E5%A4%8D%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0/"/>
      <url>/2025/05/08/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%EF%BC%9A%E5%9F%BA%E4%BA%8E%E5%8D%9A%E5%BC%88%E8%AE%BA%E7%9A%84%E5%A4%8D%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p><code>这篇文献的原名是《Multi-Task Learning as a Bargaining Game》，这是我第一次尝试直接通篇阅读英文原文的文献，中间或许会遇到许多困难。我通过这篇blog来记录阅读的过程中遇到的概念，以及简单地对文章整体的脉络进行梳理。最终我希望对论文提到的算法进行复现，这篇blog将会是一个参考。</code></p><h2 id="概念部分"><a href="#概念部分" class="headerlink" title="概念部分"></a>概念部分</h2><p><code>这一部分用于记录阅读时遇到的概念型词汇，并做一些补充解释。困难在于，有的词汇我并不清楚是否是概念型的专业词汇，亦或是只需理解表面含义的词汇...</code></p><h3 id="gradients"><a href="#gradients" class="headerlink" title="gradients"></a>gradients</h3><p>梯度，包含某函数相对自身所有自变量的偏导数。</p><p>对于损失函数$L$梯度$\nabla L$，描述损失函数在参数空间中变化率最快的方向。</p><p>在机器学习中，训练的模型时常需要最小化损失函数，这时常用的方法就是梯度下降算法，通过计算损失函数的梯度，模型可以知道应该如何调整参数，从而减小预测误差。</p><p>梯度冲突，一个任务的梯度指示减小参数，另一个梯度指示增加参数，相互矛盾，最终得不出一个较好的结果。</p><h3 id="multiple-MTL-benchmarks"><a href="#multiple-MTL-benchmarks" class="headerlink" title="multiple MTL benchmarks"></a>multiple MTL benchmarks</h3><p>指的是多个被广泛认可的基准测试数据集或任务，这些任务用于评估多任务学习模型的性能。这些基准测试可以涵盖不同的应用领域，如自然语言处理、计算机视觉、语音识别等。</p><p>benchmark本意是基准</p><ol><li><p>QM9：QM9是一个化学分子数据集，主要用于分子性质预测任务。它包含了约13万条小分子的数据，每个分子都由其原子结构表示，同时提供了9个分子性质的标签（例如，分子的能量、分子的极化率、分子的稳定性等）。这些性质是通过量子力学计算方法（例如密度泛函理论，DFT）得到的。在化学领域，QM9数据集被广泛用于评估各种机器学习模型在分子性质预测任务上的表现。这个数据集是一个相对复杂的回归问题，用于测试模型在不同分子特征下的泛化能力。</p></li><li><p>MT10：MT10数据集是一个多任务学习（MTL）基准数据集，通常用于多任务学习研究的测试。它包含了10个任务，这些任务可以是不同的机器学习任务（例如，分类任务、回归任务等），目标是评估模型在同时处理多个任务时的表现。每个任务都共享一些特征，但每个任务有不同的输出目标，因此适合用于测试多任务学习方法的性能。</p></li></ol><h3 id="Cooperative-bargaining-game"><a href="#Cooperative-bargaining-game" class="headerlink" title="Cooperative bargaining game"></a>Cooperative bargaining game</h3><p>合作博弈，是一种博弈论中的模型，旨在分析参与者如何在合作的情况下进行资源分配或达成协议。</p><p>在多任务学习（MTL）的上下文中，将梯度组合视为一个合作博弈意味着：</p><ol><li>多个任务：每个任务可以看作一个玩家，它们通过共享知识和信息来提高整体性能。</li><li>梯度组合：不同任务的梯度可以被看作是不同玩家的“要求”，通过合作来找到一个能有效结合这些梯度的方式，从而优化模型的表现。</li><li>协商一致：通过合作博弈的方法，任务之间可以“协商”如何平衡彼此的梯度，以减少冲突并提高整体性能。</li></ol><h3 id="Proportionally-fair"><a href="#Proportionally-fair" class="headerlink" title="Proportionally fair"></a>Proportionally fair</h3><p>（按比例公平）是一个博弈论和分配理论中的概念，主要用于描述一种资源分配或收益分配的方法。在这种分配方式下，任何替代方案的平均相对变化都是负的，这意味着在分配中没有任何参与者会因为其他参与者的利益而遭受显著的损失。</p><p>用于MTL中可以：避免主导效应，通过按比例公平的更新，可以确保没有单一任务的梯度（特别是那些较大的梯度）主导更新过程。这可以防止某个任务的影响过大，导致模型在其他任务上的性能下降。<br><strong>Nash bargaining solution就是一种按比例公平的方法</strong></p><h3 id="convex"><a href="#convex" class="headerlink" title="convex"></a>convex</h3><p>convex，字面含义是凸性</p><p>凸集：<br>一个集合 $C$ 被称为 凸集，如果对于集合中的任意两个点 $x_1$和 $x_2$，连接这两个点的线段上的所有点也都在集合 $C$ 中。在数学上可以表示为：</p><p>$\forall x_1, x_2 \in C, \forall \lambda \in [0, 1], \quad \lambda x_1 + (1 - \lambda) x_2 \in C$</p><p>凸函数：<br>一个函数 $f: \mathbb{R}^n \to \mathbb{R}$被称为 凸函数，如果其定义域是一个凸集，并且对于任意的 $x_1, x_2$和 $\lambda \in [0, 1]$，都有：<br>$f(\lambda x_1 + (1 - \lambda) x_2) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)$<br>这表示函数的图形在任意两个点之间的连线不会低于函数的值，换句话说，函数呈一个“碗”的形状。<strong>这与我在大一选修工科数学分析学到的相同，当时老师强调了凸有两种含义，显然这里是下凸上凹的那一种。</strong></p><p>凸性的性质：<br>局部极小值即全局极小值：对于凸函数，如果在某一点有局部极小值，那么该点也是全局极小值。这是凸优化中非常重要的性质。</p><p>非凸的情况：<br>与凸函数相对的是 非凸函数，这些函数可能存在多个局部极小值，这使得优化过程更为复杂。在非凸情况下，找到全局最优解可能更具挑战性。</p><h3 id="帕累托最优"><a href="#帕累托最优" class="headerlink" title="帕累托最优"></a>帕累托最优</h3><p>多目标优化中解的支配关系：如果解$x$支配$x’$则，$x$在$(l_1,l_2,…,l_k)$的目标函数向量上，有1个或多个分量的结果优于$x’$。</p><p>Pareto Optimal：如果在多目标任务的定义域中对于解$x$没有支配自己的其它解，则称该解为帕累托最优</p><p>Local Pareto Optimal：在定义域的子集开集中的帕累托最优，称为帕累托局部最优</p><p>Pareto Stationary：如果解空间中一个点$x$是帕累托静止点，那么在该点上对于多目标对应的各个函数存在梯度凸组合为0。这意味着在该点的所有目标函数的梯度（导数）在某种程度上是平衡的，没有明显的“倾斜”方向可以改进所有目标。</p><p><strong>Pareto stationarity 是 Pareto optimality 的必要条件，但不是充分条件。这意味着一个点如果是 Pareto 最优的，那么它必须是 Pareto stationary 的，但反过来则不一定成立。</strong></p><p>凸组合：是指在数学和优化中，利用给定点的加权平均来形成新点的一种方法。具体来说，给定一组点 $x_1, x_2, \ldots, x_n$和对应的非负权重 $\lambda_1, \lambda_2, \ldots, \lambda_n$，如果这些权重的总和为 1，即</p><p>$\lambda_1 + \lambda_2 + \ldots + \lambda_n &#x3D; 1$<br><strong>注意这些权重都是非负的</strong></p><p>那么我们可以定义一个凸组合 $x$ 为：</p><p>$x &#x3D; \lambda_1 x_1 + \lambda_2 x_2 + \ldots + \lambda_n x_n$</p><p>凸组合的几何意义：在几何上，凸组合可以被视为在给定点之间的“插值”。例如，在二维空间中，两个点的凸组合会形成这两个点之间的线段，而三个点的凸组合则会形成这三点构成的三角形内部的所有点。</p><p>在多目标优化中，凸组合常用于表示不同目标之间的权衡。</p><h3 id="一些衡量MTL模型性能的指标"><a href="#一些衡量MTL模型性能的指标" class="headerlink" title="一些衡量MTL模型性能的指标"></a>一些衡量MTL模型性能的指标</h3><p>1)Segmentation（语义分割）</p><p>mIoU：平均交并比。这个指标是语义分割中常用的评价标准，计算方式是每个类别的交并比（IoU，Intersection over Union）取平均。交并比是预测结果和真实标签之间的交集与并集的比值。这个指标反映了模型在每个类别上的分割准确性，<strong>值越大表示分割结果越好</strong>。</p><p>Pix Acc：像素准确率。这个指标计算的是正确分类的像素点占总像素点的比例。它是一个比较简单的衡量标准，但可能在类别不均衡时会有偏差。</p><p>2)Depth（深度估计）相对误差</p><p>Abs Err：绝对误差。对于每个像素点，计算其预测深度值和真实深度值之间的差异的绝对值，然后取平均。这个指标越小，表示深度估计越准确。</p><p>Rel Err：计算预测深度值与真实深度值之间的相对差异，通常表示为：<br>$  \text{Rel Err} &#x3D; \frac{| \hat{d} - d |}{d} $</p><p>3)Surface Normal（表面法线）</p><p>Angle Distance：角度距离。这个指标计算的是预测法线方向与真实法线方向之间的角度差异。通常以度数（°）表示，<strong>值越小表示模型的法线估计越准确</strong>。</p><p>Within（Within Threshold）：通常指的是预测的法线与真实法线之间的角度差异小于某个阈值的比例。例如，如果设定阈值为$\theta$（例如11.25°、22.5°等），则“Within $\theta$”表示法线估计误差小于该阈值的像素所占比例。这个<strong>指标反映了模型能够准确预测法线的比例</strong>。</p><p>4)MR（Mean Rank）</p><p>**MR(Mean Rank)**是一个排名相关的指标，常用于排序问题。它通常计算的是模型对预测结果的排名准确度。例如，如果模型的预测最接近真实值，那么它会在排名中处于较高的位置。Mean Rank是计算多个任务或样本的平均排名值。</p><p>5)Δm%</p><p>Delta m%（Δm%）表示相对于基线模型的性能提升或下降百分比。计算方式通常是：<br>$  \Delta m% &#x3D; \frac{(\text{new performance} - \text{baseline performance})}{100}$</p><h3 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h3><p>张量的定义<br>在数学上，张量是一个多维数组，扩展了标量、向量和矩阵的概念。具体来说：</p><p>标量（Scalar）: 零阶张量，即一个单一的数值。<br>向量（Vector）: 一阶张量，即一维数组，具有方向和大小。<br>矩阵（Matrix）: 二阶张量，即二维数组，具有行和列的结构。<br>高阶张量（Higher-order Tensor）: 三维及以上的多维数组。</p><p>机器学习与深度学习: 训练数据和模型参数通常可以表示为高维张量。比如在深度学习框架（如TensorFlow和PyTorch）中，数据和权重都是以张量的形式存储和操作的。</p><h3 id="字面词汇"><a href="#字面词汇" class="headerlink" title="字面词汇"></a>字面词汇</h3><p>alleviate，缓解</p><p>negotiate，协商</p><p>heuristic，启发式方法</p><p>convergence，收敛</p><h3 id="文章架构"><a href="#文章架构" class="headerlink" title="文章架构"></a>文章架构</h3><p>这篇文章引入博弈论中的Nash bargaining solution，作为MTL中的aggregation algorithm，对多任务的梯度进行整合，减小gradients conflicts带来的性能影响。</p><p>总的来看，文章分为以下几个部分：</p><ol><li>改进Nash bargaining solution以适应MTL</li><li>从理论上对算法进行分析，保证了在convex和non-convex的case下的收敛</li><li>在多个领域对算法进行了验证</li></ol><h3 id="改进Nash-bargaining-solution以适应MTL"><a href="#改进Nash-bargaining-solution以适应MTL" class="headerlink" title="改进Nash bargaining solution以适应MTL"></a>改进Nash bargaining solution以适应MTL</h3><h4 id="Nash-bargaining-solution"><a href="#Nash-bargaining-solution" class="headerlink" title="Nash bargaining solution"></a>Nash bargaining solution</h4><p><code>这一部分简单介绍纳什协商解(Nash bargaining solution)在谈判博弈(bargaining game)中出现的条件</code></p><p><strong>谈判博弈问题是指</strong>$K$名玩家，每个玩家有一个效用函数$u_i:  A \cup {B} \rightarrow \mathbb{R}$，所有玩家都追求自身<strong>效用函数的最大化</strong>。其中$A$是所有<strong>可能达成的协议</strong>的集合，$D$是<strong>不能达成的协议</strong>（并非集合），对应的是无法达成$A$中元素对应协议时，玩家会采用<strong>默认策略</strong></p><p>定义收益集合$U$与默认收益$d$如下：</p><p>$U &#x3D; {u_1(x),…,u_k(x) : x \in A} \subset \mathbb{R}^{k}$<br>$d &#x3D; (u_1(D),…,u_k(D)) \in \mathbb{R}^{k}$</p><p>假设$U$满足以下性质：</p><ol><li>convex 集合是凸的</li><li>compact 集合是紧致的，即集合满足<strong>有界性、闭性</strong>，有界意味着集合 $U$ 的所有元素（即所有的解）都位于某个有限范围内，没有元素可以无限远离原点；闭性意味着集合 $U$ 包含它的边界点，也就是说，集合中的任何极限点都属于集合 $U$。</li><li>$U$对应的$k$维空间中至少存在一个点$u &#x3D; (u_1,u_2,…,u_k), u \in U$ strictly dominates $d &#x3D; {d_1,d_2,…,d_k}$，即$\forall i &#x3D; 1,…,k: u_i &gt; d_i$</li></ol><p><strong>注：</strong>（在效用函数确定的时候，$U$中不同的点是由协议或称博弈问题的解$x$所影响，产生的。所以当直接讨论$u$的时候实际上可以看作是在讨论某个解$x$，如果直接讨论$u_1,u_2…,u_n$而没有其作为“分量”的上下文，也可以将其看作对应的某些解$x_1,x_2,…,x_n$，或者直接称$U$是解的集合，其中的元素$u_1,u_2,…u_n$就是解）</p><p>则该问题<strong>存在</strong>唯一的解$x$即<strong>最优的策略、协议</strong>，称为<strong>Nash bargaining solution</strong>，且该类博弈具有如下性质：</p><ol><li>Pareto optimality：唯一解$x$满足帕累托最优，此处指的是不存在其它任何解$y$，可以使得$(u_1(y),…u_k(y))$支配$(u_1(x),…,u_k(x))$<br><del>（这个性质确保了，在这类博弈问题中具有唯一的最优解，因为没有其它任何解可以通过损害某方的利益，增大另一方的利益。）</del></li><li>Symmetry： 对称性，交换玩家的排列顺序，最终得到的解仍然是$x$<br><del>（这个性质确保了，这类博弈问题中没有任何特别的角色存在，player的效用函数只于其“所处位置”有关，而与角色自己没有关系。例如player1、palyer2在博弈中扮演character1、character2，则$x$协议下的效用函数对应$(u_1(x),u_2(x))$，交换玩家位置，player1、player2对应character2、character1，同样协议下的效用函数对应$(u_2(x),u_1(x))，效用函数不会因为player的不同而改变，只与character有关$）</del></li><li>Independence of Irrelevant Alternatives (IIA)：不相关选择的独立性，如果在解集合$U$中加入其它<strong>不相关</strong>选项，最优解仍然是$(u_1(x),…,u_k(x))$，不会改变。</li><li>Invariance to affine transformation：仿射变换的不变性，将每个效用函数 $u_i(x)$转换为 $u_i(x) &#x3D; c_i u_i(x) + b_i$且 $c_i &gt; 0$，那么如果原始协议的效用是 $(y_1,…,y_k)$，则经过变换后的协议效用将是 $(c_1 y_1 + b_1,…, c_k y_k + b_k)$。</li></ol><p>你问得非常好！我们来深入讨论：</p><hr><h3 id="🌟-问题聚焦：第二步结论是否在非最优点也成立？"><a href="#🌟-问题聚焦：第二步结论是否在非最优点也成立？" class="headerlink" title="🌟 问题聚焦：第二步结论是否在非最优点也成立？"></a>🌟 <strong>问题聚焦：第二步结论是否在非最优点也成立？</strong></h3><p>回顾第二步的核心结论：</p><blockquote><p>最优点的梯度满足</p><p>$$<br>$$</p></blockquote><p>\sum_i \frac{1}{\Delta\theta^\top g_i} g_i &#x3D; \lambda \Delta\theta<br>]</p><p>这表示目标函数的梯度是与 $\Delta\theta$ <strong>同方向</strong> 的，即在边界上的最优点，梯度“指向径向”，因为我们正最大化一个函数在球面上的值。</p><hr><h3 id="✅-这是否对任意（非最优）-Delta-theta-成立？"><a href="#✅-这是否对任意（非最优）-Delta-theta-成立？" class="headerlink" title="✅ 这是否对任意（非最优）$\Delta\theta$ 成立？"></a>✅ <strong>这是否对任意（非最优）$\Delta\theta$ 成立？</strong></h3><p><strong>答案：不成立。</strong></p><p>这是一个<strong>必要条件</strong>，只在最优解处成立。原因如下：</p><hr><h3 id="🔍-细致分析"><a href="#🔍-细致分析" class="headerlink" title="🔍 细致分析"></a>🔍 <strong>细致分析</strong></h3><p>我们最大化的目标函数是：</p><p>$$<br>f(\Delta\theta) &#x3D; \sum_i \log(\Delta\theta^\top g_i)<br>\quad \text{subject to } |\Delta\theta| \le \epsilon<br>$$</p><p>我们引入拉格朗日乘子来处理这个约束：</p><p>$$<br>\mathcal{L}(\Delta\theta, \lambda) &#x3D; \sum_i \log(\Delta\theta^\top g_i) - \lambda (|\Delta\theta|^2 - \epsilon^2)<br>$$</p><p>对 $\Delta\theta$ 求梯度并令其为 0：</p><p>$$<br>\nabla_{\Delta\theta} \mathcal{L} &#x3D; \sum_i \frac{g_i}{\Delta\theta^\top g_i} - 2\lambda \Delta\theta &#x3D; 0<br>\Rightarrow<br>\sum_i \frac{g_i}{\Delta\theta^\top g_i} &#x3D; 2\lambda \Delta\theta<br>$$</p><p>我们之前写的是 $\lambda \Delta\theta$，只是缩放不同（也就是说你可以吸收掉常数 $\lambda$，设 $\lambda’ &#x3D; 2\lambda$）。</p><p>所以这 <strong>是一个最优性的一阶必要条件</strong>，只有在最优点（stationary point）处才成立。</p><hr><h3 id="🧠-类比直觉："><a href="#🧠-类比直觉：" class="headerlink" title="🧠 类比直觉："></a>🧠 <strong>类比直觉：</strong></h3><p>就像你在优化一个函数 $f(x)$，你知道在极值点：</p><p>$$<br>\nabla f(x^*) &#x3D; 0<br>$$</p><p>但你不能说 <strong>任意点</strong> 的梯度都等于 0，对吧？同理，在这个 constrained optimization 问题中，这个</p><p>$$<br>\sum_i \frac{g_i}{\Delta\theta^\top g_i} \propto \Delta\theta<br>$$</p><p>的关系只在最优点成立。</p><hr><h3 id="✅-总结回答"><a href="#✅-总结回答" class="headerlink" title="✅ 总结回答"></a>✅ <strong>总结回答</strong></h3><blockquote><p>❓<strong>这个结论是否在非最优点也成立？</strong><br>❌ <strong>不成立</strong>。<br>这是一个一阶最优性条件（来自拉格朗日乘子法），<strong>只在最优点才成立</strong>。在非最优点，梯度方向一般不会刚好与 $\Delta\theta$ 对齐。</p></blockquote><p>如果你想，我们可以用一个二维例子可视化一下这个方向关系的变化。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习（MTL） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>高斯混合聚类</title>
      <link href="/2025/05/07/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB/"/>
      <url>/2025/05/07/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>高斯混合聚类不同于k-means、LVQ利用原型向量刻画聚类结构，而是利用概率来刻画聚类结构。</p><p>简单来说，这种算法认为数据集中的每个样本都符合一个多元高斯分布（多元的原因是样本常是多元向量），如下</p><p>所有的样本共同符合“混合高斯分布”。混合高斯分布对应的概率密度函数是所有多元高斯分布密度函数的加权量。</p><h3 id="多元高斯分布"><a href="#多元高斯分布" class="headerlink" title="多元高斯分布"></a>多元高斯分布</h3><p>若$x$服从多元高斯分布，对应概率密度函数为</p><p>$p(x) &#x3D; \frac{1}{(2\pi)^{\frac{n}{2}}\lvert \Sigma \rvert^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}$，其中$x$是样本对应的向量，$\Sigma$是协方差矩阵，$\mu$是期望</p><p>为了便于理解，参照一元高斯分布</p><p>$\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-1}{2}\frac{(x - \mu)^{2}}{\sigma^{2}}}$</p><p>协方差矩阵就对应方差、多元高斯分布中期望对应一元中的期望（只是多元高斯分布中期望是一个多维向量）</p><p>所以<strong>多元高斯分布的情况，由$\Sigma$和$\mu$唯一确定</strong></p><h3 id="混合高斯分布"><a href="#混合高斯分布" class="headerlink" title="混合高斯分布"></a>混合高斯分布</h3><p>将多元高斯分布密度函数记作$p(x|\mu,\Sigma)$</p><p>可以定义混合高斯分布如下：</p><p>若$x$服从混合高斯分布，整个样本空间对应有k种多元高斯分布，对应概率密度函数$p_M &#x3D; \sum^{k}<em>{i&#x3D;1} \alpha_i p(x|\mu_i,\Sigma_i)$，其中$\alpha$是混合系数，$\alpha_i$对应的实际意义是，选择第$i$个混合成分的概率，所以有$\alpha_i &gt; 0$且$\sum^{k}</em>{i&#x3D;1} \alpha_i &#x3D; 1$；而$p(x|\mu_i,\Sigma_i)$对应第i个混合成分的概率密度</p><h3 id="样本属于某混合成分的概率"><a href="#样本属于某混合成分的概率" class="headerlink" title="样本属于某混合成分的概率"></a>样本属于某混合成分的概率</h3><p>令数据集$D &#x3D; {x_1,x_2,…,x_m}$随机变量$z_j \in {1,2,…,k}$，$z_j$表征样本$x_j$属于哪个混合成分。</p><p>对于$x_j$并不确定的情况下，$z_j$的先验分布为</p><p>$p(z_j &#x3D; i) &#x3D; \alpha_i, i &#x3D; {1,2,…,k}$</p><p>根据贝叶斯定理，当$x_j$确定时，$z_j$的后验分布记作$p_M(z_j &#x3D; i|x_j)$，为</p><p>$p_M(z_j &#x3D; i|x_j) &#x3D; \frac{p(z_j &#x3D; i)p_M(x_j|z_j &#x3D; i)}{p_M(x_j)}$<br>其中，$p(z_j &#x3D; i) &#x3D; \alpha_i$；$p_M(x_j|z_j &#x3D; i) &#x3D; p(x|\mu_i,\Sigma_i)$，因为当确定$z_j &#x3D; i$时除了$\alpha_i &#x3D; 1$其它$\alpha_1,…\alpha_i-1,\alpha_i+1,…,\alpha_k$均为$0$；而$p_M(x_j)$即混合高斯分布的概率密度函数</p><p>将$p_M(z_j &#x3D; i|x_j)$简记作$\gamma_{ji}$，这个概率就是$x_j$的分布为$\mu_i,\Sigma_i$所对应的多元高斯分布的概率。</p><p>于是我们可以找到令$\gamma_{ji}$最大的$i \in {1,2,…,k}$，令$\lambda_j &#x3D; argmax_{i \in {1,2,…,k}} \gamma_{ji}$，$\lambda_j$即$x_j$的标签。对于每个都进行相同的操作，整个数据集便可被划分为k个多元高斯分布对应的k个簇。</p><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><h4 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h4><p>对于k-means、LVQ的模型训练实际上就是通过训练集获得对应的原型向量，有了原型向量便有了划分为簇的依据，也就完成了模型的训练。</p><p>而对于高斯混合聚类，根据前面的描述，我们划分为簇的重要依据就是$\gamma_{ji}$，进一步说实际上是<strong>计算$\gamma_{ji}$的依据</strong>，根据计算公式可知，这个依据实际上就是决定高斯混合分布的参数$\mu_i,\Sigma_i,\alpha_i, i \in {1,2,…,k}$。</p><p><strong>于是训练的目的实际上就是要得到它们的值。</strong></p><h4 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h4><p>对于模型参数${(\alpha_i,\mu_i,\Sigma_i)|1 \le i \le k}$，我们采用<strong>极大似然</strong>的方法进行求解。</p><p>这里引用南瓜书中的一句话：<br>“对于每个样本$x_j$来说，它出现的概率是$p_M(x_j)$既然现在训练集D中确实出现了$x_j$，我们当然希望待求解的参数${(\alpha_i,\mu_i,\Sigma_i)|1 \le i \le k}$能够使这种可能性$p_M(x_j)$最大”</p><p>于是根据极大似然方法，我们令<br>$LL(D) &#x3D; ln(\prod^{m}<em>{j&#x3D;1} p_M(x_j)) &#x3D; \sum^{m}</em>{j&#x3D;1} ln(\sum^k_{i&#x3D;1} \alpha_i \cdot p(x_j|\mu_i,\Sigma_i))$<br>为对数似然函数，将其最大化得到对应的参数，就是我们要求解的模型参数。</p><h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><p>经过一系列数学运算，我们可以得得到如下结果：</p><p>$\mu_i &#x3D; \frac{\sum^{m}_{j&#x3D;1} \gamma_{ji} x_j}{\sum^{m}_{j&#x3D;1} \gamma_{ji}}$</p><p>$\Sigma_i &#x3D; \frac{\sum^{m}_{j&#x3D;1} \gamma_{ji} (x_j - \mu_i)(x_j - \mu_i)^{T}}{\sum^{m}_{j&#x3D;1}} \gamma_{ji}$</p><p>$\alpha_i &#x3D; \frac{1}{m} \sum^{m}_{j&#x3D;1} \gamma{ji}$</p><p>$i &#x3D; 1,2,…,k$</p><p>注意结果当中，每一个参数的计算都要用到$\gamma{ji}$，而问题在于计算$\gamma{ji}$的时候又要用到参数本身，这似乎是一个循环的无从下手的问题。这种情况下，我们利用EM算法来求解。</p><h3 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h3><p>EM算法(Expectation-Maximization)称为“期望最大化算法”，这种算法最开始应用于：使用极大似然法对模型参数进行估计，但是已知的样本中存在还没有“观测”的变量，这种变量称为隐变量，它的值是不确定的。</p><p>令$X,Z,\Theta$分别为已观测变量集、隐变量集、参数集，则应最大化对数似然$LL(\Theta|X,Z) &#x3D; lnP(X,Z|\Theta)$，但是Z是隐变量，所以无法直接求解。</p><p>EM算法可以用于估计隐变量，并在这个过程中对参数做最大似然估计。</p><p>其基本的思想是这样的：</p><ol><li>初始化参数$\Theta$，根据参数去估计隐变量的<strong>概率分布</strong>，并利用此概率分布求得隐变量的<strong>期望</strong>——E步</li><li>将隐变量的期望作为我们观测到的隐变量本身，于是此时所有样本都已被观测，对$\Theta$做极大似然估计——M步</li></ol><p>不断重复上述两个过程——迭代，直到满足退出条件，例如$\Theta$收敛</p><p>贴一篇介绍EM算法的博客：<br><a href="https://blog.csdn.net/qq_41554005/article/details/100591525">CSDN</a></p><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p>结合EM算法，整个高斯混合聚类算法流程如下：</p><p>输入：</p><ol><li>样本集$D &#x3D; {x_1,x_2,…,x_m}$</li><li>高斯混合成分的个数k（当然也就是希望划分出的簇的个数）</li></ol><p>过程：</p><ol><li>初始化高斯混合分布的参数${(\alpha_i,\mu_i,\Sigma_i)|i &#x3D; 1,2…,k}$</li><li>repeat：</li><li>对每个样本$x_j,j &#x3D; 1,2,…,m$估计其属于第$i,i &#x3D; 1,2,…k$个成分的概率：$\gamma_{ji}$</li><li>利用公式<br>$\mu_i &#x3D; \frac{\sum^{m}_{j&#x3D;1} \gamma_{ji} x_j}{\sum^{m}_{j&#x3D;1} \gamma_{ji}}$，<br>$\Sigma_i &#x3D; \frac{\sum^{m}_{j&#x3D;1} \gamma_{ji} (x_j - \mu_i)(x_j - \mu_i)^{T}}{\sum^{m}_{j&#x3D;1}} \gamma_{ji}$，<br>$\alpha_i &#x3D; \frac{1}{m} \sum^{m}_{j&#x3D;1} \gamma{ji}$，$i &#x3D; 1,2,…,k$对参数进行更新</li><li>until:收敛条件（达到一定轮数 or 参数收敛）</li><li>求解$x_j,j &#x3D; 1,2,…,m$的标记$\lambda_j &#x3D; agrmax_{i &#x3D; 1,2,…,k} \gamma_{ji}$</li><li>根据标记划分为对应的簇</li></ol><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>Data.py:数据集部分</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">D = [</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0.697</span>, <span class="number">0.460</span>], [<span class="number">0.774</span>, <span class="number">0.376</span>], [<span class="number">0.634</span>, <span class="number">0.264</span>], [<span class="number">0.608</span>, <span class="number">0.318</span>], [<span class="number">0.556</span>, <span class="number">0.215</span>],</span><br><span class="line">    [<span class="number">0.403</span>, <span class="number">0.237</span>], [<span class="number">0.481</span>, <span class="number">0.149</span>], [<span class="number">0.437</span>, <span class="number">0.211</span>], [<span class="number">0.666</span>, <span class="number">0.091</span>], [<span class="number">0.243</span>, <span class="number">0.267</span>],</span><br><span class="line">    [<span class="number">0.245</span>, <span class="number">0.057</span>], [<span class="number">0.343</span>, <span class="number">0.099</span>], [<span class="number">0.639</span>, <span class="number">0.161</span>], [<span class="number">0.657</span>, <span class="number">0.198</span>], [<span class="number">0.360</span>, <span class="number">0.370</span>],</span><br><span class="line">    [<span class="number">0.593</span>, <span class="number">0.042</span>], [<span class="number">0.719</span>, <span class="number">0.103</span>], [<span class="number">0.359</span>, <span class="number">0.188</span>], [<span class="number">0.339</span>, <span class="number">0.241</span>], [<span class="number">0.282</span>, <span class="number">0.257</span>],</span><br><span class="line">    [<span class="number">0.748</span>, <span class="number">0.232</span>], [<span class="number">0.714</span>, <span class="number">0.346</span>], [<span class="number">0.483</span>, <span class="number">0.312</span>], [<span class="number">0.478</span>, <span class="number">0.437</span>], [<span class="number">0.525</span>, <span class="number">0.369</span>],</span><br><span class="line">    [<span class="number">0.751</span>, <span class="number">0.489</span>], [<span class="number">0.532</span>, <span class="number">0.472</span>], [<span class="number">0.473</span>, <span class="number">0.376</span>], [<span class="number">0.725</span>, <span class="number">0.445</span>], [<span class="number">0.446</span>, <span class="number">0.459</span>]</span><br><span class="line">]  <span class="comment"># 数据集，1~30，0索引不使用</span></span><br></pre></td></tr></table></figure><p>main.py:主函数部分</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> Gauss</span><br><span class="line"><span class="keyword">import</span> Data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;高斯混合聚类 的结果&quot;</span>, end=<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">    res = Gauss.gauss(Data.D)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">len</span>(res[i]), end=<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(res[i])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Guass.py:高斯混合聚类算法部分</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multi_gauss_distri_p</span>(<span class="params">sigma, mu, n, x</span>):</span><br><span class="line">    <span class="comment"># 计算多元高斯分布下取得x的概率，n是维度</span></span><br><span class="line">    vec_mu = np.array(mu)</span><br><span class="line">    vec_x = np.array(x)</span><br><span class="line">    t1 = vec_x - vec_mu</span><br><span class="line">    <span class="comment">#    t1 = t1.T</span></span><br><span class="line">    det_sigma = np.linalg.det(sigma)</span><br><span class="line">    <span class="keyword">if</span>(det_sigma &lt; <span class="number">1e-10</span>):  <span class="comment"># 当行列式过小时，添加一个较小的正则化项</span></span><br><span class="line">        sigma += np.eye(sigma.shape[<span class="number">0</span>]) * <span class="number">1e-6</span>  <span class="comment"># 添加正则化，避免奇异矩阵</span></span><br><span class="line">    t2 = np.linalg.inv(sigma)</span><br><span class="line">    t3 = vec_x - vec_mu</span><br><span class="line">    t3 = t3.T  <span class="comment"># 西瓜书上的公式里没有转置的向量默认是列向量</span></span><br><span class="line">    log_p = -<span class="number">0.5</span> * (np.dot(np.dot(t1, t2), t3) + np.linalg.slogdet(sigma)[<span class="number">1</span>] + n * np.log(<span class="number">2</span> * np.pi))</span><br><span class="line">    p = np.exp(log_p)</span><br><span class="line"><span class="comment">#    p = 1 / ((2 * np.pi) ** (n / 2) * np.linalg.det(sigma) ** (1 / 2)) * np.e ** (-0.5 * np.dot(np.dot(t1, t2), t3)</span></span><br><span class="line"><span class="comment">#    print(&quot;p:&quot;, p, end=&quot;\n&quot;)</span></span><br><span class="line">    <span class="keyword">if</span> p &lt; <span class="number">1e-10</span>:</span><br><span class="line">        p = <span class="number">1e-6</span>  <span class="comment"># 避免数值问题</span></span><br><span class="line">    <span class="keyword">return</span> p</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">new_mu_i</span>(<span class="params">D, lamda_, i</span>):</span><br><span class="line">    up_sum = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    down_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">31</span>):</span><br><span class="line">        up_sum[<span class="number">0</span>] += lamda_[j][i] * D[j][<span class="number">0</span>]</span><br><span class="line">        up_sum[<span class="number">1</span>] += lamda_[j][i] * D[j][<span class="number">1</span>]</span><br><span class="line">        down_sum += lamda_[j][i]</span><br><span class="line">    new_mu = [up_sum[<span class="number">0</span>] / down_sum, up_sum[<span class="number">1</span>] / down_sum]</span><br><span class="line">    <span class="keyword">return</span> new_mu</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">new_sigma_i</span>(<span class="params">D, mu, lamda_, i</span>):</span><br><span class="line">    vec_x = np.array(D[i])</span><br><span class="line">    vec_mu = np.array(mu)</span><br><span class="line">    up_sum = np.array([[<span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]])</span><br><span class="line">    down_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">31</span>):</span><br><span class="line">        t = vec_x - vec_mu</span><br><span class="line">        up_sum += lamda_[j][i] * (np.outer(t.T, t))</span><br><span class="line">        down_sum += lamda_[j][i]</span><br><span class="line">    new_sigma = up_sum / down_sum</span><br><span class="line">    <span class="keyword">return</span> new_sigma</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">new_alpha_i</span>(<span class="params">lambda_, i</span>):</span><br><span class="line">    up_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">31</span>):</span><br><span class="line">        up_sum += lambda_[j][i]</span><br><span class="line">    new_alpha = up_sum / <span class="number">30</span></span><br><span class="line">    <span class="keyword">return</span> new_alpha</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gauss</span>(<span class="params">D: [[<span class="built_in">float</span>]]</span>):</span><br><span class="line">    <span class="comment">#  定义混合高斯分布参数</span></span><br><span class="line">    sigma = []  <span class="comment"># 协方差矩阵</span></span><br><span class="line">    alpha = []  <span class="comment"># 混合权重</span></span><br><span class="line">    mu = []  <span class="comment"># 期望</span></span><br><span class="line">    <span class="comment">#  初始化参数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">        sigma.append(np.array([[<span class="number">0.1</span>, <span class="number">0.0</span>], [<span class="number">0.0</span>, <span class="number">0.1</span>]]))</span><br><span class="line">        alpha.append(<span class="number">1</span>/<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            mu.append(D[<span class="number">6</span>])</span><br><span class="line">        <span class="keyword">elif</span> i == <span class="number">1</span>:</span><br><span class="line">            mu.append(D[<span class="number">22</span>])</span><br><span class="line">        <span class="keyword">elif</span> i == <span class="number">2</span>:</span><br><span class="line">            mu.append(D[<span class="number">27</span>])</span><br><span class="line">    <span class="comment">#  定义xj属于第i种多元高斯分布的概率</span></span><br><span class="line">    lambda_ = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">31</span>):</span><br><span class="line">        lambda_.append([[], [], []])</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">100</span>):  <span class="comment"># 迭代</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">31</span>):  <span class="comment"># 依次对每个xj计算lambda_ji,i = 0，1，2，对应三种多元高斯分布</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">                t_sum = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">                    t_sum += alpha[l] * multi_gauss_distri_p(sigma[l], mu[l], <span class="number">2</span>, D[j])</span><br><span class="line">                lambda_ji = alpha[i] * multi_gauss_distri_p(sigma[i], mu[i], <span class="number">2</span>, D[j]) / t_sum</span><br><span class="line">                lambda_[j][i] = lambda_ji</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):  <span class="comment"># 更新混合高斯分布参数</span></span><br><span class="line">            mu[i] = new_mu_i(D, lambda_, i)</span><br><span class="line">            sigma[i] = new_sigma_i(D, mu[i], lambda_, i)</span><br><span class="line">            alpha[i] = new_alpha_i(lambda_, i)</span><br><span class="line"></span><br><span class="line">    result = [[], [], []]  <span class="comment"># 最终的划分结果</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">31</span>):  <span class="comment"># 依次对每个xj计算lambda_ji,i = 0，1，2，对应三种多元高斯分布</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">            t_sum = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">                t_sum += alpha[l] * multi_gauss_distri_p(sigma[l], mu[l], <span class="number">2</span>, D[j])</span><br><span class="line">            lambda_ji = alpha[i] * multi_gauss_distri_p(sigma[i], mu[i], <span class="number">2</span>, D[j]) / t_sum</span><br><span class="line">            lambda_[j][i] = lambda_ji</span><br><span class="line"></span><br><span class="line">        flag = <span class="number">0</span></span><br><span class="line">        ll = lambda_[j][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">3</span>):</span><br><span class="line">            <span class="keyword">if</span> ll &lt; lambda_[j][i]:</span><br><span class="line">                flag = i</span><br><span class="line">        result[flag].append(D[j])</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>进行这一部分的时候让我最大的感悟是，在使用计算机进行数据处理的时候，很可能会出现数据的溢出问题，非常大、非常小的数都在计算机的表示范围之外，就会带来问题。例如在进行协方差矩阵更新的时候，有时它的行列式值虽然不是0，但是已经非常小了，计算机会默认其为0。同样，可能作为除数的数也是一样的，如果太小变为0就会发送除以0的错误。<br><strong>这称为数值稳定性问题</strong></p><p>因此代码中有一些涉及处理这些问题的地方（主要是在求第i个多元高斯分布中取得xj这个值的概率的时候，一处是正则化、一处是对数化并添加过小的判断），尽管我现在并不清楚这样处理是否有问题。但是抛开这些细节，作为一次上手的练习，整个混合高斯聚类算法是正确的。<br><strong>不过值得一提的是，EM算法的收敛与迭代的次数没有必然的关系，通常应该使用参数变化的情况来决定是否结束迭代</strong></p><p>最后的结果如下：</p><blockquote><p>高斯混合聚类 的结果<br>2<br>[[0.608, 0.318], [0.359, 0.188]]<br>0<br>[]<br>28<br>[[0.697, 0.46], [0.774, 0.376], [0.634, 0.264], [0.556, 0.215], [0.403, 0.237], [0.481, 0.149], [0.437, 0.211], [0.666, 0.091], [0.243, 0.267], [0.245, 0.057], [0.343, 0.099], [0.639, 0.161], [0.657, 0.198], [0.36, 0.37], [0.593, 0.042], [0.719, 0.103], [0.339, 0.241], [0.282, 0.257], [0.748, 0.232], [0.714, 0.346], [0.483, 0.312], [0.478, 0.437], [0.525, 0.369], [0.751, 0.489], [0.532, 0.472], [0.473, 0.376], [0.725, 0.445], [0.446, 0.459]]</p></blockquote><p>经过我的观察，第3次迭代之后划分结果基本上就稳定了，第1次的时候分得比较均匀。这或许是数据的原因。（也有可能是我对于数值稳定性的处理不好）</p><p>如果将正则化项改大一些结果如下：</p><blockquote><p>高斯混合聚类 的结果<br>12<br>[[0.608, 0.318], [0.556, 0.215], [0.403, 0.237], [0.481, 0.149], [0.437, 0.211], [0.243, 0.267], [0.245, 0.057], [0.343, 0.099], [0.359, 0.188], [0.339, 0.241], [0.282, 0.257], [0.483, 0.312]]<br>10<br>[[0.634, 0.264], [0.666, 0.091], [0.639, 0.161], [0.657, 0.198], [0.593, 0.042], [0.719, 0.103], [0.748, 0.232], [0.714, 0.346], [0.751, 0.489], [0.725, 0.445]]<br>8<br>[[0.697, 0.46], [0.774, 0.376], [0.36, 0.37], [0.478, 0.437], [0.525, 0.369], [0.532, 0.472], [0.473, 0.376], [0.446, 0.459]]</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习（聚类） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>学习向量量化</title>
      <link href="/2025/05/06/%E5%AD%A6%E4%B9%A0%E5%90%91%E9%87%8F%E9%87%8F%E5%8C%96/"/>
      <url>/2025/05/06/%E5%AD%A6%E4%B9%A0%E5%90%91%E9%87%8F%E9%87%8F%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<p><code>这篇blog用于记录我使用python对学习向量量化这种聚类算法的复现</code></p><h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><p>学习向量量化也成为LVQ(Learning Vector Quantization)，同样属于原型聚类算法，类似于k-means通过希望划分的簇的数量求得相同数量的“簇中心”并以此为原型将数据集划分为对应的簇，LVQ通过求得与希望划分的簇数量相同的“原型向量”，并以此来将数据集划分为对应的簇。</p><p>如果说k-means也同样是借助原型向量的话，那么关键就在于两种算法更新原型向量的方法不同。k-means是不断的用原型向量划分簇，又用簇更新原型向量；LVQ则是利用样本的预先标注作为“监督信息”，不断利用样本更新原型向量。</p><h2 id="算法详解"><a href="#算法详解" class="headerlink" title="算法详解"></a>算法详解</h2><p>整个算法的大致流程如下：</p><p>输入: $D &#x3D; {(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}, q, {t_1,t_2,…,t_q},\eta \in (0,1)$其中，D是带有标记的数据集，q是原型向量个数，$t_i,i \in {1,2,…,q}$ 对应原型向量的标记，$\eta$是学习率</p><p>算法过程：</p><ol><li>初始化原型向量${p_1,p_2,…,p_q}$</li><li>repeat:</li><li>从$D$中随机挑选一个样本$(x_j,y_j)$</li><li>找到与$x_j$最近的原型向量$p_i^{*}$</li><li>if($t_i^{*}$ &#x3D;&#x3D; $y_j$): $p’ &#x3D; p_i^{*} + \eta(p_i^{*} - x_j)$</li><li>else: $p’ &#x3D; p_i^{*} - \eta(p_i^{*} - x_j)$</li><li>判断是否到达退出条件</li></ol><p>整个算法过程的关键就在于5、6，实际上相当于找到距离原型向量最近的样本，如果是同标记的则将该原型向量向该样本“拉近”，如果是不同标记的则“推远”（因为对应的是同标记的在一个簇中可能性较大，不同标记在不同簇中可能性较大）</p><p>关于7的退出条件，通常可以设置一个最大迭代轮数，或者是原型向量的更新程度已经小于了一个阈值。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>使用python对西瓜书上的示例复现代码如下（30个样本，9-21号样本标记为2，其它样本标记为1，随机选择5个样本作为原始向量，标记分别为1、2、2、1、1，学习率为0.1）</p><p>Data.py数据集部分:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">D = [</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0.697</span>, <span class="number">0.460</span>], [<span class="number">0.774</span>, <span class="number">0.376</span>], [<span class="number">0.634</span>, <span class="number">0.264</span>], [<span class="number">0.608</span>, <span class="number">0.318</span>], [<span class="number">0.556</span>, <span class="number">0.215</span>],</span><br><span class="line">    [<span class="number">0.403</span>, <span class="number">0.237</span>], [<span class="number">0.481</span>, <span class="number">0.149</span>], [<span class="number">0.437</span>, <span class="number">0.211</span>], [<span class="number">0.666</span>, <span class="number">0.091</span>], [<span class="number">0.243</span>, <span class="number">0.267</span>],</span><br><span class="line">    [<span class="number">0.245</span>, <span class="number">0.057</span>], [<span class="number">0.343</span>, <span class="number">0.099</span>], [<span class="number">0.639</span>, <span class="number">0.161</span>], [<span class="number">0.657</span>, <span class="number">0.198</span>], [<span class="number">0.360</span>, <span class="number">0.370</span>],</span><br><span class="line">    [<span class="number">0.593</span>, <span class="number">0.042</span>], [<span class="number">0.719</span>, <span class="number">0.103</span>], [<span class="number">0.359</span>, <span class="number">0.188</span>], [<span class="number">0.339</span>, <span class="number">0.241</span>], [<span class="number">0.282</span>, <span class="number">0.257</span>],</span><br><span class="line">    [<span class="number">0.748</span>, <span class="number">0.232</span>], [<span class="number">0.714</span>, <span class="number">0.346</span>], [<span class="number">0.483</span>, <span class="number">0.312</span>], [<span class="number">0.478</span>, <span class="number">0.437</span>], [<span class="number">0.525</span>, <span class="number">0.369</span>],</span><br><span class="line">    [<span class="number">0.751</span>, <span class="number">0.489</span>], [<span class="number">0.532</span>, <span class="number">0.472</span>], [<span class="number">0.473</span>, <span class="number">0.376</span>], [<span class="number">0.725</span>, <span class="number">0.445</span>], [<span class="number">0.446</span>, <span class="number">0.459</span>]</span><br><span class="line">]  <span class="comment"># 数据集，1~30，0索引不使用</span></span><br><span class="line"><span class="comment"># 数据集的标记，LVQ使用</span></span><br><span class="line">T = [</span><br><span class="line">    <span class="number">0</span>,</span><br><span class="line">    <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>,</span><br><span class="line">    <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>,</span><br><span class="line">    <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>,</span><br><span class="line">    <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>,</span><br><span class="line">    <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>,</span><br><span class="line">    <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span></span><br><span class="line">]</span><br><span class="line"><span class="comment"># 原始向量标记的输入</span></span><br><span class="line">q_vect = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>main.py主函数部分：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> K_means</span><br><span class="line"><span class="keyword">import</span> LVQ</span><br><span class="line"><span class="keyword">import</span> Data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(): </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;LVQ 的结果&quot;</span>, end=<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">    res = LVQ.lvq(Data.D, Data.T, Data.q_vect)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">len</span>(res[i]) - <span class="number">1</span>, end=<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(res[i]) == <span class="number">1</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;[]&quot;</span>, end=<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(res[i])):</span><br><span class="line">            <span class="keyword">if</span> j == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="built_in">print</span>(res[i][j], end=<span class="string">&quot; &quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(res[i]) != <span class="number">1</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot; &quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">        main()</span><br></pre></td></tr></table></figure><p>LVQ.py学习向量量化算法部分：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lvq</span>(<span class="params">D:[[<span class="built_in">float</span>]], T:[<span class="built_in">int</span>], q_vect:[]</span>):</span><br><span class="line">l_rate = <span class="number">0.1</span></span><br><span class="line"><span class="keyword">import</span> random <span class="keyword">as</span> rd</span><br><span class="line">q_vec_index = rd.sample(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">31</span>), <span class="number">5</span>)  <span class="comment"># 随机选取5个样本作为原型向量</span></span><br><span class="line">q_vec = []  <span class="comment"># 原型向量</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">5</span>):</span><br><span class="line">    q_vec.append(D[q_vec_index[i]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">10000</span>):  <span class="comment"># 迭代10000轮</span></span><br><span class="line">    j_dex = rd.randint(<span class="number">1</span>,<span class="number">30</span>)  <span class="comment"># 随机挑选一个样本，randint函数的参数是一个闭区间！</span></span><br><span class="line">    q = <span class="number">0</span> <span class="comment"># 距离j_dex最近的原型向量的索引</span></span><br><span class="line">    d = (D[j_dex][<span class="number">0</span>] - q_vec[<span class="number">0</span>][<span class="number">0</span>])**<span class="number">2</span> + (D[j_dex][<span class="number">1</span>] - q_vec[<span class="number">0</span>][<span class="number">1</span>])**<span class="number">2</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">5</span>):</span><br><span class="line">        d_i = (D[j_dex][<span class="number">0</span>] - q_vec[i][<span class="number">0</span>])**<span class="number">2</span> + (D[j_dex][<span class="number">1</span>] - q_vec[i][<span class="number">1</span>])**<span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> d_i &lt; d:</span><br><span class="line">            q = i</span><br><span class="line">            d = d_i</span><br><span class="line">    <span class="keyword">if</span> q_vect[q] == T[j_dex]:  <span class="comment"># 将原型向量与样本拉近</span></span><br><span class="line">        q_vec[q][<span class="number">0</span>] = q_vec[q][<span class="number">0</span>] + l_rate*(D[j_dex][<span class="number">0</span>] - q_vec[q][<span class="number">0</span>])</span><br><span class="line">        q_vec[q][<span class="number">1</span>] = q_vec[q][<span class="number">1</span>] + l_rate*(D[j_dex][<span class="number">1</span>] - q_vec[q][<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment"># 将原型向量与样本推远</span></span><br><span class="line">        q_vec[q][<span class="number">0</span>] = q_vec[q][<span class="number">0</span>] - l_rate * (D[j_dex][<span class="number">0</span>] - q_vec[q][<span class="number">0</span>])</span><br><span class="line">        q_vec[q][<span class="number">1</span>] = q_vec[q][<span class="number">1</span>] - l_rate * (D[j_dex][<span class="number">1</span>] - q_vec[q][<span class="number">1</span>])</span><br><span class="line">result = [[], [], [], [], []]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">5</span>):</span><br><span class="line">    result[i].append(q_vec[i])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">31</span>):</span><br><span class="line">    j = <span class="number">0</span></span><br><span class="line">    d = (D[i][<span class="number">0</span>] - q_vec[j][<span class="number">0</span>])**<span class="number">2</span> + (D[i][<span class="number">1</span>] - q_vec[j][<span class="number">0</span>])**<span class="number">2</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">5</span>):</span><br><span class="line">        d_ = (D[i][<span class="number">0</span>] - q_vec[_][<span class="number">0</span>])**<span class="number">2</span> + (D[i][<span class="number">1</span>] - q_vec[_][<span class="number">1</span>])**<span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> d_ &lt; d:</span><br><span class="line">            d = d_</span><br><span class="line">            j = _</span><br><span class="line">    result[j].append(D[i])</span><br><span class="line"><span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><p><code>捉个一个虫，py中万物皆对象，在初始化q_vec的时候直接将数据集D中的元素append进去，实际上共享了内存，这会导致更新原型向量的时候，数据集中的元素被更新，解决方案是使用深拷贝</code></p><p>修改方法：<br>将q_vec初始化的地方：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">q_vec = []  <span class="comment"># 原型向量</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">5</span>):</span><br><span class="line">    q_vec.append(D[q_vec_index[i]])</span><br></pre></td></tr></table></figure><p>更改为：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">q_vec = [x.copy() <span class="keyword">for</span> x <span class="keyword">in</span> (D[i] <span class="keyword">for</span> i <span class="keyword">in</span> q_vec_index)]  <span class="comment"># 关键修复点：使用列表拷贝</span></span><br></pre></td></tr></table></figure><p>最终运行结果：</p><blockquote><p>3<br>[0.36, 0.37] [0.478, 0.437] [0.446, 0.459]<br>8<br>[0.634, 0.264] [0.556, 0.215] [0.666, 0.091] [0.639, 0.161] [0.657, 0.198] [0.593, 0.042] [0.719, 0.103] [0.748, 0.232]<br>9<br>[0.403, 0.237] [0.481, 0.149] [0.437, 0.211] [0.243, 0.267] [0.245, 0.057] [0.343, 0.099] [0.359, 0.188] [0.339, 0.241] [0.282, 0.257]<br>5<br>[0.697, 0.46] [0.774, 0.376] [0.714, 0.346] [0.751, 0.489] [0.725, 0.445]<br>5<br>[0.608, 0.318] [0.483, 0.312] [0.525, 0.369] [0.532, 0.472] [0.473, 0.376]  </p></blockquote><p>对应为5个簇中样本的数量和相应的样本</p><p><code>注：理论上应该将数据集D划分为训练集和测试集，通过训练集训练模型（得到所有合理的原型向量），然后利用测试集测试，利用原型向量将测试集划分为对应数量的簇；这样才能完整地体现“机器学习”，但是这里只是一个简单的例子，将数据集D全用作训练集，得到原型向量，再把整个训练集划分为了对应的簇。</code><br><code>有了原型向量之后，划分为簇是很简单的，样本距离哪个原型向量最近，就纳入对应的簇即可。</code></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习（聚类） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>六级词汇</title>
      <link href="/2025/05/06/%E5%85%AD%E7%BA%A7%E8%AF%8D%E6%B1%87/"/>
      <url>/2025/05/06/%E5%85%AD%E7%BA%A7%E8%AF%8D%E6%B1%87/</url>
      
        <content type="html"><![CDATA[<p><code>这篇博客用于记录我在备考英语六级时所做的词汇准备，目前我已经完成了核心词汇的记忆，我会在这里为这些词汇补充一些例句。</code><br><code>目前的打算是每天20个词语，此外我会开始着手阅读、听力以及写译的准备，这篇blog中也会用于记录相应的内容</code></p><h2 id="词汇"><a href="#词汇" class="headerlink" title="词汇"></a>词汇</h2><h3 id="2025-05-06"><a href="#2025-05-06" class="headerlink" title="2025.05.06"></a>2025.05.06</h3><p>1</p><blockquote><p>stabilize  “How can I stabilize the colour of our love, my dear.”</p></blockquote><p>2</p><blockquote><p>manipulate “Your mean is that you can manipualte such a monster machine.”</p></blockquote><p>3</p><blockquote><p>ambiguous “Don’t treat me with such a ambiguous attitude.”</p></blockquote><p>4</p><blockquote><p>interaction “The relationship forms from the interaction among people little by little.”</p></blockquote><p>5</p><blockquote><p>perception “What’s your perception about our future?”</p></blockquote><p>6</p><blockquote><p>strive “I’ll strive to achieve our happiness in future.”</p></blockquote><p>7</p><blockquote><p>expense “How much is the expense of our lives in the month?”</p></blockquote><p>8</p><blockquote><p>cater “We will cater for the party.”</p></blockquote><p>9</p><blockquote><p>summon “I am trying to summon up the courage to love you.” “He was summoning the elevator.”</p></blockquote><p>10</p><blockquote><p>plug “The broken bus pluged the traffic.”</p></blockquote><p>11</p><blockquote><p>elaspe “I don’t think our love will elapse.”</p></blockquote><p>12</p><blockquote><p>authorize “The law won’t autorize anybody to kill.”</p></blockquote><p>13</p><blockquote><p>commentary “With a conception about freedom of expression, the commentraies online are too cynic and dirty sometimes.”</p></blockquote><p>14</p><blockquote><p>conservative “Some people are still conservative about sex, which’s treasurable.”</p></blockquote><p>15</p><blockquote><p>arson “People would arson to destory all the thing of a witch even her body in the middle centery, however many women called wicth are innocent.”</p></blockquote><p>16</p><blockquote><p>litre “How much water should a people to drink a day? 2 litres?”</p></blockquote><p>17</p><blockquote><p>vengeance “The lifelong traveling of Gess is to make vengeane.”</p></blockquote><p>18</p><blockquote><p>expenditure “He is rigorous in his contorl of expenditure.”</p></blockquote><p>19</p><blockquote><p>overwhelm “The fear overwhelmed in his mind.”</p></blockquote><p>20</p><blockquote><p>surpass “His achievement supasses anybody.”</p></blockquote><h3 id="2025-05-07"><a href="#2025-05-07" class="headerlink" title="2025.05.07"></a>2025.05.07</h3><p>1</p><blockquote><p>visualize “If you can visualize your success, you’ll succeed.”</p></blockquote><p>2</p><blockquote><p>tension “For several hours tension mounted.”</p></blockquote><p>3</p><blockquote><p>shrewd “She is a shrewd bussinesswoman indeed, however she is a kind mother as well.”</p></blockquote><p>4</p><blockquote><p>phase “For our aim of happiness, we are staying the phase of adapating to each other.”</p></blockquote><p>5</p><blockquote><p>abject “If you are down to abject poverty, you actually get everything.”</p></blockquote><p>6</p><blockquote><p>intense “The fight between Gess and Geri is intense.”</p></blockquote><p>7</p><blockquote><p>recruit “His dream is to be recruited as a chairman by Tiktok.”</p></blockquote><p>8</p><blockquote><p>abolish “The princess will abolish the man’s title of knight, granting a new title of lover to him.”</p></blockquote><p>9</p><blockquote><p>reluctant “I don’t want your hug because you are reluctant.”</p></blockquote><p>10</p><blockquote><p>vacant “The room is vacant so you can use it.”</p></blockquote><p>11</p><blockquote><p>priest “My priest, listen to my confession. Please.”</p></blockquote><p>12</p><blockquote><p>transparent “My heart is just like a transparent river.”</p></blockquote><p>13</p><blockquote><p>hamper “Why did you hamper me forever?”</p></blockquote><p>14</p><blockquote><p>rust “The blank-red rust grow on the sword, saying the pride from the past.”</p></blockquote><p>15</p><blockquote><p>sulphur “Whenever I see the word sulphur, I think of my chemistry teacher Mr.Fu.”</p></blockquote><p>16</p><blockquote><p>vivid “The birds on the picture is vivid.”</p></blockquote><p>17</p><blockquote><p>intensive “You need to keep intensive when you are taking exam.”</p></blockquote><p>18</p><blockquote><p>diagnose “The doctor diagnosed him, saying he just need to take a rest.”</p></blockquote><p>19</p><blockquote><p>convict “God will convict you!”</p></blockquote><p>20</p><blockquote><p>implement “When will we implement this law?”</p></blockquote><h3 id="2025-05-08"><a href="#2025-05-08" class="headerlink" title="2025.05.08"></a>2025.05.08</h3><p>1</p><blockquote><p>vertical “The wall is vertical.”</p></blockquote><p>2</p><blockquote><p>clinical “Clinical medicine is a difficuilt subject.”</p></blockquote><p>3</p><blockquote><p>thrill “When she heared sound like this, she thrilled from the bottom of feet to the upper head.”</p></blockquote><p>4</p><blockquote><p>paw “Don’t touch the paw of a cat.”</p></blockquote><p>5</p><blockquote><p>conscience “Where are your conscience?”</p></blockquote><p>6</p><blockquote><p>scope “This is a scope so you should scope this question to find where is the key.”</p></blockquote><p>7</p><blockquote><p>excceed “His power excceed a lot.”</p></blockquote><p>8</p><blockquote><p>indicative “How can you speak out with such a indicative tone.”</p></blockquote><p>9</p><blockquote><p>mayor “The mayor of the city is a lion.”</p></blockquote><p>10</p><blockquote><p>rectify “Should I rectify my teeth?”</p></blockquote><p>11</p><blockquote><p>commemorate “People made a sculpture to commemorate this hero.”</p></blockquote><p>12</p><blockquote><p>nourish “We are noursihed by the land we stand.”</p></blockquote><p>13</p><blockquote><p>lease “I make the lease so I lease the house.”</p></blockquote><p>14</p><blockquote><p>flee “She fleed away and will never go back.”</p></blockquote><p>15</p><blockquote><p>criterion “Who makes the criterion, whose talking work.”</p></blockquote><p>16</p><blockquote><p>credentials “I spend lots of time in my life to study so I got this credentials finally.”</p></blockquote><p>17</p><blockquote><p>notable “It’s not a notable thing.”</p></blockquote><p>18</p><blockquote><p>secure “Please secure that the ladder is secure so I can feel secure.”</p></blockquote><p>19</p><blockquote><p>gear “There is a little gear which is broken so the forth gear of the car can’t be used.”</p></blockquote><p>20</p><blockquote><p>axis “Fllow the axis so you can draw anything accurately.”</p></blockquote><h3 id="2025-05-09"><a href="#2025-05-09" class="headerlink" title="2025.05.09"></a>2025.05.09</h3><p>1</p><blockquote><p>filter “If you want a cup of clear water, you could use the filter.”<br>2<br>confine “Your imagination will confine your arrival at the further place.”<br>3<br>rumour “If you really love her, you shouldn’t believe the rumour.”<br>4<br>category “There are some categories of people, which means that good people and bad people all exist.”<br>5<br>paradox “Sometimes I think I am the paradox itself, which hurts her a lot.”<br>6<br>transistor “Do you konw about transister? It sounds like some high technology.”<br>7<br>illusion “Give up your illusion and be ready to fight!”<br>8<br>ingredient “I once heard that the ingredients of girl are sweet, flower and any other things of happiness.”<br>9<br>feasible “Your idea is exactly genius and feasible.”<br>10<br>petroleum “If you mention my monitor in high shcool, I will think of petroleum which has relation to her profession.”<br>11<br>revenue “How much was the revenue of our government in the past year.”<br>12<br>transplant “If you want to make the flower alive, you can transplant it to another land to nourish it. If you want to make a people alive, you can give him a transplant.”<br>13<br>earnest “Please don’t hurt him, he is a earnest boy.”<br>14<br>instinct “Animals’ instinct will make them own the capablity to live in wild.”<br>15<br>resort “Whenever I listen to the word resort, I will think of people everywhere.”<br>16<br>solitude “Do you want solitude? Although sometimes you will feel lonely, you are free at all.”<br>17<br>notion “The notions about MTL is not easy.”<br>18<br>silicon “The silicon has a deep relation to the production of CPU.”<br>19<br>provoke “This funtion is used to provoke the next layer of GNN.”<br>20<br>intuition “You can’t do everything based on your intuition and you need to think independently sometimes.”</p></blockquote><h3 id="2025-05-10"><a href="#2025-05-10" class="headerlink" title="2025.05.10"></a>2025.05.10</h3><p>1</p><blockquote><p>trial “Do you want to make a trial to chase her?”<br>2<br>dense “Someone will fear something tiny and tense.”<br>3<br>identical “There is no leaf identical to another one on the world.”<br>4<br>feminine “An expert said that sensitive psychology is feminine.”<br>5<br>tuition “The tuition from the teacher is fantastic but the tuition is expensive.”<br>6<br>empirical “I’m a empirical man and I’m not good at intution.”<br>7<br>predominant “A efficient algorithm should be predominant on the advantage of time or space.”<br>8<br>marvellous “Gess is a marvellous man with a life like legend.”<br>9<br>ignite “IGNITE!”<br>10<br>conquest “The conquest over her satisfy my hugury and thirsty.”<br>11<br>tenant “The tenant make lease with me yesterday.”<br>12<br>sphere “A sphere is an abstractive name of a kind of objects like basketball and football.”<br>13<br>negligible “You are actually negligible for me like a tiny sand swaying in the air.”<br>14<br>cynical “If you feel you are injured by the world, you can change it but never be a cynical people.”<br>15<br>tempt “The game is so tempting that he can’t focus on study.”<br>16<br>portion “Can you indentify these words:portion, composition, segment and part.”<br>17<br>competent “He is a competent man and he can be our leader.”<br>18<br>jury “She was finally commited by the lawyer and the jury.”<br>19<br>vibrate “The small tool is vibrating, making her thrilled constantly.”<br>20<br>session “They asked for the thrilling feeling from bottom of their feet to the upper head during the session of such a crazy activity.”</p></blockquote><h3 id="2025-05-11"><a href="#2025-05-11" class="headerlink" title="2025.05.11"></a>2025.05.11</h3><p>1</p><blockquote><p>upright “The tower on the hill is upright.”<br>2<br>estate “A beautiful woman was killed in the estate lived by a lot of people.”<br>3<br>symphony “The symphony played by us sounds so smooth.”<br>4<br>medium “His ablity is medium among these people.”<br>5<br>commence “Let’s commence this job.”<br>6<br>exceptional “His advancement is exceptional.”<br>7<br>induce “You can induce him but not persude him.”<br>8<br>drought “This drought lasted 20 years, making no life here again.”<br>9<br>executive “He is recruited as the executive.”<br>10<br>initiative “She offers us a new initiative with the high initiative.”<br>11<br>riot “Tension occurred, society is turbulent and riots always appeared in everywhere.”<br>12<br>affirm “I can make an affirm that she isn’t a girl like that.”<br>13<br>outward “You can touch the outward texture. It’s so soft.”<br>14<br>denial “You can make a denial for me but I’ll never give up.”<br>15<br>faculty “The faculty refers to all the teacher of a colleage.”<br>16<br>supervise “We must make a principle to supervise officers.”<br>17<br>imperative “The task is so imperative that we must handle it now.”<br>18<br>mingle “If you mingle something, you have mixed them.”<br>19<br>obscure “Dying as a well-konwn hero or obscure as normal people, which one would you choose?”<br>20<br>forth “He paced back and forth.”</p></blockquote><h3 id="2025-05-12"><a href="#2025-05-12" class="headerlink" title="2025.05.12"></a>2025.05.12</h3><p>1</p><blockquote><p>shuttle “Which shuttle wii you choose, if you want to go to the western coast. The plane shuttles to carry apples?”<br>2<br>personnel “Look at me! Personnel in attendance!”<br>3<br>intensify “What hurts my heart will intensify my power.”<br>4<br>turbulent “Intension occurred and society was turbulent.”<br>5<br>attendant “Two attandants of the KTV saw a monster and the anttendant disaseter.”<br>6<br>prospect “Do you think that we have the prospect of our future.”<br>7<br>texture “The texture of the cloth makes me feel comfortable.”<br>8<br>span “The ache of the wound is spanning and the span has been more than 12 hours.”<br>9<br>tragic “Your targic destiny makes you great.”<br>10<br>commodity “She is not a commodity and you should respect her.”<br>11<br>downfall “The reason of Osman’s downfall began from 2000 years ago.”<br>12<br>timber “We can use the timber to arson.”<br>13<br>accessory “I am not your accessory!”<br>14<br>ridiculous “You want to change her, a selflish woman, which is so ridiculous!”<br>15<br>textile “The country’s prosperity is based on the textile.”<br>16<br>composition “What’s the composition of the liquid.”<br>17<br>bribe “Don’t want to bribe him because he is a judge with justice.”<br>18<br>treaty “There are two treaties between me and Lele.”<br>19<br>archive “The archive showes us his experience of honor.”<br>20<br>retreat “Don’t retreat! We have guts, a very awesome man.”</p></blockquote><h3 id="2025-05-13"><a href="#2025-05-13" class="headerlink" title="2025.05.13"></a>2025.05.13</h3><p>1</p><blockquote><p>dazzle “A dazzle dazzles my eyes.”<br>2<br>chronic “I’m so sorry to tell you that you have a chronic disease.”<br>3<br>compromise “I would never make a compromise on the view of love even for you.”<br>4<br>flourish “I hope that you can grow up well like plants flourishing under the sun.”<br>5<br>occupancy “The lasting occupancy of the computer results into the overflow of memory.”<br>6<br>vulnerable “Although his body is weak, his heart is never vulnerable.”<br>7<br>exclusive “For the president, the car is exclusive.”<br>8<br>favourable “Everybody holds a favourable attitude for the decision to recurit him as manager.”<br>9<br>spacecraft “The spacecraft is mainly made by element called Al.”<br>10<br>preserve “We are supposed to preserve the environment.”<br>11<br>analogy “How do you dare to make an analogy between a poor guy and the king.”<br>12<br>orient “You must orient, if you wanna to drive a ship on the ocean.”<br>13<br>warehouse “You can pile your goods in the warehouse.”<br>14<br>testify “I can testify that he is a good man.”<br>15<br>petty “Lele is actually a petty girl.”<br>16<br>cargo “Cargo is another name of goods.”<br>17<br>reckon “Gery reckons as a hero.” 一个本就含有被动语态的词语<br>18<br>bind “Binding a name to a object is the begining of artifical intelligence.”<br>19<br>expertise “The management of stack is the expertise of a CS students.”<br>20<br>conform “I won’t conform you without my principle.” “This action dosen’t conform our rules.”</p></blockquote><h3 id="2025-05-14"><a href="#2025-05-14" class="headerlink" title="2025.05.14"></a>2025.05.14</h3><p>1</p><blockquote><p>ego “If you can feel your own ego, you will acquire your real happiness.”<br>2<br>deficiency “The deficiency of your ego results in the regret of your life.”<br>3<br>assemble “We assemble something to get a set.”<br>4<br>punch “He was punched to death.”<br>5<br>irritate “Don’t irritate him otherwise he would kill you.”<br>6<br>patch “The patch on the pant makes her so sexy.”<br>7<br>prompt “A prompt prompt prompts him to give a right answer.”<br>8<br>oblige “I wanna to kiss you but I won’t oblige you until you are relunctant.”<br>9<br>correspondent “The correspondents are essential because they can deliver the information concerning the battle.”<br>10<br>flock “A flock of flock follow the God they trust.”<br>11<br>ponder “Confronted with this problem he pondered all the night.”<br>12<br>provision “The provision of food comfrot us a lot.”<br>13<br>stem “The stem of the flower is so thin.”<br>14<br>anticipate “I have anticipated that you might leave me one day but actually we will never leave each other during the life.”<br>15<br>threshold “The Forward Phase is just a threshold of your lengend jounery to pary for Elden Ring.”<br>16<br>expire “Don’t be unforgettable about your youngest which had expired and be concertrated on present.”<br>17<br>profess “I can’t believe you still profess that you love me even that you betray me.”<br>18<br>severe “She made such a severe worry and he won’t forgive her.”<br>19<br>ornament “We make quantities of bright ornaments to ornament our shop.”<br>20<br>germ “On the place you can’t see do exist lots of germs.”</p></blockquote><h3 id="2025-05-15"><a href="#2025-05-15" class="headerlink" title="2025.05.15"></a>2025.05.15</h3><p>1</p><blockquote><p>reconcile “Do you think that I can’t reconcile myself to the prospect of losing you?”<br>2<br>transit “The car to transit money is protected carefully.”<br>3<br>whereas “I don’t want to leave you whereas we can’t reconcile our contradiction.”<br>4<br>prominent “Success in this competition is a prominent achievement.”<br>5<br>mount “You can mount the file system or mount a horse to go out!”<br>6<br>naval “The naval power of the country is strengthful.”<br>7<br>waist “I wanna to hold your waist.”<br>8<br>academy “Don’t look down upon a student of the academy, especially in Germany.”<br>9<br>isolate “She is isolated by all of them.”<br>10<br>acute “This is an acute problem and you should note it in details.”<br>11<br>feeble “Though he is phsically feeble, his soul is strong.”<br>12<br>pledge “You have pledged to me that you won’t leave me forever.”<br>13<br>pension “Don’t use the pension of your parents otherwise you would really make me disappointed.”<br>14<br>terminate “Could we terminate this crazy plan?”<br>15<br>assert “I can assert that I won’t make you lose!”<br>16<br>conceal “Where do you want to conceal yourself? No matter where you go to, I will find you finally.”<br>17<br>skim “If you just skim what I have said or what I have done, you will never really know me.”<br>18<br>authentic “I don’t know what the authentic character of me is because I am a people full of paradox.”<br>19<br>presumably “Presumably, she has never produced heartbeat for you so she can leave you easily.”<br>20<br>masculine “Although you just like a puppy confronted with her, I know you are actually masculine.”</p></blockquote><h3 id="2025-05-18"><a href="#2025-05-18" class="headerlink" title="2025.05.18"></a>2025.05.18</h3><p>1</p><blockquote><p>applaud “Everyone at presence applauds for what he has done.”<br>2<br>intrigue “Such a strange thing intrigues him who is curious.”<br>3<br>delicate “You are supposed to treat her carefully because her heart is delicate.”<br>4<br>consolidate “What I have gotten could consolidate my confidence.”o<br>5<br>cable “Stay away from the cable otherwise you will get an electric shock.”<br>6<br>manifest “This is a manifest thing that manifests your loss!”<br>7<br>catastrophe “Looking at the broken world makes me know what a real catastrophe is.”<br>8<br>offensive “I’m so sorry that what I had said is offensive.”<br>10<br>dwell “Where your body dwells in, where your heart is then you will feel happy.”<br>11<br>eliminate “Why do you always want to eliminate who holds a different idea? You won’t be welcome.”<br>12<br>grief “Sometimes leaving from you would result in grief of ours but we all know that it’s the best anwser.”<br>13<br>concede “I’m relieved that you’d like to concede the wrong made by you.”<br>14<br>stagger “He is drunk deeply and he is just staggering.”<br>15<br>colonial “Our country will never be your colonial land!”<br>16<br>liquor “Liquor is a liquid will make you drunk.”<br>17<br>negotiation “The negotiation will be held concerning the theme of convicting the convict.”<br>18<br>peculiar “I’m peculiar sometimes because I am the parodox itself.”<br>19<br>confess “Your confessing will make us relieved.”<br>20<br>hoist “People will hoist a witch.”</p></blockquote><h3 id="2025-05-19"><a href="#2025-05-19" class="headerlink" title="2025.05.19"></a>2025.05.19</h3><p>1</p><blockquote><p>breadth “Breadth means a big width.”<br>2<br>convene “We will convene a class meeting after this class.”<br>3<br>tremendous “What a tremendous script.”<br>4<br>toss “You make make a toss to decide which one to be tossed.”<br>5<br>utter “He utters this speaking with an utter justice.”<br>6<br>flap “Don’t use the flap to flap your belly.”<br>7<br>notorious “Don’t stay with her otherwise you will be notorious.”<br>8<br>uphold “Everyone should uphold the pride of constitution.”<br>9<br>cruise “This period of time of cruise makes me unforgettable, crusing on the sea and feeling the wind.”<br>10<br>overlap “You are supposed to notice the overlap among these books, which is important.”<br>11<br>scrape “Stop scraping the blackboard. We can’t stand it!”<br>12<br>insult “Someone is just cheap to like being insulted.”<br>13<br>permeate “Don’t let the virus permeating your blood or you will die.”<br>14<br>liability “Liability is responsibility from some aspects.”<br>15<br>comply “complying somebody means you conform him or her.”<br>16<br>harsh “Sometimes the reallity is harsh.”<br>17<br>stereotype “Nowadays Gaokao in China has become a kind of stereotype.”<br>18<br>generalize “If you can generalize this paper, you will understand it more deeply.”<br>19<br>defect “Her defects are that she is stingy sometimes.”<br>20<br>scheme “I want to see a scheme to explain your plans.”</p></blockquote><h3 id="2025-05-20"><a href="#2025-05-20" class="headerlink" title="2025.05.20"></a>2025.05.20</h3><p>1</p><blockquote><p>folklore “The folklore in China is that two people who want to get married with each other should ask the suggestions of their parents.”<br>2<br>drown “The huge and grand water will drown us.”<br>3<br>loose “Don’t touch it! It’s about to loose.”<br>4<br>transaction “If you make a deal, you would make a transaction.”<br>5<br>trivial “Trivial thing is something negligible.”<br>6<br>proportion “This proportion of our enterprise is belong to Mr.L, a beautiful lady.”<br>7<br>concrete “The concrete is concrete thing which isn’t abstract.”<br>8<br>confidential “We all are supposed to protect the confidential material of our country.”<br>9<br>portray “Can you portray the beautiful view of the spring.”<br>10<br>additive “We don’t want the food with lots of additive.”<br>11<br>flip “When you toss the coin is flipped to the air.”<br>12<br>furnish “We can furnish you with some desks and chairs to furnish your home.”<br>13<br>suspicion “I don’t like your suspicion even that I can understand you.”<br>14<br>refute “Don’t incline to refute me and be patient to listen to me.”<br>15<br>migrate “Our ancestors migrated from a very far place to here.”<br>16<br>beam “I want the girl to beam because the smile of her is just like quantities of beam lighten my heart.”<br>17<br>token “There are lots of tokens so you don’t need money.”<br>18<br>navie “Confronted with love we are all navie.”<br>19<br>corporate “As a person recruited by us you are supposed to think of the corporate interests.”<br>20<br>emit “Your smile can emit the beam.”</p></blockquote><h3 id="2025-05-21"><a href="#2025-05-21" class="headerlink" title="2025.05.21"></a>2025.05.21</h3><blockquote></blockquote><p>1</p><blockquote><p>curl “Her hair is curled and she has the curl.”<br>2<br>funeral “I won’t attend your funeral because it would make me sad.”<br>3<br>postpone “This meeting is postponed so we should attend this meeting on next monday.”<br>4<br>polish “He polish his sword so that it is shiny.”<br>5<br>wander “Don’t wander in front of me, which will distract my attention.”<br>6<br>deputy “She is the deputy of the policeman and she is a good assistant.”<br>7<br>coward “He almost fears everything so he is a coward.”<br>8<br>missile “If the country still offends the pride of China, we will treat it with missile.”<br>9<br>allege “If you allege something, you assert something.”<br>10<br>destiny “Don’t just accept the destiny and fight back!”<br>11<br>facilitate “This strategy of our country facilitate our life, which means that it makes our life more convenient.”<br>12<br>revolve “Let us revolve this sphere and you won’t feel anything changed.”<br>13<br>doctrine “Do you want the priest to tell you our doctrine that ‘God always right’.”<br>14<br>distinct “The difference among these distinct species is distinct.”<br>15<br>ascertain “We are supposed to ascertain the truth of this case.”<br>16<br>symptom “If a doctor want to diagnose what’s disease she has, he will be based on the symptom.”<br>17<br>drift “We stay on the small boat and drift in the river.”<br>18<br>inherent “Kindness is an inherent characteristic of her which forms from the experience of her life so it won’t change.”<br>19<br>aggravate “Somking will aggravate the disease in her lung.”<br>20<br>alternative “There are lots of women one of whom will become an alternative of her.”</p></blockquote><h3 id="2025-05-22"><a href="#2025-05-22" class="headerlink" title="2025.05.22"></a>2025.05.22</h3><blockquote></blockquote><p>1</p><blockquote><p>obedient “I am not an obedient person so I won’t comply you.”<br>2<br>lane “There is only a lane which can’t accommodate such a big truck.”<br>3<br>cite “I can cite lots of proofs to prove you never love him.”<br>4<br>tissue “She is crying and I guess that she might need some tissues.”<br>5<br>refrain “You should refrain your desires and keep calm to study continually.”<br>6<br>integrity “Integrity is a quality of mine so I won’t lie to you.”<br>7<br>evaporate “The water had evaporated and there is nothing now.”<br>8<br>fragment “How can we assemble a whole sotry with these fragments.”<br>9<br>antique “This is just an antique legend which began from 5,000 years ago.”<br>10<br>clumsy “Her feet is too clumsy to do this.”<br>11<br>venture “A venture means that some business like adventure but there is possibly a good payoff.”<br>12<br>contaminate “We should protect our environment and not contaminate the river.”<br>13<br>toxic “This water is toxic so you don’t drink it!”<br>14<br>curb “Curb your horse! It are almost crazy!”<br>15<br>ingenious “The CPU is an ingenious product which is full of people’s wisdom.”<br>16<br>intricate “This layout of our estate is intricate so follow me closely otherwise you will miss your direction.”<br>17<br>conspicuous “This is a conspicuous result that she dosen’t love you anymore.”<br>18<br>asset “We should be an asset but not just be a person with lots of asset.”<br>19<br>primitive “The primitive human used the fire to protect themselves.”<br>20<br>embody “You should tell me your advancements to embody your ability.”</p></blockquote><h3 id="2025-05-23"><a href="#2025-05-23" class="headerlink" title="2025.05.23"></a>2025.05.23</h3><blockquote></blockquote><p>1</p><blockquote><p>stationary “I juat want the stationary relation with people.”<br>2<br>radical “You are too radical to find out the radical reason.”<br>3<br>ritual “Following the ritual rule or as a part of the ritual, you should give a hug to each other.”<br>4<br>subtle “A subtle rabbit would have three homes.”<br>5<br>precaution “You should make a good precaution before you have a intimate contact with her.”<br>6<br>roar “The wolf is roaring and you are going to die.”<br>7<br>subordinate “Taiwan is sunordinate to China.”<br>8<br>indignant “He is angry and indignant.”<br>9<br>intrinsic “An intrinsic characteristic is inherent.”<br>10<br>After a period of time, there will be a new prevalent thing being popular among people.<br>11<br>grieve “Don’t grieve because it’s normal about the change of our relation.”<br>12<br>diplomatic “This diplomatic event symbolize the improvement of the relationship between China and the US.”<br>13<br>tuck “Just tuck it into my body and it will thrill me too much.”<br>14<br>alienate “The remote distance won’t alienate us.”<br>15<br>banner “What written on the banner tell us the truth.”<br>16<br>exile “The princess was exiled and was down to be a hooker.”<br>17<br>remedy “This remedy will remedy your condition of the disease.”<br>18<br>preach “The priest is preaching.”<br>19<br>precede “My love to her precede you.”<br>20<br>exert “I will exert myself to make it.”</p></blockquote><h3 id="2025-05-24"><a href="#2025-05-24" class="headerlink" title="2025.05.24"></a>2025.05.24</h3><blockquote></blockquote><p>1</p><blockquote><p>withstand “You can’t withstand the pain.”<br>2<br>deem “Do you deem the reason? I don’t believe it.”<br>3<br>retail “Please retail the condition about the change of the price when we take a way of retail.”<br>4<br>predecessor “He is the predecessor of her.”<br>5<br>flaw “This is just a flaw and don’t deem the mistake.”<br>6<br>destined “Can you take us to our destined room.”<br>7<br>epidemic “This epidemic will destory our country and many people will die because of the disease.”<br>8<br>gossip “Don’t deem the gossip because it is just the rumor.”<br>9<br>ward “Stay away from the ward because the patients were illed by the epidemic and you might be illed as well.”<br>10<br>integrate “If you are not usd to being alone you can integrate yourself into a group.”<br>11<br>tackle “If you can tackle something, you can deal with something.”<br>12<br>perceive “Can you perceive that she don’t understand you.”<br>13<br>fluctuate “Don’t let your emotion fluctuate and calm down to think and exert.”<br>14<br>sediment “Look at the sediment from the water, which shows it’s toxic.”</p></blockquote><h2 id="阅读"><a href="#阅读" class="headerlink" title="阅读"></a>阅读</h2><h2 id="写译"><a href="#写译" class="headerlink" title="写译"></a>写译</h2><h2 id="听力"><a href="#听力" class="headerlink" title="听力"></a>听力</h2><h3 id="第一套测试题"><a href="#第一套测试题" class="headerlink" title="第一套测试题"></a>第一套测试题</h3><blockquote></blockquote><p>1</p><blockquote><p>A and B have conspired to do “A与B共同作用导致了…”<br>2<br>rarely(&#x2F;ˈrerli&#x2F;)对比really(&#x2F;ˈriːəli&#x2F;)<br>3<br>tranquility “安静，安宁”<br>4<br>contemplation  “沉思” contemplate v.<br>5</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> 大学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计组复习</title>
      <link href="/2025/05/03/%E8%AE%A1%E7%BB%84%E5%A4%8D%E4%B9%A0/"/>
      <url>/2025/05/03/%E8%AE%A1%E7%BB%84%E5%A4%8D%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p><code>我在大二下选修了计算机组成原理，这篇blog用来梳理相关知识点</code></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>一些学习计算机组成原理之前应该知道的知识…</p><ol><li>计算机结构：系统程序员所能见到的<strong>硬件特性</strong>，指的是计算机的<strong>逻辑结构</strong></li><li>计算机组成：计算机硬件的<strong>具体实现</strong>，指的是计算的<strong>物理结构</strong></li><li>两类汇编语言，RISC &amp; CISC，对应精简与复杂的指令系统，MIPS属于RISC的一种</li><li>计算机组成原理涉及：汇编，处理器、内存、IO三者对应的逻辑系统与硬件实现（数据通路），课程定位在整个计算机系统中处于硬件方面的数字电路之上，软件层面的操作系统之内（因为上到汇编），但在编译器之下（编译器同样属于OS的范畴）</li><li>核心内容：CPU Organization(data path &amp; controller), Caches</li><li>重点内容：MIPS汇编，Virtual Memory</li><li>了解内容：I&#x2F;O, Bus</li></ol><p><strong>最后请谨记，该门课特点是概念抽象、繁琐…但是“清澈见底”，只要想弄清楚，一定可以！</strong></p><h2 id="指令系统"><a href="#指令系统" class="headerlink" title="指令系统"></a>指令系统</h2><h3 id="指令系统设计"><a href="#指令系统设计" class="headerlink" title="指令系统设计"></a>指令系统设计</h3><p><code>这一部分主要是一些有关指令系统设计的知识点</code></p><p>于是，首先看看这三个知识点：</p><ol><li>指令：二进制的机器语言</li><li>汇编指令：助记符，每种条符号语句都映射到一条二进制的机器代码</li><li>ISA：指令系统（指令集体系结构），<strong>软硬件交汇的地方</strong></li></ol><p>接下来，一条指令应该包含以下信息：</p><ol><li>操作码（定长 or 变长）</li><li>源操作数参照（from where）</li><li>目的位置参照（to where）</li><li>下一条指令地址（what to do next?）</li></ol><p>按地址数的指令分类：</p><p>零、一、二、三、多地址指令，其中二三是典型的RISC风格。三的特点是显示指定了dst，一或二的dst是隐含的（built-in or src）</p><p>指令执行的阶段：</p><p>取指令-&gt;译码-&gt;取操作数-&gt;运算（执行）-&gt;存放结果-&gt;取下一条指令<br><strong>不一定所有指令都涉及所有步骤，但是考虑的时候应该按最复杂的来，何尝不是一种设计原则？</strong></p><p>指令设计基本原则：<br>完备性，兼容性，均匀性，可扩展性<br><strong>应当明白词语背后的含义</strong></p><p>最简单的完备指令系统：<br>load, store, inc, brn</p><h3 id="操作数类型"><a href="#操作数类型" class="headerlink" title="操作数类型"></a>操作数类型</h3><p>…</p><h3 id="寻址方式"><a href="#寻址方式" class="headerlink" title="寻址方式"></a>寻址方式</h3><p>…</p><h3 id="扩展操作码编码"><a href="#扩展操作码编码" class="headerlink" title="扩展操作码编码"></a>扩展操作码编码</h3><p>这是涉及关于如何给操作码编码，以及对应数量关系的问题。<br><strong>核心思想是一种数字状态，一个编码</strong></p><p>涉及到的相关信息有：</p><ol><li>指令字长，例如16位、32位…</li><li>地址长度，如6位…</li><li>操作码长度，通常不同地址数量的编码不同</li><li>不同地址数的指令的条数</li></ol><p>通常会已知1、2，4中某些地址数的指令条数，求剩余一种地址数的指令<strong>最多</strong>的条数。</p><p>关键点是：</p><ol><li>明确<strong>已知的某些地址数的指令条数——剩下的一种地址数的指令条数</strong>一定存在函数关系</li><li>从多地址数的指令开始考虑，考虑它的操作码有多少位，可求得这种指令至多有多少条</li><li>利用“已知”的实际条数与至多有多少条，可以求得这种指令的<strong>剩余状态</strong>数量</li><li>考虑减少一条地址的指令，对应操作码有多少位，<strong>记得计算操作码长度的时候，不仅是指令字长减去地址长度，还要减去上种指令操作码所用长度</strong></li><li>求得这种指令至多有多少条，利用<strong>剩余状态</strong>×<strong>操作码长度</strong></li><li>显然这个过程可以反复进行，由地址数量最多的情况，如3个地址码，到最少的情况，如零地址码<br><strong>最后，不一定所有的状态都有使用…</strong></li></ol><h3 id="指令设计的风格"><a href="#指令设计的风格" class="headerlink" title="指令设计的风格"></a>指令设计的风格</h3><p>尤其关注RISC的风格。</p><p>RISC是<strong>load&#x2F;store型指令系统</strong>，特点是只有load、store命令才能访问存储器，其它运算类的指令通通不能访问存储器<br>（值得注意的是这种指令系统，属于<strong>通用寄存器型指令系统</strong>的子集，特点是使用通用寄存器存放临时数据，而不使用累加器）</p><p>RISC的特点是：</p><ol><li>指令数目少</li><li>指令格式规整</li><li>Load&#x2F;store风格</li><li>采用流水线的指令执行方式</li><li>采用大量通用寄存器</li><li>采用硬连线控制器</li><li>采用优化的编译器</li></ol><h3 id="异常与中断"><a href="#异常与中断" class="headerlink" title="异常与中断"></a>异常与中断</h3><p>…</p><h3 id="程序的机器级表示（MIPS指令系统）"><a href="#程序的机器级表示（MIPS指令系统）" class="headerlink" title="程序的机器级表示（MIPS指令系统）"></a>程序的机器级表示（MIPS指令系统）</h3><p><code>这一部分是重点知识，所以会有多级的副标题。请着重掌握！</code></p><h4 id="MIPS有关的基础知识"><a href="#MIPS有关的基础知识" class="headerlink" title="MIPS有关的基础知识"></a>MIPS有关的基础知识</h4><p><code>一些零碎的知识点，不好纳入后面的各级标题之中，于是集中在此...</code><br><code>或者说并非无法纳入，而是比较重要...单拎出来也方便记忆</code></p><ol><li>MIPS指令长度都是32位</li><li>MIPS中设计了32个通用寄存器</li><li>MIPS使用大端的存储方式</li><li>MIPS设计的存储器按照字节编址，1Byte对应一个存储单元，有自己的专属地址</li><li>MIPS中人为修改pc的指令，如j、beq等，在机器级存储的转移值是相对转移的指令的条数（即应该修改pc的相对量除以4后的值）</li></ol><h4 id="MIPS指令的机器级表示"><a href="#MIPS指令的机器级表示" class="headerlink" title="MIPS指令的机器级表示"></a>MIPS指令的机器级表示</h4><p>MIPS中指令格式包括R型、I型、J型。</p><h5 id="1-R型指令"><a href="#1-R型指令" class="headerlink" title="1.R型指令"></a>1.R型指令</h5><p>机器级表示:<br>op6+rs5+rt5+rd5+shamt5+func5</p><ol><li>op是操作码，对于R型来说全是0</li><li>shamt是用于处理移位操作的</li><li>func是用于区分操作码的</li><li>rs, rt为源寄存器1、2</li><li>rd为目的寄存器</li></ol><p><strong>注意，R型指令助记符表示的时候，实际上是 op rd rs rt的顺序，要和机器级位置区分开</strong><br><code>不妨考虑一下，op全为0的好处是什么</code></p><h5 id="2-I型指令"><a href="#2-I型指令" class="headerlink" title="2.I型指令"></a>2.I型指令</h5><p>机器级表示：<br>op6+rs5+rt5+imm16</p><ol><li>常用I型指令：双目运算，rs与imm运算，送至rt，例如<code>addi $2,$1,imm</code></li><li>常用I型指令，load、store，采用的<strong>MIPS采用基址+相对位移量</strong>的访存方式，例如<code>lw $2,100($1)</code></li><li>常用I型指令，beq、bne，条件分支，例如<code>beq $1,$2,L</code></li><li>imm是16位，但是与其运算的寄存器rs是32位的，需要进行扩展，扩展的规则如下：<br>①用于进行双目运算的时候，i是符号扩展imm，iu是零扩展imm<br>②用于load、store的时候，imm总是符号扩展，虽然有u的存在，例如lhu, lbu等，但是这里的u对应的用于0扩展不足32位的存储内容，从而加载到寄存器<br>③用于条件分支的时候，beq、bne应该与slt搭配使用，所以imm通常只与1、0进行相等与否的比较，于是也不存在什么扩展与否的问题<br><strong>注意，实际上slt也可以是I型的，并且很常用，但是也有R型的</strong></li></ol><h5 id="3-J型指令"><a href="#3-J型指令" class="headerlink" title="3.J型指令"></a>3.J型指令</h5><p>机器级表示：<br>op6+target address26</p><ol><li>实际的目的地址计算方式，pc高4位:target address:00<br>最后总是00的原因，在于MIPS中指令总是32位的，从0地址开始访存：0,100,1000,1100…（0、4、8、12…），末尾总是00，这个特点在后面设计用于MIPS的CPU的时候也非常有用</li><li>显然，这是一种局部寻址的方式</li></ol><h4 id="MIPS设计的通用寄存器"><a href="#MIPS设计的通用寄存器" class="headerlink" title="MIPS设计的通用寄存器"></a>MIPS设计的通用寄存器</h4><p>MIPS使用32个通用寄存器，我们应该掌握以下有关的知识。</p><h5 id="1-两种助记符号的使用"><a href="#1-两种助记符号的使用" class="headerlink" title="1.两种助记符号的使用"></a>1.两种助记符号的使用</h5><ol><li>编号：”$”+“数字：0~31”</li><li>名称</li></ol><p><code>程序中一般都使用名称，举例子的时候使用编号多一些。或许，前者体现“助记”，后者体现“通用”</code></p><h5 id="2-经常使用的寄存器"><a href="#2-经常使用的寄存器" class="headerlink" title="2.经常使用的寄存器"></a>2.经常使用的寄存器</h5><ol><li>zero<br>编号为0，其功能是提供0值，寄存器中始终是全0</li><li>v0-v1<br>编号2-3，功能是存放过程调用的返回值</li><li>a0-a3<br>编号4-7，功能是存放过程调用的参数</li><li>t0-t7<br>编号8-15，功能是存放临时使用的变量</li><li>s0-s7<br>编号16-23 被调用者保存的寄存器</li><li>t8-t9<br>编号24-25，功能是存放临时使用的变量</li><li>sp, fp, ra<br>编号29-31，功能是，栈指针（栈顶），帧指针（栈底），存放调用过程返回地址</li></ol><p>记忆的方法，“一个过程调用，使用了4个参数a，返回了两个值v，调用者保存了8个寄存器s，被调用者保存了10个寄存器t，关键在于sp、fp与ra”+zero</p><h5 id="3-了解的寄存器"><a href="#3-了解的寄存器" class="headerlink" title="3.了解的寄存器"></a>3.了解的寄存器</h5><ol><li>at<br>编号为1，保留给编译器使用</li><li>k0-k1<br>编号为26-27，保留给系统使用</li><li>gp<br>编号为28，全局指针</li></ol><h4 id="MIPS汇编指令"><a href="#MIPS汇编指令" class="headerlink" title="MIPS汇编指令"></a>MIPS汇编指令</h4><p><code>在开始做这一部分的笔记之前，我思考了一个问题——如何才能更好的记忆MIPS汇编指令。我得出的答案是，一般性规律的记忆+特殊性个例的记忆。对于一般性规律的记忆，其规律包括：指令的助记符+什么类型的指令+指令的特性，前两者可以帮助我们正确地写出指令，后者可以帮助我们正确地理解指令。对于特殊性个例，我们不妨记住全部。在记忆的过程中带着这个思想，或许会容易记忆一些。</code></p><p>我们接下来按照指令的类别进行。</p><h5 id="1-算术类指令"><a href="#1-算术类指令" class="headerlink" title="1.算术类指令"></a>1.算术类指令</h5><ol><li>算术运算包括，加、减、乘、除<br>对应的基本助记符是add、sub、mult、div</li><li>加、减有I型和R型，使用I型的时候，如addi、subi</li><li>加、减默认会判断溢出，也有不判断溢出，对应u扩展（undo），如addu、subu<br><strong>当然也有addiu、subiu</strong></li><li>乘、除比较特殊，仅有R型，但是是双目操作符，因为结果存放在默认寄存器hi，lo（乘法hi，lo分别为高低32位，除法hi为32位余数，lo为32位商）</li><li>乘、除分有符号数和无符号数，对应为u扩展（unsigned），如multu、divu<br><strong>注意区分unsigned和undo</strong></li></ol><p><code>一般性的规律是，助记符：加、减有I、R型，乘、除只有R型</code></p><h5 id="2-存储访问"><a href="#2-存储访问" class="headerlink" title="2.存储访问"></a>2.存储访问</h5><p>存储访问，按照访存字节，分为按字（word，MIPS中是32位），按半字（half word，16位），按字节（byte，8位）访问</p><p>以及MIPS最有特色的指令<strong>lui</strong></p><ol><li>lw、sw，按字访问lw&#x2F;sw $1 100($2)，sw是MIPS中唯一一个dst在src之后的指令</li><li>lhu、lbu，按半字、字节加载，16位内存到32位存储器涉及扩展，格式与lw、sw相同，内存中数据是按无符号扩展<br><strong>注意，必须加u，这就意味着使用半字或字节的时候，内存中内容只能按字节扩展到寄存器</strong><br><strong>但是imm的扩展只能是符号扩展，没有undo的选择</strong></li><li>sh、sb，按半字、字节存储，格式与lw、sw相同，由于是寄存器到存储器，不用考虑扩展。<br><strong>sh、sb同样的只要是访存，imm都是按符号扩展</strong></li><li>lui，I型，使用方法，lui rs,imm，将16位imm放置在rs的高16位<br><em>这是一个很能体现MIPS特色的指令，如果程序员，<del>按照自己的想象</del>（这个数字并不存在于任何其它的位置），想要将一个32位的数字放置在寄存器中，就可以按照16位、16位的存放。可以先lui，再addi</em><br><strong>按照自己的想象是很重要的一点！这将lui和lw、lhu、lbu区分开来，因为lui根本没有访存！</strong></li></ol><p><code>一般性的规律是，助记符；访存都是I型</code></p><h5 id="3-逻辑运算"><a href="#3-逻辑运算" class="headerlink" title="3.逻辑运算"></a>3.逻辑运算</h5><p>MIPS中常用的逻辑运算是与、或、异或</p><ol><li>与，and，有R型、I型，I型对应i扩展，如addi rd,rs,rt</li><li>或，or，同与</li><li>异或，xor，同与</li></ol><p><code>一般性的规律是，助记符，逻辑运算有R型、I型</code></p><h5 id="4-移位操作"><a href="#4-移位操作" class="headerlink" title="4.移位操作"></a>4.移位操作</h5><p>MIPS中的涉及的移位操作有，逻辑左、右移动，算术右移</p><ol><li>逻辑左移，sll，I型，如sll rt,rs,imm</li><li>逻辑右移，srl，同上</li><li>算术右移，sra，同上<br><strong>注意，表示是逻辑还是算术的l和a，放在最后</strong></li></ol><p><code>一般性的规律是，助记符，移位只有I型</code></p><h5 id="5-条件分支"><a href="#5-条件分支" class="headerlink" title="5.条件分支"></a>5.条件分支</h5><p>MIPS中涉及的条件分支，常用的是，slt、beq、bne</p><ol><li>slt，有R型、I型，如slt rt,rs,imm<br><strong>注意slt的I型，不使用i扩展！</strong></li><li>beq，I型，如beq rs,rt,L<br><strong>实际编程中L的位置，通常写label，进一步的处理或许是交给汇编器进行的…</strong></li><li>bne，同beq</li></ol><p><code>一般性的规律是，助记符，slt有R型，其它都是I型</code></p><h5 id="6-无条件跳转指令"><a href="#6-无条件跳转指令" class="headerlink" title="6.无条件跳转指令"></a>6.无条件跳转指令</h5><p>MIPS中的跳转指令常用的是j、jr、jal</p><ol><li>j，J型指令，如j L（实际使用L是label）</li><li>jr，J型指令，地址存放在Register中，如j rd</li><li>jal，J型指令，<strong>注意这个指令有两个操作</strong>，一个如普通的j一般跳转到L，另一个是$ra &#x3D; PC + 4（存放返回地址，所以这个指令常用于过程调用）</li></ol><p><code>一般性的规律是，助记符，都是J型</code></p><h4 id="MIPS汇编代码"><a href="#MIPS汇编代码" class="headerlink" title="MIPS汇编代码"></a>MIPS汇编代码</h4><p><code>这一小节主要是掌握MIPS汇编代码的编写的常见结构，包括分支结构、循环结构、还有过程调用</code></p><h5 id="1-分支结构"><a href="#1-分支结构" class="headerlink" title="1.分支结构"></a>1.分支结构</h5><p>分支结构主要有如下两种：</p><ol><li><code>if(i == j) or if(i != j)</code>这种等或不等，主要使用<code>bne</code>,<code>beq</code>进行</li><li><code>if(i &lt; j)</code>这种大于小于的关系，主要使用<code>slt</code>与<code>bne</code>,<code>beq</code>进行</li></ol><p>下面是两个例子<br>eg1</p><pre><code>if(i == j)f = g + helse f = g - h</code></pre><p><code>$s1&lt;-i $s2&lt;-j  $s3&lt;-f  $s4&lt;-g $s5&lt;-h</code></p><pre><code>start: bne $s1,$s2,else       add $s3,$s4,$s5       j exitelse:  sub $s3,$s4,$s5exit:  ...</code></pre><h5 id="2-循环结构"><a href="#2-循环结构" class="headerlink" title="2.循环结构"></a>2.循环结构</h5><p>这里以while循环为例<br>eg</p><pre><code>while(i != k)&#123;    x = x + a[i];    i = i + 1;&#125;</code></pre><p><code>$s1&lt;-x $s2&lt;-i $s3&lt;-k  $s5&lt;-a</code></p><pre><code>loop: beq $2,$3,exit      sll $s7,$s2,2 #注意这行，不能直接将i &lt;&lt;= 2 （Bits -&gt; Byte）      add $s7,$s5,$s7      lw $s6,0($7)      add $s1,$s1,$s6      addi $s2,$2,1 #注意这行，i = i+1（Bits）      j loopexit: ...</code></pre><p>注意，将偏移i换算为地址的时候要乘4，换算成对应按字节编址的情况</p><h5 id="3-过程调用"><a href="#3-过程调用" class="headerlink" title="3.过程调用"></a>3.过程调用</h5><p>首先我们应该清楚整个过程调用的执行过程：</p><ol><li>P保存相应的寄存器（$t）</li><li>P将参数放置于Q可以访问的位置（$a）</li><li>P将返回位置保存，从而让Q可以执行返回（$ra）</li><li>P修改栈帧（$sp $fp）切换到Q的栈帧</li><li>Q将P的相关寄存器进行保存（$s、$ra、$fp）</li><li>Q为自己的局部变量分配栈帧空间</li><li>执行Q的过程</li><li>返回P（使用P最开始保存的$ra，或者是Q自己保存的$ra）</li></ol><p>注意，以上是以最严格、完整的过程来叙述的，实际上都是根据需要来进行。于是可以做以下几点说明：</p><ol><li>P是根据需要保存$t的，如果可以确保之后不再使用，不保存也行，对应了Q可以随意使用$t</li><li>如果参数多于4个，$a不够用了，需要将参数放到相应的栈帧中（如果必要的话$a也可以与$t类似，由P保存）</li><li>$ra的保存实际上是用jal来隐式执行的</li><li>在MIPS中$fp,$sp不一定都要修改，通常是只修改$sp，然后以其作为参考即可，当$fp需要修改的时候，$fp &#x3D; $sp + 栈帧空间大小</li><li>Q也是根据需要保存，如果要使用$s的话必须保存，如果自己还要进行过程调用（会修改$ra、$fp，那么也应该自行保存）</li><li>由于MIPS通用寄存器非常多，$t就多达10个，通常不需要将局部变量分配到栈帧中，直接使用寄存器即可</li><li>…</li><li>返回时总是使用$ra，如果Q中间执行了过程调用修改了$ra，当Q的调用返回时，应该根据Q保存的P的$ra值，将$ra进行还原；并且需要先释放Q的栈帧空间，通常可以使用$sp &#x3D; $fp（如果开始时修改了$fp，同样嵌套调用时若Q修改了$fp，在Q的调用结束时要先将$fp还原，就像$ra一样），或$sp &#x3D; $sp - 栈帧空间，来释放Q的栈帧；最后使用jr $ra返回P的执行。</li></ol><p>以上几点说明都是针对最开始描述的每一点过程进行的</p><p>下面还有一些需要补充的点</p><p>Ⅰ MIPS中栈帧是由高地址到低地址，这意味着分配栈帧空间对$sp执行的是减法操作</p><blockquote><p>eg:在栈帧中分配空间，保存$ra,$a0</p></blockquote><pre><code>subi $sp,$sp,8sw $a0,4($sp)sw $ra,0($sp)</code></pre><p>Ⅱ 一般只有在由数组或结构体等占用空间较大的复杂数据结构的时候才需要使用栈帧分配局部变量（$t不够用）</p><p>Ⅲ Q没有进一步嵌套调用其它函数的情况，Q被称为叶子过程。一般的叶子过程通常在MIPS中甚至不需要开辟栈帧，因为有足够多的通用寄存器</p><p>Ⅳ 如果$fp不使用（建立当前函数的栈帧时并没有维护$fp），可以将$fp作为$s8来使用</p>]]></content>
      
      
      
        <tags>
            
            <tag> 计算机基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Py_learning</title>
      <link href="/2025/05/01/Py-learning/"/>
      <url>/2025/05/01/Py-learning/</url>
      
        <content type="html"><![CDATA[<p><code>由于我在学习机器学习算法的时候，希望通过Python来对相关的算法进行复现。而自己在此之前其实零零散散不成体系地接触过Python语言，也了解一些基本的东西，但是对于Python中一些语言“特性”方面的东西所知甚少，例如变量的作用域与生命周期，不同模块间的访问等等；此外我对Python风格的代码写法也并不熟悉，其实写什么感觉都是C的味道......于是写下这篇blog用来记录，进一步对相关内容的学习</code></p><h2 id="模块化的Python程序"><a href="#模块化的Python程序" class="headerlink" title="模块化的Python程序"></a>模块化的Python程序</h2><h3 id="内置变量-name"><a href="#内置变量-name" class="headerlink" title="内置变量__name__"></a>内置变量__name__</h3><p>__name__是python模块当中的一个内置变量，每个模块都有。如果你选择当前模块开始执行，那么当前模块内置的__name__会被置为__main__；如果一个模块是被令一个模块import进去的，那么这个模块的__name__会被置为__模块名__，但是不会引入后缀。</p><h3 id="模块化"><a href="#模块化" class="headerlink" title="模块化"></a>模块化</h3><p>通过__name__我们就可以将我整个项目文件模块化的组织起来。将一个模块作为程序的执行入口，并始终自我约束地从这个模块开始启动整个项目程序。这样做的关键在于使用如下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>:</span><br><span class="line">    something</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == __main__:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p><strong>关键点即，不要使用判断__name__以外的任何顶层代码</strong></p><h3 id="一些特性"><a href="#一些特性" class="headerlink" title="一些特性"></a>一些特性</h3><p>Python是一种解释性语言，特点就是不需要编译，而是在运行时通过解释器逐行读取、分析和执行源代码。对应的特点之一就是交互式的编程环境（可以在命令行中输入代码，并立刻看到执行的结果）</p><p>我联想到与这种特点相对应的就是——“顶层代码”，即相关的语句不会被封装在任何函数和类当中，点击运行，便会至上而下地逐行开始执行。</p><p><strong>所以一个关键的特性就是，使用import导入模块化后，该模块的顶层代码会立刻执行。</strong></p><p>启示：编写规范化的工程代码时，除了判断程序执行入口，不要使用顶层代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#k_means.py</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;this is k_means&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#main.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;this is mainn&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> __name__ == main:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>输出结果：</p><p><code>this is k_means this is main</code></p><h2 id="变量的作用域和生命周期"><a href="#变量的作用域和生命周期" class="headerlink" title="变量的作用域和生命周期"></a>变量的作用域和生命周期</h2><h3 id="单一模块"><a href="#单一模块" class="headerlink" title="单一模块"></a>单一模块</h3><ol><li><p>全局变量<br>在同一模块当中，定义于模块层的变量（顶层代码部分），对应的是<code>global varible</code>全局变量，这些变量的作用域是全局可见，生命周期是从程序开始执行开始，执行完毕结束。</p></li><li><p>局部变量<br>定义于函数中的变量是<code>local varible</code>局部变量，作用域局部可见。对于嵌套函数，外层变量对内层可见，内层对外层不可见。在Python中这种函数嵌套更加的显然。下面是一个例子：</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">outer_function</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;this is outer&quot;</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inner_function</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;this is inner&quot;</span>)</span><br><span class="line">    inner_function()</span><br></pre></td></tr></table></figure><p>对应变量的生命周期，都是从定义自己的函数开始，到函数执行完毕结束。</p><p><strong>另外值得一提的是，在上面这个例子当中，inner_function不能从顶层代码调用。</strong><br>3. 内置变量<br><code>Built-in varible</code>内置变量的作用域是在任何地方都可以访问，且生命周期贯穿整个程序的运行期，最开始提到的__name__就是一个很好的例子。<br>4. 访问规则<br>python对于变量遵循<code>LEGB</code>的访问规则，即局部、嵌套、全局、内置。当发现了变量，即刻使用。</p><p>最后简单补充以下Python的变量定义规则，变量在“第一次赋值”时被定义。当然这意味着我们要定义一个变量必须考虑一个初始值，如果暂时没有初始值的话可以使用<code>None</code>作为初始值。随后根据需要赋予想要的初始值即可。当然，变量的类型也是根据你赋予的值来确定的。</p><h3 id="多模块"><a href="#多模块" class="headerlink" title="多模块"></a>多模块</h3><p>为了理解多模块情况下相关变量的作用域和生命周期，引入以下概念：</p><ol><li><p>模块对象，在导入模块的时候Python会为模块创建一个对象，这个对象的生命周期由其作用域确定</p></li><li><p>全局导入，模块对象在全局作用域中导入，此时模块变量生命周期同程序一样。作用域同全局变量。</p></li><li><p>局部导入，模块对象在局部作用域中导入，此时模块变量生命周期同导入了它的函数。作用域同相应的局部变量。</p></li><li><p>模块中的顶层代码在被导入时会立刻执行，相应的对应的全局变量会即刻创建，所以对应的全局变量生命周期、作用域，同模块对象。</p></li></ol><p><del>口语化的来说，模块被导入的时候也相当于一个变量（或者是一个类），如果是被主函数所在的模块作为全局变量导入，那么被导入模块的生命周期、作用域同全局变量，如果被作为局部变量导入，也同局部变量。相应的，被导入的时候，被导入模块中的“全局变量”也会即刻被创建，其生命周期同被导入的模块。</del>（毫不精准的表述…）</p><h2 id="名称冲突"><a href="#名称冲突" class="headerlink" title="名称冲突"></a>名称冲突</h2><p>在使用以下代码的时候，名称冲突时常发生。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> somemodule <span class="keyword">import</span> somename</span><br></pre></td></tr></table></figure><p>这类似是跳过了模块对象，直接导入了其中某个全局变量，自然就很可能与当前模块已有的全局变量、函数发生名称冲突。</p><p>常用的解决方法，也是我们使用模块化的常用方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> somemodule</span><br><span class="line">somemodule.somename <span class="comment">#使用模块对象名来访问相应的变量、函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> somemodule <span class="keyword">import</span> somename <span class="keyword">as</span> another_name <span class="comment">#或者是别名</span></span><br></pre></td></tr></table></figure><h2 id="列表生成式"><a href="#列表生成式" class="headerlink" title="列表生成式"></a>列表生成式</h2><p>语法如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">list_name = [formula <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(start, end)]</span><br><span class="line">list_name = [x**<span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>)]</span><br></pre></td></tr></table></figure><p>这种创建列表的方法成为列表生成式，formula是用于生成列表的表达式，可以是返回一些值的函数，后面的循环是列表中生成元素的次数，循环一次便会调用一次formula。</p><p>当然formula也可以直接是数学表达式，例如第二个例子展示的，用于生成1到9的平方的列表。</p><p>注意end不被包含在内。</p><h2 id="元组"><a href="#元组" class="headerlink" title="元组"></a>元组</h2><p>元组（Tuple）是一种内置的数据结构，属于不可变序列类型，用于存储多个元素。与列表（List）不同，<strong>元组一旦创建，其内容就不能更改（即不可变）</strong>。元组常用于存储一组相关的数据，例如函数返回多个值时，可以使用元组来打包这些值。</p><h3 id="元组的创建"><a href="#元组的创建" class="headerlink" title="元组的创建"></a>元组的创建</h3><p>使用<code>()</code>来创建一个元组，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个空元组</span></span><br><span class="line">empty_tuple = ()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个包含多个元素的元组</span></span><br><span class="line">example_tuple = (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="string">&quot;hello&quot;</span>, <span class="number">4.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个单元素的元组（注意逗号）</span></span><br><span class="line">single_element_tuple = (<span class="number">1</span>,)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以省略小括号，直接用逗号分隔元素</span></span><br><span class="line">another_tuple = <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 访问元素</span></span><br><span class="line"><span class="built_in">print</span>(example_tuple[<span class="number">0</span>])  <span class="comment"># 输出 1</span></span><br><span class="line"><span class="built_in">print</span>(example_tuple[<span class="number">3</span>])  <span class="comment"># 输出 &quot;hello&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取元组的长度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(example_tuple))  <span class="comment"># 输出 5</span></span><br></pre></td></tr></table></figure><h3 id="元组的常见用途"><a href="#元组的常见用途" class="headerlink" title="元组的常见用途"></a>元组的常见用途</h3><ol><li>多值返回，用于让函数返回多个值</li><li>作为字典的键，这是由于元组的不可变性</li></ol><h2 id="函数的参数以及返回值"><a href="#函数的参数以及返回值" class="headerlink" title="函数的参数以及返回值"></a>函数的参数以及返回值</h2><p>在python中函数的参数不需要提前声明类型，同样的返回值也不需要提前进行声明。但是在大型的项目中为了便于程序的维护，以及提供静态的检查，可以使用注解符号。例如，下面这个例子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>, <span class="type">Tuple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">k_means</span>(<span class="params">D:<span class="type">List</span>[<span class="type">List</span>[<span class="built_in">float</span>]], n:<span class="built_in">int</span>, k:<span class="built_in">int</span></span>) -&gt; <span class="type">Tuple</span>(<span class="type">List</span>[<span class="type">List</span>[<span class="type">List</span>[<span class="built_in">float</span>]]], Lsit[<span class="type">List</span>[<span class="built_in">float</span>]])</span><br></pre></td></tr></table></figure><p>其中typing是类型注解使用的包，如果不需要使用类型进行注解可以不使用这个包。</p><p>常见的类型注解有：</p><ol><li>List     eg: List[int]</li><li>Tuple    eg: Tuple[float,str]</li><li>Dict     eg: Dict[int,str]</li><li>Set      eg: Set[str]<br>还有许多可用的…用到再查吧…</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clustering-learning-route</title>
      <link href="/2025/04/30/Clustering-learning-route/"/>
      <url>/2025/04/30/Clustering-learning-route/</url>
      
        <content type="html"><![CDATA[<p><code>我从现在开始学习聚类相关的内容，最终目标是希望发表一篇相关的论文。我以现在浅显的眼光给自己定下的学习路线如下</code><br><code>1. 完成西瓜书聚类部分的学习，完成的标志是将书上给出的伪代码进行真实地复现</code><br><code>2. 阅读综述论文，了解聚类对应的科研领域当前大概的情况</code><br><code>3. 阅读聚类有关的顶会论文......</code><br><code>我以现在的知识，无法继续制定下面的计划了，因为我并不了解3、往后的真正开始着手科研工作会是怎样的。我目前粗浅的想法是，或许我会了解到一些聚类的具体应用，然后为了完成一篇相关的论文：我也必须将聚类投入到具体的应用当中去，这个时候我不得不学习一些其它领域的知识（当然，目前我并不清楚那些会是什么）；又或许我会做一些对聚类算法进行改进的工作，但是这或许会更加艰难（因为曾经一位厉害的学长告诉我将A运用于B会比将A升级为A+简单许多）</code><br><code>此外，我将这篇blog用作自己的学习日志与计划路线</code></p><h2 id="阶段一"><a href="#阶段一" class="headerlink" title="阶段一"></a>阶段一</h2><p>2025.4.30</p><ol><li>学习西瓜书上有关聚类的基础知识（概念、性能指标）</li><li>学习“k均值算法”、学习“学习向量量化算法”</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习（聚类） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clustering-watermelon-book</title>
      <link href="/2025/04/30/Clustering-watermelon-book/"/>
      <url>/2025/04/30/Clustering-watermelon-book/</url>
      
        <content type="html"><![CDATA[<h2 id="聚类任务简介"><a href="#聚类任务简介" class="headerlink" title="聚类任务简介"></a>聚类任务简介</h2><p>简单地说，就是要对一个n维向量元素的集合求一个划分，划分后的子集就是一类的（不相交的簇）。</p><p>对于数据集$D &#x3D; {x_1,x_2,…,x_m}$，划分为k个不相交的集合$C_1, C_2, …, C_k$，若$x_i \in C_j$，则$\lambda_i &#x3D; j$，其中$j \in {1,2,…,k}$，对应$\lambda_i$就是$x_i$的标签。聚类任务要做的是就是求出一个聚类结果$\lambda &#x3D; (\lambda_1,\lambda_2,…,\lambda_m)$，其中$\lambda$为数据集的簇标记向量，第$i$个分量标记了$x_i$属于哪一个簇。</p><h2 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h2><p>怎样的聚类是好的：</p><ol><li>簇内的样本尽量相似</li><li>簇间的样本尽量不同</li></ol><h3 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h3><p>外部指标：将聚类结果和某个“参考模型”进行比较，称为外部指标</p><p>对于数据集$D &#x3D; {x_1,x_2,…,x_m}$，使用聚类模型A，得到簇标记向量$\lambda$，另外使用参考聚类模型B，得到簇标记向量$\lambda^{*}$。</p><p>于是我们可以根据$\lambda_i$与$\lambda_j$相同与否的关系以及$\lambda^{<em>}_i$<br>与$\lambda^{</em>}_j$是否相同的关系定义如下集合。</p><p>$DD,DS,SD,SS$一共四个集合，这些集合中的元素类似$(x_i,x_j)$，是一个“向量对”，分别按照如下规则界定类似的向量对是否属于相应的集合</p><ol><li>$x_i$与$x_j$在模型A、B的划分下都属于同一簇，则$(x_i,x_j) \in SS$</li><li>$x_i$与$x_j$在模型A、B的划分下都不属于同一簇，则$(x_i,x_j) \in DD$</li><li>$x_i$与$x_j$在模型A划分下属于同一簇，在B划分下不属于同一簇，则$(x_i,x_j) \in SD$</li><li>$x_i$与$x_j$在模型A划分下不属于同一簇，在B划分下属于同一簇，则$(x_i,x_j) \in DS$</li></ol><p><code>D即different，S即same 这样就非常容易理解了</code></p><p>根据上面的集合，我们可以定义如下过度变量</p><ol><li>$\lvert SS \rvert &#x3D; a$</li><li>$\lvert SD \rvert &#x3D; b$</li><li>$\lvert DS \rvert &#x3D; c$</li><li>$\lvert DD \rvert &#x3D; d$</li></ol><p>进一步，我们定义常用于性能度量的第一组系数</p><ol><li><strong>JC系数</strong> $JC &#x3D; \frac{a}{a+b+c}$</li><li><strong>FMI系数</strong> $FMI &#x3D; \sqrt{\frac{a}{a+b} \ast \frac{a}{a+c}}$</li><li><strong>Rand指数</strong> $RI &#x3D; \frac{2(a+b)}{m(m-1)}$</li></ol><p>这些性能指标的范围都是$[0,1]$，并且越大说明聚类效果越好<br><code>当然，前提是参考的模型是“正确”的</code></p><h3 id="距离计算"><a href="#距离计算" class="headerlink" title="距离计算"></a>距离计算</h3><h4 id="闵可夫斯基距离"><a href="#闵可夫斯基距离" class="headerlink" title="闵可夫斯基距离"></a>闵可夫斯基距离</h4><p>定义函数$dist(\cdot,\cdot)$，用于计算两个向量的距离。则它应该满足下述三个性质</p><ol><li>非负性</li><li>对称性</li><li>直递性</li></ol><p>常用的距离是闵可夫斯基距离</p><p>$dist_mk(x_i,x_j) &#x3D; (\sum_{\mu &#x3D; 1}^{n} \lvert x_{i\mu} - x_{j\mu} \rvert ^{p})^{\frac{1}{p}}$<br>显然当$p &#x3D; 2$时即我们常用的欧氏距离，$p &#x3D; 1$时为曼哈顿距离</p><h4 id="有序属性和无序属性"><a href="#有序属性和无序属性" class="headerlink" title="有序属性和无序属性"></a>有序属性和无序属性</h4><p>在考虑属性之间的距离的时候，序十分重要。这里通过简单的例子引入有序和无序。属性值出自于能够直接计算距离的属性称为有序属性，例如属性定义域为${1,2,3}$，而不能的就是无序属性，例如${货车,西瓜,乐乐}$。</p><p>显然，闵可夫斯基距离是用于衡量有序属性的距离的。</p><h4 id="VDM——衡量无序属性的距离"><a href="#VDM——衡量无序属性的距离" class="headerlink" title="VDM——衡量无序属性的距离"></a>VDM——衡量无序属性的距离</h4><p>假设有$k$个样本簇，$m_\mu,a$表示在属性$\mu$上取值为$a$的样本的个数，$m_\mu,a,i$表示在第i个样本簇中，属性$\mu$取值为$a$的样本个数。定义VDM如下。</p><p>$VDM &#x3D; \sum_{i&#x3D;1}^{k} \lvert \frac{m_\mu,a,i}{m_\mu,a} - \frac{m_\mu,b,i}{m_\mu,b}\rvert ^{p}$</p><p>值得注意的是，这里衡量的只是无序属性的距离，而要衡量两个无序样本$x_i$与$x_j$的距离，即其中的各个属性（类比向量的分量）都是无序属性，我们应该对各个属性的$VDM$求和。</p><h4 id="混合元素的距离"><a href="#混合元素的距离" class="headerlink" title="混合元素的距离"></a>混合元素的距离</h4><p>不失一般性，我们可以定义混合元素的距离如下：<br>$MinkovDM_p(x_i,x_j) &#x3D; (\sum_{\mu&#x3D;1}^{n_c} \lvert x_{i\mu} - x_{j\mu} \rvert ^{p} + \sum_{\mu&#x3D;n_c+1}^{n} VDM_p(x_{i,\mu},x_{j,\mu}))^{\frac{1}{p}}$</p><p>其中$x_i,x_j$为混合属性的元素，$1到n_c$对应为有序属性，$n_c到n$对应为无序属性</p><h3 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h3><p>于是我们可以根据元素的不同（有序、无序、混合），选取我们需要的距离函数$dist(\cdot,\cdot)$，定义如下常用于刻画簇的性质的量</p><ol><li>$\mu_i &#x3D; \frac{1}{\lvert C \rvert} \sum_{1 \le i \le \lvert C \rvert} x_i, \mu &#x3D; (\mu_1,\mu_2,…,\mu_m)$为簇$C$的中心点</li><li>$avg(C) &#x3D; \frac{2}{\lvert C \rvert (\lvert C \rvert - 1)} \sum_{1 \le i &lt; j \le \lvert C \rvert} dist(x_i,x_j)$ 簇$C$内样本间的平均距离</li><li>$diam(C) &#x3D; max_{1 \le i &lt; j \le \lvert C \rvert} dist(x_i,x_j)$ 簇$C$内样本间的最远距离</li><li>$d_{min}(C_i,C_j) &#x3D; min_{x_i \in C_i,x_j \in C_j} dist(x_i,x_j)$ 簇$C_i$和簇$C_j$中最近样本的距离</li><li>$d_{cen}(C_i,C_j) &#x3D; dist(\mu_i,\mu_j)$ 簇$C_i$和簇$C_j$的中心点距离</li></ol><p>进一步我们定义一些内部指标如下。</p><ol><li>$DBI &#x3D; \frac{1}{k} \sum_{i&#x3D;1}^{k} max_{j \ne i}(\frac{avg(C_i)+avg(C_j)}{d_{cen}(\mu_i,\mu_j)})$</li><li>$DI &#x3D; min_{1 \le i \le k} { min_{j \ne i}(\frac{d_{min}(C_i,C_j)}{min_{1 \le l \le k} diam(C_i)}) }$</li></ol><p>DB指数越小越好，Dunn指数越大越好</p><h2 id="原型聚类"><a href="#原型聚类" class="headerlink" title="原型聚类"></a>原型聚类</h2><p>原型的概念对应的是空间中的点。原型聚类的前提是认为，数据集中的聚类结构可以通过一组原型来描述。而原型聚类要做的就是通过某些方法找出这组“原型”。常见的原型聚类算法的代表有<code>k-means（k均值算法）</code>、<code>学习向量量化算法</code>等等</p><p>在后续的blog中会记录我复现相关算法的过程</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习（聚类） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Traceroute</title>
      <link href="/2025/04/27/Traceroute/"/>
      <url>/2025/04/27/Traceroute/</url>
      
        <content type="html"><![CDATA[<p>这篇blog用于记录我在学习计算机工程系统导论时，一个与网络有关的实验。</p><h2 id="实验目的"><a href="#实验目的" class="headerlink" title="实验目的"></a>实验目的</h2><ol><li>学习Ping和Traceroute工具的使用，能够在网络开发和维护中熟练使用该工具获取网络状态和进行错误排查。</li><li>巩固课堂学习的因特网分层设计实现知识，理解因特网的分组转发网络的特性，通过分析互联网数据包因转发而产生的时延，加深理解尽力而为的因特网设计思想。</li><li>通过分析ICMP协议，学习在分层设计中进行跨层通信的设计实现方法，思考因特网网络层在错误处理上的折中设计和处理技巧。</li><li>通过Traceroute工具基于错误处理包进行路径探测功能的设计，体会在实践中体会设计方案在解决问题的同时是如何带来新的设计机会的。</li></ol><h2 id="实验过程与习题"><a href="#实验过程与习题" class="headerlink" title="实验过程与习题"></a>实验过程与习题</h2><h3 id="ping应用"><a href="#ping应用" class="headerlink" title="ping应用"></a>ping应用</h3><ol><li><p><strong>用不超过200字简要概括ping命令</strong><br>ping命令用于向主机或网关发送请求，使用的是ICMP协议的ECHO_REQUSET数据包，并且会接收ICMP ECHO_RESPONSE作为回应。其中ECHO_REQUSET数据包包含了IP和ICMP头部，其后是一个timeval结构以及若干的填充字节。ping命令支持IPv6以及IPv4协议，其中对IPv6节点的信息查询是根据RFC4620进行的，但是由于IPv6源路由被起用，中间跳点可能不被允许。<br><em><code>man ping</code>结果如图1所示</em></p></li><li><p><strong>说明实验现象背后的原因</strong><br>我使用ping命令，分别对<code>www.sud.edu.cn</code>和<code>www.ouc.edu.cn</code>进行了10次<code>ping</code>操作，每次都发送了56字节的数据包。从结果可以看出，到<code>www.sud.edu.cn</code>的网络连接是通畅的，每一次都成功受到了回复，没有丢包发生。但是延迟时间在10.9ms-170ms之间不等，这可能是由于网络拥塞或者其它网络干扰因素导致的。但是发送到<code>www.ouc.edu.cn</code>始终没有得到回复，可能是因为OUC的网络配置了防火墙或者其它网络安全策略，阻止了<code>ping</code>请求。<br><em>实验现象如图2所示</em></p></li><li><p><strong>通过查询资料，画出所使用ICMP数据包的结构</strong><br>其中相关部分说明如下<br>(1)Type，8bits，用于指定ICMP消息类型，例如0表示<code>Echo Reply</code>，8表示<code>Echo Requset</code><br>(2)Code，8bits，用于对消息类型进行更详细的说明，例如目的地不可达，网络不可达等等<br>(3)Checksum，16bits，用于错误检查<br>(4)Identifier，16bits，用来匹配请求和响应<br>(5)Data，可变长度，包含可选的附加数据。<br><em>ICMP数据包如图3所示</em></p></li></ol><h3 id="traceroute应用"><a href="#traceroute应用" class="headerlink" title="traceroute应用"></a>traceroute应用</h3><ol><li><p><strong>不超过200字简要概括traceroute命令</strong><br>traceroute可以用于追踪数据包从源主机到目标主机的网络路径中，它通过发送一个含有TTL字段的IP数据包进行工作，数据包每传递到一个路由器TTL就会减1，当TTL为0的时候路由器就会丢弃该数据包，并给源主机发送一个ICMP（超时响应），其中包含了当前路由器的地址。traceroute在整个工作流程中会从TTL为1开始发送数据包，每次收到ICMP响应后就增大TTL值（2、3、4…），直到收到目的主机的响应。于是根据每一次收到的ICMP响应，traceroute就可以知晓从源主机到目标主机经过的每个路由器，并且显示相应的IP地址和响应时间。<br><em><code>man traceroute</code>结果如图4所示</em></p></li><li><p><strong>确定并说明从源计算机到<code>www.baidu.com</code>的路径</strong><br>源计算机到<code>www.baidu.com</code>实现路径探测，结果如下图如下图，一共经过了30跳，其中第一跳<code>172.19.0.1</code>是源计算机ip，第三、四、五跳<code>10.149.32.1</code>和<code>10.70.7.2</code>以及<code>10.90.0.4</code>是内部网络路由器，之后经过了若干公网路由器，最终到达目标服务器。显示为<code>***</code>的，可能是由于网络设备被配置为了不响应ICMP请求，所以路径不能完全确定。<br><em><code>traceroute www.baidu.com</code>结果如图5所示</em></p></li><li><p><strong>说明输出结果每个字段的意义</strong><br>(1)<code>traceroute to www.baidu.com (182.61.200.108)</code>为traceroute命令输出说明，目标的IP地址为182.61.200.108<br>(2)<code>30 hops max</code>指最多经过30跳<br>(3)<code>60 byte packets</code>指每个数据包大小为60字节<br>往下是每一跳的详细信息，以第一跳为例<br>(4)<code>1 1(172.19.0.1) 0.647ms 1.041ms 1.285ms</code><br>第一个数字1代表第一跳，172.19.0.1代表第一跳的设备IP，后续三个时间是三个ICMP包的往返时间；后续存在<code>***</code>的输出对应没有收到该跳的ICMP回复（或者是网络拥塞导致的丢包）。</p></li></ol><h3 id="traceroute探索"><a href="#traceroute探索" class="headerlink" title="traceroute探索"></a>traceroute探索</h3><p><code>注：这部分实验，我先是在自己本地的电脑上进行的实验，所以后续的描述是根据本地的输出来的，实验截图也是使用的本地截图</code></p><ol><li><strong>解释traceroute 18.31.0.200的输出</strong><br>通过这条指令，我们使用traceroute追踪了一条从本地网络，通过ISP、NAT字段、跨越国际骨干网再进入教育和研究网络的传输路径，最终到达了目标IP（18.31.0.200）</li></ol><p>以下按照每一跳对路径进行分析<br>(1)（1-3跳）<br>hop1是本地网络，hop2、hop3是私有IP（10.x.x.x），可能对应内部网络路由器、防火墙等<br>(2)（4跳）<br>显示为<code>***</code>，路由器没有响应ICMP请求（或者是丢包）<br>(3)（5-12跳）<br>hop5-hop12对应的是公网地址，数据包是在ISP网络中传输的（211.64.x.x和101.4.x.x地址段对应ISP或骨干网提供商，100.64.x.x属于共享地址空间，通常用于ISP内部的网络地址转换）<br>(4)（13-30跳）<br>hop13对应<code>***</code>同样可能没有响应ICMP请求或丢包；hop14开始数据包进入国际网络，延迟有显著的增加（20ms左右，到后续数百毫秒）；hop19开始数据包进入了亚洲太平洋先进网络（APAN网络），之后进入了教育和研究网络（Internet2），可以看到最后已经显示出了较高的延迟（200~400ms左右）<br><em><code>traceroute 18.31.0.200的输出</code>结果如图6所示</em><br>2. <strong>说明从源地址到<code>www.baidu.com</code>和到<code>cn.bing.com</code>的网络路径差异</strong><br>两者的网络路径在初始几跳上有共同点（除去<code>***</code>，至少前10跳是相同的），在进入外部网络之后，路径显然会分道扬镳（除去<code>***</code>，从第14跳开始不相同）；到<code>cn.bing.com</code>后续经过了一些国际骨干网，（例如202.97.x.x的IP地址属于中国电信骨干网），而到<code>www.baidu.com</code>主要是在国内网络中传输的。<br><em><code>traceroute www.baidu.com</code>与<code>traceroute cn.bing.com</code>的结果如图7、8所示</em><br>3. <strong>如果IPv6上实现路径探测，应该使用包头的哪个字段</strong><br>在IPv6网络中进行路径探测，主要依赖IPv6包头中的Hop limit字段，这个字段是“跳限字段”，其作用和IPv4中的TTL（生存周期）字段相同，用来限制数据包在网络传输的跳数，每经过一个路由器，这个字段的值都会减1，当减为0后数据包就会被丢弃，并且通常会返回一个ICMPv6的超时消息给源地址。</p><h2 id="遇到的问题及解决方法"><a href="#遇到的问题及解决方法" class="headerlink" title="遇到的问题及解决方法"></a>遇到的问题及解决方法</h2><p>在实验过程中我主要遇到了问题是在<code>ping www.ouc.edu.cn</code>始终无法收到返回的结果，掉包率总是100%。经过查阅相关资料，我推测应该是我们学校的服务器配置了防火墙或者其它安全策略，拒绝向ping发送的数据包进行响应。并且我使用<code>traceroute www.ouc.edu.cn</code>尝试追踪，我发现结果从第7跳开始往后始终都只返回<code>***</code>，这间接作证了我的猜测。（因为它们都使用的ICMP数据包）<br><em>结果如图9所示</em></p><h2 id="课后实验与思考"><a href="#课后实验与思考" class="headerlink" title="课后实验与思考"></a>课后实验与思考</h2><h3 id="Traceroute在网络故障排查中有哪些应用场景"><a href="#Traceroute在网络故障排查中有哪些应用场景" class="headerlink" title="Traceroute在网络故障排查中有哪些应用场景"></a>Traceroute在网络故障排查中有哪些应用场景</h3><p>Traceroute的主要应用场景有路径发现（它可以用于追踪整个网络路径）、网络延迟分析（可以识别在哪一条引入了延迟，可以判断哪一条延迟较大）、故障点识别（确定数据包在哪一条丢失或无法到达，例如<code>***</code>的出现可能就是这个原因）、路由问题诊断（找到可能存在的路由环路）、网络性能评估（综合以上功能可以进行网络性能评估）等</p><h3 id="Traceroute中如何通过修改参数来优化网络诊断效果"><a href="#Traceroute中如何通过修改参数来优化网络诊断效果" class="headerlink" title="Traceroute中如何通过修改参数来优化网络诊断效果"></a>Traceroute中如何通过修改参数来优化网络诊断效果</h3><p>(1)修改初始TTL值<br><code>traceroute -f 5 example.com</code> 使用参数f可以完成这种操作，用来跳过已知的安全路径<br>(2)设置最大TTL值<br><code>traceorute -m 20 example.com</code> 使用参数m可以完成这种操作，用于避免弹错过长的路径<br>(3)修改探测包的数量<br><code>traceroute -q 5 example.com</code> 使用参数q可以完成这种操作，用来减少或增加探测粒度</p><p>其它的还有使用不同的协议、修改探测包的端口号等等</p><h3 id="Traceroute交换源节点和目标结点，在同一时刻探测到两条路径是相同的吗"><a href="#Traceroute交换源节点和目标结点，在同一时刻探测到两条路径是相同的吗" class="headerlink" title="Traceroute交换源节点和目标结点，在同一时刻探测到两条路径是相同的吗"></a>Traceroute交换源节点和目标结点，在同一时刻探测到两条路径是相同的吗</h3><p>由于网络路由的动态性、负载均衡、多路径路由等因素，网络路径一般是非堆成的，即A到B的路径和B返回A的路径可能会经过不同的路由器以及有不同的跳数。所以一般情况下上述两条路径不会相同</p><h2 id="实验总结"><a href="#实验总结" class="headerlink" title="实验总结"></a>实验总结</h2><p>通过本次实验我掌握了<code>ping</code>和<code>traceroute</code>命令的基础使用方法，例如<code>traceroute</code>的一些常见参数的使用（f、m、q、l等）。并且能够分析两条命令的输出信息，例如ping命令输出的掉包率、时延等等，通过这个过程，我更深刻地体会了网络的结构，以及数据包在网络上传递的整个过程。此外我还了解了两条命令的基本实现原理，例如traceroute就是不断通过增大ICMP数据包的TTL，使其能够传播得更远，并每一次都记录返回的响应，最终形成完整的路径，作为信息进行输出。最后，我掌握了ICMP数据包的结构，对数据包有了更加清晰的认识。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 计算机基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构与算法——树</title>
      <link href="/2025/02/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E6%A0%91/"/>
      <url>/2025/02/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<h2 id="如何输入一棵树"><a href="#如何输入一棵树" class="headerlink" title="如何输入一棵树"></a>如何输入一棵树</h2><p>在这里记录一些算法题中常见的树的输入方法</p><p>*输入格式一<br>第一行输入一个正整数n，代表树中结点的数量。<br>第二行输入n个正整数w1…wn，代表i点的权重<br>接下来n行，每行输入两个整数，其中第i行的li、ri表示i点的左儿子与右儿子，若为-1则表示不存在。</p><h2 id="如何存储一颗树"><a href="#如何存储一颗树" class="headerlink" title="如何存储一颗树"></a>如何存储一颗树</h2><p>在这里记录常见的树的存储方法</p><p>*存储方式一<br>我们可以使用一个树结点的结构体，通过将这些结构体使用指针连接起来，从而构建一棵完整的树。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">tree_node</span>&#123;</span><br><span class="line">    <span class="type">int</span> value;</span><br><span class="line">    tree_node * ls;</span><br><span class="line">    tree_node * rs;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 数据结构与算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>unity锚点</title>
      <link href="/2025/02/03/unity%E9%94%9A%E7%82%B9/"/>
      <url>/2025/02/03/unity%E9%94%9A%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<h2 id="如下"><a href="#如下" class="headerlink" title="如下"></a>如下</h2><p>参考文章<a href="https://blog.csdn.net/Terie/article/details/111433486">https://blog.csdn.net/Terie/article/details/111433486</a></p><p>透过这篇博客，我大概了解到了两个点：<br>pviot：轴心，采用相对坐标系，(0,0)-&gt;(1,1)，位于某ui的方框内，其表征着在ui被自适应缩放的时候，以哪个位置为标准缩放</p><p>锚点：unity提供了三种模式<br>1、四个锚点合在一起的模式<br>该模式下，在屏幕分辨率改变的过程当中，ui相对于锚点的位置不改变，ui的宽、高也不变</p><p>2、四个锚点两两在一起的模式<br>该模式下，ui的对应对于锚点连成的线的距离不改变，宽高比不改变，随之缩放</p><p>3、四个锚点分开的模式<br>该模式下，ui的四个边对应锚点连线矩形的四边距离不改变，宽高随着四向距离的改变，自适应。</p><p>一般情况下，设置Canvas的UI为随屏幕缩放的模式，理解pivot，记住不同位置的ui常用的锚点模式和放置位置就好。</p><p>一下是gpt的建议：<br>a. 确定元素的锚点<br>对于屏幕边缘的UI元素（如按钮、面板），将其Anchors设置在父容器的相应边缘。例如：<br>左上角的按钮：Anchors设为（0，1）到（0，1）<br>右上角的按钮：Anchors设为（1，1）到（1，1）<br>底部中央的按钮：Anchors设为（0.5，0）到（0.5，0）<br>b. 中央对齐的元素<br>对于中心对齐的UI元素（如中心的面板），将其Anchors设置在父容器的中心。例如：<br>中央面板：Anchors设为（0.5，0.5）到（0.5，0.5）<br>c. 保持宽高比例<br>在Inspector中勾选Canvas Scaler组件的“UI Scale Mode”选项，设置为“Scale With Screen Size”，并设置参考分辨率为你设计时的分辨率（如1920x1080）。这样Unity会根据屏幕分辨率自动缩放UI元素。</p>]]></content>
      
      
      
        <tags>
            
            <tag> unity </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读：基于复杂港口环境下的无人船自主靠泊最优控制方案研究</title>
      <link href="/2024/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E5%9F%BA%E4%BA%8E%E5%A4%8D%E6%9D%82%E6%B8%AF%E5%8F%A3%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84%E6%97%A0%E4%BA%BA%E8%88%B9%E8%87%AA%E4%B8%BB%E9%9D%A0%E6%B3%8A%E6%9C%80%E4%BC%98%E6%8E%A7%E5%88%B6%E6%96%B9%E6%A1%88%E7%A0%94%E7%A9%B6/"/>
      <url>/2024/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E5%9F%BA%E4%BA%8E%E5%A4%8D%E6%9D%82%E6%B8%AF%E5%8F%A3%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84%E6%97%A0%E4%BA%BA%E8%88%B9%E8%87%AA%E4%B8%BB%E9%9D%A0%E6%B3%8A%E6%9C%80%E4%BC%98%E6%8E%A7%E5%88%B6%E6%96%B9%E6%A1%88%E7%A0%94%E7%A9%B6/</url>
      
        <content type="html"><![CDATA[<h2 id="论文阅读过程当中新了解的知识"><a href="#论文阅读过程当中新了解的知识" class="headerlink" title="论文阅读过程当中新了解的知识"></a>论文阅读过程当中新了解的知识</h2><p>1、无人船MMG模型：<br>一种用于模拟和分析船舶在水中运动行为的数学模型。</p><p>MMG模型的基本思想是将船舶的运动分解为船体本身、推进系统（如螺旋桨）和舵等操纵装置的独立贡献，然后将这些贡献综合起来，以描述船舶的整体运动。</p><p>主要包括四个部分：<br>船体动力学模型：描述船体在水中的基本运动特性，包括水动力、惯性力和阻力等。<br>螺旋桨模型：描述推进装置（如螺旋桨）对船舶的推力和转矩输出，这些输出通常与螺旋桨的转速和船速等参数相关。<br>舵模型：描述舵对船舶的横向力和转矩输出，这些输出通常与舵角和水流速度等参数有关。<br>干扰力模型：包括环境中的风、浪、流等对船舶运动的影响。</p><p>2、横荡、纵荡和艏摇：<br>横荡（Sway）：<br>横荡指船舶沿着横向（即垂直于船体中心线）的运动。对于一艘在水中的船舶来说，横荡运动通常是侧向的滑动，可能由于外部作用力（如侧风、侧流或舵面调整）而发生。横荡是三自由度（3-DOF）船舶动力学模型中的一个重要运动分量。<br>纵荡是指船舶沿着船体中心线方向的运动，即船舶的前进或后退运动。纵荡通常由船舶的推进装置（如螺旋桨）提供的推力驱动。纵荡运动决定了船舶的航速变化，在无人船中，通过调整纵荡速度可以实现对速度的控制和调整。<br>艏摇是指船舶围绕垂直轴的旋转运动，即船首（艏）左右摇摆的运动。艏摇角度的变化决定了船舶的航向。艏摇运动通常是由舵角调整或推进系统产生的转矩导致的，它是船舶操纵中控制航向的重要参数。无人船的自动导航系统会通过控制艏摇运动来实现精确的航向调整和路径跟踪。<br>3、航向角 艏摇角 舵角 漂流角 绝对风舷角和相对风舷角</p><p>4、无量纲化处理 运动参数和无量纲参数的对应关系<br>无量纲化处理的核心思想是通过适当选择基准量（如长度、时间、质量等），将物理量转换为无量纲量。这样做的目的是减少方程中的变量数量，揭示系统的内在规律，并使得不同尺度的系统可以进行比较和分析。</p><p>选择一组基准量，例如特征长度。无量纲变量的定义：将原始变量除以相应的基准量。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 大学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>srdp</title>
      <link href="/2024/10/17/srdp/"/>
      <url>/2024/10/17/srdp/</url>
      
        <content type="html"><![CDATA[<h2 id="关于srdp"><a href="#关于srdp" class="headerlink" title="关于srdp"></a>关于srdp</h2><ol><li>srpd在别校算是校级大创，国级与省级大创会在推免阶段很有优势</li><li>文章只有认可度较高的才会有影响因子，EI和会议期刊这些认可度不高，不过这些对本科生而言还是比较有难度，SCI中有影响因子</li><li>出国、企业就业一般看JCR</li></ol><h2 id="相关方向"><a href="#相关方向" class="headerlink" title="相关方向"></a>相关方向</h2><p>海洋技术；<br>声学：声信号数据处理、算法<br>光学：仿真、数据处理、算法<br>遥感：遥感信息数据处理、算法</p><p>光科：<br>光谱、光电结合（单片机）、光学设计等</p><p>电子信息：<br>单片机、算法等</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p>Web of Science、中国知网、ScienceDirect、小绿鲸、截屏翻译APP</p><p>刚入门的直接看SCI会比较困难，最好从硕博文章开始看，在看SCI之前最好先看三篇博士文章（600多页）</p><p>看英文文章可以用小绿鲸，不用应用软件，用官网主页。截屏翻译、小绿鲸是看SCI时用到的，前期看博士文章最好。</p><p>GPT可以用，但是写出来的东西一定要自己看得懂。</p><p>zlibrary中有很多参考书，很好用。多运用CSDN、GitHub</p><h2 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h2><p>国创、省创注意截至日期，中期的时候会有一个机会用来申请国创、省创，最好要把srdp升一个台阶，不要原原本本地交srdp。</p><p>理工科相关的公式要熟悉（明白如何推导的，或者说如何产生的）</p><p>流程：<br>10、11月立项（撰写立项书、制作ppt、填写系统、准备立项答辩）-&gt;4、5月中期检查（省创国创立项）-&gt;10、11月结项（省创国创中期检查）-&gt;4、5月省创国创结项</p><p>如果目标国创、省创的话最好前期干完百分之60到百分之80，除此之外一定要干满工作量，否则会延期，得不偿失。</p><p>选题的话主要看指导老师。</p><p>制定开发计划（敏捷开发计划）、迭代开发（要先有一个成果，逐步迭代，迎接变化）、定期开会（要和老师同步）、共同开发</p><h2 id="相关语言"><a href="#相关语言" class="headerlink" title="相关语言"></a>相关语言</h2><p>Origin（画图好用）、Latex、MathType（打字母公式）、Office…</p><h2 id="选题"><a href="#选题" class="headerlink" title="选题"></a>选题</h2><p>小发明、小制作、小设计；实际教学中的综合性、设计性、创新性、应用性课题</p>]]></content>
      
      
      
        <tags>
            
            <tag> 大学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>按位运算相关内容</title>
      <link href="/2024/10/16/%E6%8C%89%E4%BD%8D%E8%BF%90%E7%AE%97%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9/"/>
      <url>/2024/10/16/%E6%8C%89%E4%BD%8D%E8%BF%90%E7%AE%97%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9/</url>
      
        <content type="html"><![CDATA[<h2 id="logs"><a href="#logs" class="headerlink" title="logs"></a>logs</h2><p>目前的打算是先更新思路，然后在有图片处空出，后续补充图片。</p><h2 id="使用算数右移实现逻辑右移"><a href="#使用算数右移实现逻辑右移" class="headerlink" title="使用算数右移实现逻辑右移"></a>使用算数右移实现逻辑右移</h2><p>以int 32位为例</p><p>我们清楚，当进行算数右移的时候，对于符号位为1的情况下，右移之后符号位、第32位的1，被移动到31位后，32位右被补上了1</p><p>而逻辑右移与之不同的是，逻辑右移后，最高位、32位会被补0</p><p>现在要用算数右移实现逻辑右移，看代码：</p><pre><code>x = x &gt;&gt; n;y = ~((1 &lt;&lt; 31) &gt;&gt; n &lt;&lt; 1);  x = x&amp;y;</code></pre><p>1、x符号位为0，算数右移动n位后，x为 0..（n+1个0）XXXXXXX…<br>2、x符号位为1，算数右移动n位后，x为 1..（n+1个1）XXXXXXX…<br>而对于逻辑右移n位后，无论x符号位是0还是1，都是算数右移中1、的情况</p><blockquote><p>所以我们只需要：x &#x3D; x&amp;0..（n+1个0）1111..1就可以将算数右移的结果转换为逻辑右移的结果。<br>关键在于凑出掩码0..（n+1个0）1111..1，即y处的操作。</p></blockquote><h2 id="分治法求二进制数中1的个数"><a href="#分治法求二进制数中1的个数" class="headerlink" title="分治法求二进制数中1的个数"></a>分治法求二进制数中1的个数</h2><p>分治法的思想：一个复杂的问题分解成若干个规模较小但相似的子问题，“递归”地解决这些子问题，然后将这些子问题的解组合起来，得到原问题的解。</p><p>让我们先以8位二进制数为例子：</p><blockquote><p>原问题：11011110 整体这个数（或者说“一”部分）有几个1<br>分解：求 1101 1110 两部分，分别有几个1<br>分解：求 11 01 11 10 四部分，分别有几个1<br>分解：求 1 1 0 1 1 1 1 0 八部分，分别有几个1</p></blockquote><p>显然，对于上一个问题，各个部分的数字0 or 1就代表了这个部分有几个1，即八个部分时各个部分有几个1已经清楚。最小问题的答案已知，考虑如何利用将最小问题的答案合并，求解上层的问题。</p><p>注意：“各个部分的数字0 or 1就代表了这个部分有几个1”正是八部分时最大的特点，记住这个特点，因为我们的目标是“让‘一’部分时，该部分的数字就代表该部分有几个1。”</p><p>下面开始合并，使用到的关系是：上一个问题中各个部分有几个1 &#x3D; 当前问题中相邻两个部分1的个数相加</p><p>采用上面的关系对问题合并，我们会发现每解决一个问题后，问题中各个部分的数字，就代表原来该部分有几个1。</p><blockquote><p>合并： 四部分状况的问题的答案：10 01 10 01<br>合并： 二部分状况的问题的答案：0011 0011<br>合并： “一”部分（原问题）的答案：00000110</p></blockquote><p>根据前面的描述，00000110即原本1的个数。</p><p>下面是具体的实现代码</p><pre><code>cin&gt;&gt;xint x1,x2,x3,x4,x5,s,m;s = x &gt;&gt; 1;m = ~((1 &lt;&lt; 31));x1 = s&amp;m;x = (x &amp; 0x55555555) + (x1 &amp; 0x55555555);s = x &gt;&gt; 2;m = ~((1 &lt;&lt; 31) &gt;&gt; 1);x2 = s&amp;m;x = (x &amp; 0x33333333) + (x2 &amp; 0x33333333);s = x &gt;&gt; 4;m = ~((1 &lt;&lt; 31) &gt;&gt; 3);x3 = s&amp;m;x = (x &amp; 0x0F0F0F0F) + (x3 &amp; 0x0F0F0F0F);s = x &gt;&gt; 8;m = ~((1 &lt;&lt; 31) &gt;&gt; 7);x4 = s&amp;m;x = (x &amp; 0x00FF00FF) + (x4 &amp; 0x00FF00FF);s = x &gt;&gt; 16;m = ~((1 &lt;&lt; 31) &gt;&gt; 15);x5 = s&amp;m;x = (x &amp; 0x0000FFFF) + (x5 &amp; 0x0000FFFF);return x;</code></pre><p>关键在于理解如何实现“上一个问题中各个部分有几个1 &#x3D; 当前问题中相邻两个部分1的个数相加”之中，相邻两部分相加。<br>不难想到，我们可以使用掩码。<br>例如：</p><blockquote><p>01010101和10101010（相邻的两位相加，结果为四部分状况答案）<br>x &#x3D; x&amp;0b01010101 + x&amp;0b10101010<br>00110011和11001100（相邻的四位相加，结果为两部分状况答案）<br>x &#x3D; x&amp;0b00110011 + x&amp;0b11001100<br>00001111和11110000（相邻的四位相加，结果为“一”部分状况、最终答案）<br>x &#x3D; x&amp;0b00001111 + x&amp;0b11110000</p></blockquote><p>当然这只是对最开始用例的解释，对于具体的计算机当中的int类型，32位可以如此类比。<br>最终我们需要合并使用当前问题的答案求解上一个问题5次（32 &#x3D; 2^5），对应代码中5处使用了掩码的位置。<br>值得一提的是，也可以在一次计算当中不更换掩码，但是要将x左或右移（逻辑右移！）对应的位数，就像我在代码中的那样。</p><h2 id="按位运算实现对数值变量实现逻辑Not"><a href="#按位运算实现对数值变量实现逻辑Not" class="headerlink" title="按位运算实现对数值变量实现逻辑Not"></a>按位运算实现对数值变量实现逻辑Not</h2><p>具体要求：对于数值型数据x，若x &#x3D;&#x3D; 0x00000000，则输出 0b00000000000000000000000000000001，否则输出0b00000000000000000000000000000000<br>注：上面的常数均为补码值，用于做相等的比较的时候也是用的x的补码值，而非真值。</p><p>思路：对于非0数，其相反数的符号位一定与原数的符号位不同。考虑原数与相反数相或后考虑符号位的情况，来判断x本身是否为0。</p><h2 id="7-fitsBits-判断x可否使用n位补码表示"><a href="#7-fitsBits-判断x可否使用n位补码表示" class="headerlink" title="7 fitsBits 判断x可否使用n位补码表示"></a>7 fitsBits 判断x可否使用n位补码表示</h2><p>思路：<br>1、我们知道n位补码的表示范围是-2^n~2^n-1，因此我们需要判断x是否在这个范围之内。<br>2、在当前环境下，x是由32位二进制补码储存在计算机当中的，我们不难发现：如果x只需要n位补码，那么在计算机当中前面的32-n位补码是空闲的，所以我们可以将x在计算机当中的补码，先左移32-n位，再右移动还原，通过判断这样操作之后得到的补码与x的补码是否仍然相同，来判断x是否只需要n位补码<br>3、最后加上一点补充，来更好地这个问题：<br>上面的思路对于正数来说是可以直接使用的，并且也是好理解的（因为闲置位置上的补码都是0，包括符号位也是0）</p><p>但是对于负数而言，最高位是1（符号位），我们或许会下意识认为，对于实际上的计算机而言，（从右往左，以下都是）第32位是没有闲置的，对于只需要n位补码的负数x，在32位的环境下，实际上被闲置的是第31位到第n位，而非和正数一样的第32位到第n+1位。进一步，我们会下意识认为，闲置的位置都是0，那么第31位是0，一旦左移符号位1就会被弃置，而再左移回来时在大多数情况下，符号位都只会是1，而认为由于32位环境最高位表示负数符号位的特殊性，导致了对于负数这样的“左移右移”无法解决问题。</p><p>显然，这样的理解是错误的，举个简单的反例，-1在32位中的补码是111…11（32个1），并非我们理解的闲置位置是0，所以其实对于一般的负数，我们会发现当它只需要n位补码表示的时候，在32位的环境下，其闲置的位置都是1，这样就不会出现我们认为的错误的情况，所以这个方法对于负数也是适用的。</p><pre><code>int l = 32 + ~n + 1; return !(x ^ (x &lt;&lt; l &gt;&gt; l));</code></pre><h2 id="按位运算计算-x-2-n-，向0取整"><a href="#按位运算计算-x-2-n-，向0取整" class="headerlink" title="按位运算计算 x&#x2F;2^n ，向0取整"></a>按位运算计算 x&#x2F;2^n ，向0取整</h2><p>思路：<br>1、首先我们要思考为什么有取整的问题，答案很简单，从一种简化的形式来说，答案可以用“1除以2除不尽（整数范围）”来概括，于是我们要考虑，该如何处理一下，让这个算式有一个结果。处理的方法就是向下取整（1&#x2F;2 &#x3D; 0）或者向上取整(1&#x2F;2 &#x3D; 1)<br>2、为了后面描述方便，我们在此前先看看向上、下取整，在1、在二进制下的形式。<br>不妨考虑（默认二进制，十进制末尾用D表示）奇数：XXXXX11（末位为1），除以2D，即右移1。XXXXX011&#x2F;2D &#x3D; (XXXXX010&#x2F;2D)+1&#x2F;2D，这就回到了1、，且更具有一般性。<br>如果要向下取整即1&#x2F;2D &#x3D; 1，在这个过程当中实际上等效于将1，当作10使用，即对XXXXX011进行了加1操作后进行XXXXX110&gt;&gt;1 &#x3D; XXXXXX11，而不同于原来的XXXXX011&gt;&gt;1 &#x3D; XXXXXX01<br>如果要向上取整，也就是直接抛弃最低位1，仍由右移时将它弃置。<br>3、所以1、中的情况在二进制下实际上就是考虑，要不要让末尾的1在一次右移当中被抛弃，如果是的话则对应向下取整，直接右移即可，如果不是的话则应该加1，让1-&gt;10，从而在除以2D时达到1D当作2D用的效果<br>4、所以一般地，对于除以2^n，即要右移动n次，是否要将前n位可能的1直接抛弃，就对应了是否要向下取整，所以我们直接对原数在前n位分别加上1，确保它们都有“1D当2D用的效果”即可达到向上取整的目的。<br>5、题目要求向0取整，对于正数而言向0取整就是向下取整，对于负数而言是向上取整。所以对正数直接右移n位，对负数在第n位后加1，再右移动n位。</p><h2 id="按位操作求-x"><a href="#按位操作求-x" class="headerlink" title="按位操作求-x"></a>按位操作求-x</h2><p>思路：原数的负数对应的补码，等于原数的补码取反再加1。</p><h2 id="按位运算判断x是否是正数"><a href="#按位运算判断x是否是正数" class="headerlink" title="按位运算判断x是否是正数"></a>按位运算判断x是否是正数</h2><p>思路：<br>1、考虑x的相反数，正数的相反数的补码符号位一定是1，负数及0的相反数的补码一定是0。<br>2、但是注意有一个特殊的负数存在，及-2^(n-1) - 1 &#x3D; -2147483648(32位)，这个负数是在32位补码对应的表示范围之内没有对应的相反数，按照常规的求相反数的补码（按位取反再加1）之后，得到的仍然是它本身，符号位不会改变。<br>3、所以在我们求完x的相反数y之后，只需要在最后查看y的符号（1则x为正，0则x为负或0）时：(y&gt;&gt;31)&amp;1（用于查看符号），加上(y&amp;(y^x)&gt;&gt;31)&amp;1（似乎还有一点问题）即可。因为y^x，的相当于查看x与其相反数的符号位是否相同，如果相同y^x的符号位是0，否则是1，即对-2147483648进行了特判。</p><h2 id="按位运算判断x是否小于等于y"><a href="#按位运算判断x是否小于等于y" class="headerlink" title="按位运算判断x是否小于等于y"></a>按位运算判断x是否小于等于y</h2><p>思路：<br>1、这道题要分三种情况考虑，我最开始想到的是情况1，直接判断x-y（即x + ~y + 1）的符号位是什么，如果是0，则x &gt; y，如果是1，则x &lt; y。<br>2、显然，上面的做法存在问题，很容易发现，无法处理x &#x3D;&#x3D; y的情况，这种情况下，对应x-y的符号位是0，而不同于x &lt; y时题目种要求的1。<br>所以我考虑了第二种情况，x &#x3D;&#x3D; y时，只需要判断!(x^y)的状态，如果是0则，x、y不等，这时交给1、判断，如果是1则x、y相等，只需要将1、2、中两个式子用|连接，如果2、中得到1则会直接得出答案，如果2、中得出0，也不会影响1、中的判断结果（零一律）。<br>3、然而还有一种情况是我们没有考虑到的，那就是在x、y异号的时候，可能出现的overflow，比如-2147483648，让其减去任意的负数，在32位补码的情况下，它都会变成一个正数（符号位为0），而这种情况是只会在x、y异号的时候出现的（同号时，只要是减法，都只会向0靠近，而不会向边界靠近，出现overflow），所以我们只需要和2、中一样，在加上一种情况的式子，并用|与前两个式子连接即可。所加的式子，首先判断x、y是否异号，如果是的话，我们可以直接通过x的符号给出答案，因为负数一定小于正数，所以第三种情况对应的式子是：((x^y)&gt;&gt;31&amp;1&amp;(x&gt;&gt;31&amp;1))，其中(x^y)&gt;&gt;31&amp;1在第一位给出了x、y符号是否相异，如果是的话，为1，后者(x&gt;&gt;31&amp;1)会在第一位给出x的符号；否则为0，整个式子为0，当前的大小情况交给1、2、来判断。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 计算机基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>汇编语言学习日志</title>
      <link href="/2024/10/15/my-first-post/"/>
      <url>/2024/10/15/my-first-post/</url>
      
        <content type="html"><![CDATA[<h2 id="DEBUG模式下几种基本命令"><a href="#DEBUG模式下几种基本命令" class="headerlink" title="DEBUG模式下几种基本命令"></a>DEBUG模式下几种基本命令</h2><p>t 追踪执行命令<br>a 指定内存后（回车）写入指令<br>d 从哪段内存开始 显示多长的内存&#x2F;&#x2F;显示内存中内容<br>e 从哪段内存开始（回车）修改内存<br>u 从哪段内存开始&#x2F;&#x2F;显示翻译后内存中内容</p><pre><code>mov ax,bxadd ax,bxsub ax,bx</code></pre><blockquote><p>注意高八位、低八位，h、l<br>注意数据溢出，加法溢出则舍去，减法不足则借位，指定了高八位、低八位就以对应的两个数字为参考标准确定是否要舍去与借位，不能修改其它数字。</p></blockquote><h2 id="常见的运算符"><a href="#常见的运算符" class="headerlink" title="常见的运算符"></a>常见的运算符</h2><h3 id="mul"><a href="#mul" class="headerlink" title="mul"></a>mul</h3><pre><code>mul bx</code></pre><p>相乘的数都是八位或者都是十六位<br>八位：一个默认放在AL中，另一个放在8位reg或内存中；结果默认放在AX中<br>十六位：一个默认放在AX中，另一个放在16位reg或内存中；结果默认低16位放在AX中，高16位放在DX中。</p><blockquote><p>8位：AL<em>BL &#x3D; AX<br>16位：AX</em>BX &#x3D; DXAX</p></blockquote><h3 id="div"><a href="#div" class="headerlink" title="div"></a>div</h3><pre><code>div bx</code></pre><p>除数：8位、16位（2个数字、4个数字）两种<br>被除数：默认放在AX或DX和AX中，除数为8位则被除数为16位默认放在AX中；如果除数为16位，则被除数为32位，默认放在DX和AX中，DX存放高16位，AX存放低16位。<br>结果：如果除数为8位，则AL中存放结果，AH储存余数；如果除数为16位，则AX存储商，DX存储余数。</p><blockquote><p>除数8位:AX&#x2F;BL &#x3D; AL … AH<br>除数16位：DXAX&#x2F;BX &#x3D; AX … DX</p></blockquote><h3 id="and、or"><a href="#and、or" class="headerlink" title="and、or"></a>and、or</h3><p>以二进制为标准<br>指定8位还是16位进行运算</p><h3 id="shl"><a href="#shl" class="headerlink" title="shl"></a>shl</h3><pre><code>shl ax,1</code></pre><p>shl、shr 左移、右移，超出的舍去，缺少的0补<br>rol、ror 循环左移、右移，超出的补到后面缺少的<br>带进位的循环左移、右移 rcl rcr（了解）<br>以二进制为标准</p><h3 id="inc、dec"><a href="#inc、dec" class="headerlink" title="inc、dec"></a>inc、dec</h3><pre><code>inc axdec ax</code></pre><p>相当于ax++、ax–<br>（进位、借位与普通加减法相同）</p><h3 id="其它杂项"><a href="#其它杂项" class="headerlink" title="其它杂项"></a>其它杂项</h3><p>如果除法除以0会进入一个中断<br>使用int 0也可以进入这个中断<br>如果除以0是一个错误，会触发int 0的中断，会找到一个地址，代码运行的指针会指向这个地址，接下来或许可以通过在这个地址准备相应的处理方法来解决异常（了解）</p><p>我们需要知道的就是当代码发生错误，代码运行的指针会跳转到其它地址。</p><p>常见的中断编号int 0、int 9</p><h2 id="ds寄存器与地址"><a href="#ds寄存器与地址" class="headerlink" title="ds寄存器与地址"></a>ds寄存器与地址</h2><blockquote><p>物理地址 &#x3D; 段地址*16+偏移地址</p></blockquote><p>DS寄存器，数据段地址寄存器<br>    r ds<br>可编辑DS当中的内容</p><p>配合mov指令使用，如</p><pre><code>mov ax,[60]</code></pre><p>效果会是将DS中的段地址结合[60]偏移地址指向的内容复制给ax<br>[]中对应的都是低位</p><blockquote><p>但是注意：在内存当中字单元的概念，即将一个字型数据（16位）的内存单元，由两个地址连续的内存单元，高地址内存单元（非起始）存放字符型高位字节，低地址内存单元（起始）存放字符型低位字节。<br>如我们将DS设置为21F0后，使用mov指令会将21F0:0060中的内容复制给ax<br>而d 21F0:0060这样显示：<br>12 34<br>则0060为低位，0061为高位<br>所以对应AXL 和 AXH，由此得到ax为3412<br>但是以上内容在使用al、ah等直接对八位操作的时候，对应的物理地址上的八位数会直接被移动到对应位置，而不需要考虑是高地址还是低地址内存</p></blockquote><p>另外</p><blockquote><p>注意：不能使用mov ds，10 00<br>但是可以<br>mov 1000，ax<br>mov ds，ax</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> 计算机基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>诗</title>
      <link href="/2020/01/01/%E8%AF%97/"/>
      <url>/2020/01/01/%E8%AF%97/</url>
      
        <content type="html"><![CDATA[<h2 id="壹"><a href="#壹" class="headerlink" title="壹"></a>壹</h2><blockquote><p>《甲辰正月初一为灵泉诸香客赋》<br>水澹金粼雾舞空，烟锁灵山山囚龙。<br>霰尽芳菲行者至，露上凝珠紫气东。<br>父老提携童子笑，诸君揖互彩云从。<br>香火连天今日有，爆竹声毕夕已终。</p></blockquote><p>~</p><blockquote><p>《危勉》<br>黄烛灯火暗凝香，红袖酥肩幻梦长。<br>醉酒吟誐任疏狂，百落临渊战心惶。<br>前路无光路满霜，苦难同尘藏四方。<br>来年不求万事祥，我愿与之较锋芒。</p></blockquote><p>~</p><blockquote><p>《贻君》<br>春秋五载与君识，几经离别未有知。<br>可怜怎奈书生骨，举杯邀酒话无时。<br>今昔难见来日昶，参商共映明月光。<br>高山流水何其多，相见不言别离长。</p></blockquote><h2 id="贰"><a href="#贰" class="headerlink" title="贰"></a>贰</h2><blockquote><p>《秋意尽》<br>秋霜一点红，寒露语松榕。<br>越鸟惊乡客，南巢临北冬。<br>遍揲晓风月，满羽繁星空。<br>择地息未有，淡裁碌日中。<br>何日花前酒，携诗青崖诵。</p></blockquote><p>~</p><blockquote><p>《不意雨》<br>云野伴骄阳，仲夏风曦长。<br>身待雨缦缦，旱久难滂滂。<br>卧阴苇下驻，遮乌泮水旁。<br>念死闲行日，不意甘霖降。</p></blockquote><h2 id="叁"><a href="#叁" class="headerlink" title="叁"></a>叁</h2><blockquote><p>《月夜寄故人》<br>细柳抚荫晓云畔，青山带水过君岸。<br>虽守陌路长相隔，念此故人月上看。</p></blockquote><p>~</p><blockquote><p>《碧波行》<br>见海连天月下开，踏彩逐浪思楼台。<br>台下霓潮绕浪舞，台上金麟为我来。</p></blockquote><p>~</p><blockquote><p>《临钟书阁》<br>长天碧色里，高阁一湖中。<br>传即千古士，行则知无穷。</p></blockquote><p>~</p><blockquote><p>《嘲》<br>携卷履地因风进，卧看长空皓月行。<br>明眸晶莹玉齿杯，蓬头恹面笑长情。</p></blockquote><h2 id="肆"><a href="#肆" class="headerlink" title="肆"></a>肆</h2><blockquote><p>《夏日乱笔》<br>墨上还新山水通，此间独骑临霜松。<br>昨年闻香落笔柔，意与锋芒寄青空。<br>海破日出乾坤动，河汉月邀金乌重。<br>沧浪折煞浮萍起，百波沉浮犬马恸。<br>他日锥断窠臼风，白鹤冲天烟柳中。<br>任子何笑鱼龙游，东风吹我麟阁梦。</p></blockquote><p>~</p><blockquote><p>《闻雨祭剑》<br>鸣剑八尺，柄灼伤伤。<br>血蚀锈迹，斑斑其煌。<br>剑折剑伤，不改剑刚。<br>素钢以铸，可刻金光。<br>百剑之中，其伤为扬。<br>剑鸣剑光，旦列剑榜。</p></blockquote><p>~</p><blockquote><p>《田间乱笔》<br>百日随春尽，新曦按夜来。<br>碧草凝寒露，陌上芳菲开。<br>折枝近门扉，黄䤋扶老待。<br>相论无功名，赤子初心在。</p></blockquote><p>~</p><blockquote><p>《告己书》<br>自负有翅，既委金笼。<br>不绝长空，何异斑鸠！<br>虽离群所，日起当思。<br>衣冠正否，行止绳否。<br>若背师恩，女颜不存。<br>若耽暖衾，女辞为空。<br>而至今日，才非盖世。<br>略有所长，只在微志。<br>今志但弃，形飞神灭。<br>渺渺于世，不知为人。<br>幸生父母，又遇恩师。<br>教吾配兰，视吾圣贤。<br>前见子丘，后瞻东坡。<br>周环吾身，竖子小儿。<br>其钻弥坚？其爱一欢？<br>若以为前，即当顺羽。<br>若就后居，不如便埋！</p></blockquote><p>~</p><blockquote><p>百年征程易难兴，只言片语寥可尽？<br>听罢放翁示儿语，途闻鹏举复国心。<br>鱼耽海者树鸟亲，风化雨兮草木喑。<br>今时往已难可忆，唯叹受多报无冯！</p></blockquote><p>~</p><blockquote><p>《白天野》<br>蹇驴踏寒山，欲寻春可安。<br>此去十二载，未觉铁岭难。<br>路别桃花庵，又至神女山。<br>朱唇抿贝齿，纵邀即日欢。<br>谢蛾眉观，仰首望河汉。<br>心驶沧浪处，欲起罢钓竿！</p></blockquote><h2 id="伍"><a href="#伍" class="headerlink" title="伍"></a>伍</h2><blockquote><p>《夏》<br>微月照鬓角，夏风拂过柳眉梢。<br>微踮首，咧嘴笑，花色泛上白裙脚。<br>又是一年蝉鸣时，荏苒莺飞鸟。<br>犹觉夏日胜好。</p></blockquote><p>~</p><blockquote><p>《览逍遥游有感》<br>余见千里之修厚鲲鹏者，以六月之息，适于南冥，一跃而上九万里，水击三千里。<br>余览其逍遥也，见其适者，天时也，非无所依也。然余谓之，己之限及身之短者，了然于胸，明所适者而享逍遥之微寥，不亦 逍遥也？<br>余见蜩与学鸠，以决起而飞，抢树而至，下鲲鹏之所适者，成其逍遥之逍遥也，特不解鲲鹏之所适者也。<br>余谓之，虽是井底之蛙，然于其身者，乐也，见世间万般皆下品，为己独高。于己之界，所达皆可有，所求皆以应，不亦逍遥 也？<br>且夫不死之椿，长生之冥灵，而宋荣子之辈不亦似鲲鹏哉，或有上中下之境焉也。<br>至于至人，神人，圣人，于天地之间无所适，乘万物之正，驾六气之辩者，余谓之人力所莫能及。然拊度其中，乘万物之正者 ，即庄周所言，物我为一，以天地万物之百变为己之息，如己心之动，血之流者。所适者皆如己出，故无所适也，通透哉！而 御六气之变，且夫游于无穷之中，仙人之谓也，非物力所能及。<br>故吾小儿，斗胆列逍遥之次，小，中，大者也。<br>小之逍遥，所限莫所大焉，然其不见，独达于己，鄙薄万物而如稚童，亦有乐者也。<br>中之逍遥，所限或有见微，然其了然，善适万物，夷其所长善之所短，诚然逍遥也。<br>大之逍遥，所限者于己身，通透明达，处之世间，无所不乐无所不适，此真逍遥也。<br>然三者之上，所能有游于无穷者，特为仙人之所处耳，言之何用？<br>小之逍遥，行年必出，中之逍遥，或死无穷。<br>故吾见中之逍遥亦有三般。<br>一曰，蟪蛄已死，化茧成蝶，初出井底，蓦然之余，惶惶之间，不知所为，旦怀为先，嘲之小儿以自饰，宋荣子之辈也。<br>二曰，明于世间，知所何为，明所不为，朝朝暮暮，施力其中，然有所乐，亦有所乏，或感万物之何为，或冥灵大椿也。<br>三曰，旷达自若，气定神闲，可借薄力，而撼世间，明哲处事，善以待人，乐而长命，无所喟叹无所疲，或鲲鹏之状也。<br>然一者之上，二者之下，或鄙之所谓也，目力有限，思有所疲，略省二者寥寥，而思三者谓何，至于大逍遥者，或愚竭毕生之 力而无以及，乃今之人或有一二，故余妄言，见之可由。余之所望，尽己之力，达于三者而望大逍遥者，仙人之风，果然之间 ，或付诸笑谈一二。</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> 诗歌 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
