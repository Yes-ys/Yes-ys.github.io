


<!DOCTYPE html>
<html lang="ch">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<title>高斯混合聚类 [ 代码和诗 ]</title>
	
	
	<!-- stylesheets list from _config.yml -->
	
	<link rel="stylesheet" href="/css/PreciousJoy.css">
	
	<link rel="stylesheet" href="/css/top-bar.css">
	
	<link rel="stylesheet" href="/css/menu-outer.css">
	
	<link rel="stylesheet" href="/css/content-outer.css">
	
	<link rel="stylesheet" href="/css/bottom-outer.css">
	
	<link rel="stylesheet" href="/css/atom-one-dark.css">
	
	<link rel="stylesheet" href="/css/recent-posts-item.css">
	
	<link rel="stylesheet" href="/css/article-sidebar-toc.css">
	
	<link rel="stylesheet" href="/css/jquery.fancybox.min.css">
	
	<link rel="stylesheet" href="/css/search.css">
	
	<link rel="stylesheet" href="/css/toc.css">
	
	<link rel="stylesheet" href="/css/sidebar.css">
	
	<link rel="stylesheet" href="/css/archive.css">
	
	<link rel="stylesheet" href="/css/jquery.mCustomScrollbar.min.css">
	
	<link rel="stylesheet" href="/css/Z-last-cover-others.css">
	
	
	
<meta name="generator" content="Hexo 7.3.0"></head>




<body id="wrapper">

	<div id="">
		
		<div id="top-bar">
			
			<div id="avatar-box">
				<img 
				class="avatar"
				src="/images/my-avatar.jpg" //网站头像
				alt="avatar">
			</div>

			<div id="top-bar-text">
				<div id="top-bar-title">
					阳生。
				</div>
				<div id="top-bar-slogan">
					风毛丛劲节，只上尽头竿。
				</div>
			</div>

		</div>

		<div id="menu-outer">
			<div id="menu-inner">
				
				
				<div class="menu-item">
					<a href="/">Home</a>
				</div>
				
				<div class="menu-item">
					<a href="/about">About</a>
				</div>
				
				<div class="menu-item">
					<a href="/archives">Archives</a>
				</div>
				

				<div class="menu-item menu-item-search">
					
  <span class="local-search local-search-google local-search-plugin">
      <input type="search" placeholder="站内搜索" id="local-search-input" class="local-search-input-cls" style="">
      <div id="local-search-result" class="local-search-result-cls"></div>
  </span>
	
				</div>

			</div>
		</div>

		<div id="content-outer">
			<div id="content-inner">

				
<div id="details">
	
	<article id="details-post">
		<div id=details-post-item>
			<h1>高斯混合聚类</h1>
			<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>高斯混合聚类不同于k-means、LVQ利用原型向量刻画聚类结构，而是利用概率来刻画聚类结构。</p>
<p>简单来说，这种算法认为数据集中的每个样本都符合一个多元高斯分布（多元的原因是样本常是多元向量），如下</p>
<p>所有的样本共同符合“混合高斯分布”。混合高斯分布对应的概率密度函数是所有多元高斯分布密度函数的加权量。</p>
<h3 id="多元高斯分布"><a href="#多元高斯分布" class="headerlink" title="多元高斯分布"></a>多元高斯分布</h3><p>若$x$服从多元高斯分布，对应概率密度函数为</p>
<p>$p(x) &#x3D; \frac{1}{(2\pi)^{\frac{n}{2}}\lvert \Sigma \rvert^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}$，其中$x$是样本对应的向量，$\Sigma$是协方差矩阵，$\mu$是期望</p>
<p>为了便于理解，参照一元高斯分布</p>
<p>$\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-1}{2}\frac{(x - \mu)^{2}}{\sigma^{2}}}$</p>
<p>协方差矩阵就对应方差、多元高斯分布中期望对应一元中的期望（只是多元高斯分布中期望是一个多维向量）</p>
<p>所以<strong>多元高斯分布的情况，由$\Sigma$和$\mu$唯一确定</strong></p>
<h3 id="混合高斯分布"><a href="#混合高斯分布" class="headerlink" title="混合高斯分布"></a>混合高斯分布</h3><p>将多元高斯分布密度函数记作$p(x|\mu,\Sigma)$</p>
<p>可以定义混合高斯分布如下：</p>
<p>若$x$服从混合高斯分布，整个样本空间对应有k种多元高斯分布，对应概率密度函数$p_M &#x3D; \sum^{k}<em>{i&#x3D;1} \alpha_i p(x|\mu_i,\Sigma_i)$，其中$\alpha$是混合系数，$\alpha_i$对应的实际意义是，选择第$i$个混合成分的概率，所以有$\alpha_i &gt; 0$且$\sum^{k}</em>{i&#x3D;1} \alpha_i &#x3D; 1$；而$p(x|\mu_i,\Sigma_i)$对应第i个混合成分的概率密度</p>
<h3 id="样本属于某混合成分的概率"><a href="#样本属于某混合成分的概率" class="headerlink" title="样本属于某混合成分的概率"></a>样本属于某混合成分的概率</h3><p>令数据集$D &#x3D; {x_1,x_2,…,x_m}$随机变量$z_j \in {1,2,…,k}$，$z_j$表征样本$x_j$属于哪个混合成分。</p>
<p>对于$x_j$并不确定的情况下，$z_j$的先验分布为</p>
<p>$p(z_j &#x3D; i) &#x3D; \alpha_i, i &#x3D; {1,2,…,k}$</p>
<p>根据贝叶斯定理，当$x_j$确定时，$z_j$的后验分布记作$p_M(z_j &#x3D; i|x_j)$，为</p>
<p>$p_M(z_j &#x3D; i|x_j) &#x3D; \frac{p(z_j &#x3D; i)p_M(x_j|z_j &#x3D; i)}{p_M(x_j)}$<br>其中，$p(z_j &#x3D; i) &#x3D; \alpha_i$；$p_M(x_j|z_j &#x3D; i) &#x3D; p(x|\mu_i,\Sigma_i)$，因为当确定$z_j &#x3D; i$时除了$\alpha_i &#x3D; 1$其它$\alpha_1,…\alpha_i-1,\alpha_i+1,…,\alpha_k$均为$0$；而$p_M(x_j)$即混合高斯分布的概率密度函数</p>
<p>将$p_M(z_j &#x3D; i|x_j)$简记作$\gamma_{ji}$，这个概率就是$x_j$的分布为$\mu_i,\Sigma_i$所对应的多元高斯分布的概率。</p>
<p>于是我们可以找到令$\gamma_{ji}$最大的$i \in {1,2,…,k}$，令$\lambda_j &#x3D; argmax_{i \in {1,2,…,k}} \gamma_{ji}$，$\lambda_j$即$x_j$的标签。对于每个都进行相同的操作，整个数据集便可被划分为k个多元高斯分布对应的k个簇。</p>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><h4 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h4><p>对于k-means、LVQ的模型训练实际上就是通过训练集获得对应的原型向量，有了原型向量便有了划分为簇的依据，也就完成了模型的训练。</p>
<p>而对于高斯混合聚类，根据前面的描述，我们划分为簇的重要依据就是$\gamma_{ji}$，进一步说实际上是<strong>计算$\gamma_{ji}$的依据</strong>，根据计算公式可知，这个依据实际上就是决定高斯混合分布的参数$\mu_i,\Sigma_i,\alpha_i, i \in {1,2,…,k}$。</p>
<p><strong>于是训练的目的实际上就是要得到它们的值。</strong></p>
<h4 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h4><p>对于模型参数${(\alpha_i,\mu_i,\Sigma_i)|1 \le i \le k}$，我们采用<strong>极大似然</strong>的方法进行求解。</p>
<p>这里引用南瓜书中的一句话：<br>“对于每个样本$x_j$来说，它出现的概率是$p_M(x_j)$既然现在训练集D中确实出现了$x_j$，我们当然希望待求解的参数${(\alpha_i,\mu_i,\Sigma_i)|1 \le i \le k}$能够使这种可能性$p_M(x_j)$最大”</p>
<p>于是根据极大似然方法，我们令<br>$LL(D) &#x3D; ln(\prod^{m}<em>{j&#x3D;1} p_M(x_j)) &#x3D; \sum^{m}</em>{j&#x3D;1} ln(\sum^k_{i&#x3D;1} \alpha_i \cdot p(x_j|\mu_i,\Sigma_i))$<br>为对数似然函数，将其最大化得到对应的参数，就是我们要求解的模型参数。</p>
<h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><p>经过一系列数学运算，我们可以得得到如下结果：</p>
<p>$\mu_i &#x3D; \frac{\sum^{m}_{j&#x3D;1} \gamma_{ji} x_j}{\sum^{m}_{j&#x3D;1} \gamma_{ji}}$</p>
<p>$\Sigma_i &#x3D; \frac{\sum^{m}_{j&#x3D;1} \gamma_{ji} (x_j - \mu_i)(x_j - \mu_i)^{T}}{\sum^{m}_{j&#x3D;1}} \gamma_{ji}$</p>
<p>$\alpha_i &#x3D; \frac{1}{m} \sum^{m}_{j&#x3D;1} \gamma{ji}$</p>
<p>$i &#x3D; 1,2,…,k$</p>
<p>注意结果当中，每一个参数的计算都要用到$\gamma{ji}$，而问题在于计算$\gamma{ji}$的时候又要用到参数本身，这似乎是一个循环的无从下手的问题。这种情况下，我们利用EM算法来求解。</p>
<h3 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h3><p>EM算法(Expectation-Maximization)称为“期望最大化算法”，这种算法最开始应用于：使用极大似然法对模型参数进行估计，但是已知的样本中存在还没有“观测”的变量，这种变量称为隐变量，它的值是不确定的。</p>
<p>令$X,Z,\Theta$分别为已观测变量集、隐变量集、参数集，则应最大化对数似然$LL(\Theta|X,Z) &#x3D; lnP(X,Z|\Theta)$，但是Z是隐变量，所以无法直接求解。</p>
<p>EM算法可以用于估计隐变量，并在这个过程中对参数做最大似然估计。</p>
<p>其基本的思想是这样的：</p>
<ol>
<li>初始化参数$\Theta$，根据参数去估计隐变量的<strong>概率分布</strong>，并利用此概率分布求得隐变量的<strong>期望</strong>——E步</li>
<li>将隐变量的期望作为我们观测到的隐变量本身，于是此时所有样本都已被观测，对$\Theta$做极大似然估计——M步</li>
</ol>
<p>不断重复上述两个过程——迭代，直到满足退出条件，例如$\Theta$收敛</p>
<p>贴一篇介绍EM算法的博客：<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41554005/article/details/100591525">CSDN</a></p>
<h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p>结合EM算法，整个高斯混合聚类算法流程如下：</p>
<p>输入：</p>
<ol>
<li>样本集$D &#x3D; {x_1,x_2,…,x_m}$</li>
<li>高斯混合成分的个数k（当然也就是希望划分出的簇的个数）</li>
</ol>
<p>过程：</p>
<ol>
<li>初始化高斯混合分布的参数${(\alpha_i,\mu_i,\Sigma_i)|i &#x3D; 1,2…,k}$</li>
<li>repeat：</li>
<li>对每个样本$x_j,j &#x3D; 1,2,…,m$估计其属于第$i,i &#x3D; 1,2,…k$个成分的概率：$\gamma_{ji}$</li>
<li>利用公式<br>$\mu_i &#x3D; \frac{\sum^{m}_{j&#x3D;1} \gamma_{ji} x_j}{\sum^{m}_{j&#x3D;1} \gamma_{ji}}$，<br>$\Sigma_i &#x3D; \frac{\sum^{m}_{j&#x3D;1} \gamma_{ji} (x_j - \mu_i)(x_j - \mu_i)^{T}}{\sum^{m}_{j&#x3D;1}} \gamma_{ji}$，<br>$\alpha_i &#x3D; \frac{1}{m} \sum^{m}_{j&#x3D;1} \gamma{ji}$，$i &#x3D; 1,2,…,k$对参数进行更新</li>
<li>until:收敛条件（达到一定轮数 or 参数收敛）</li>
<li>求解$x_j,j &#x3D; 1,2,…,m$的标记$\lambda_j &#x3D; agrmax_{i &#x3D; 1,2,…,k} \gamma_{ji}$</li>
<li>根据标记划分为对应的簇</li>
</ol>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>Data.py:数据集部分</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">D = [</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0.697</span>, <span class="number">0.460</span>], [<span class="number">0.774</span>, <span class="number">0.376</span>], [<span class="number">0.634</span>, <span class="number">0.264</span>], [<span class="number">0.608</span>, <span class="number">0.318</span>], [<span class="number">0.556</span>, <span class="number">0.215</span>],</span><br><span class="line">    [<span class="number">0.403</span>, <span class="number">0.237</span>], [<span class="number">0.481</span>, <span class="number">0.149</span>], [<span class="number">0.437</span>, <span class="number">0.211</span>], [<span class="number">0.666</span>, <span class="number">0.091</span>], [<span class="number">0.243</span>, <span class="number">0.267</span>],</span><br><span class="line">    [<span class="number">0.245</span>, <span class="number">0.057</span>], [<span class="number">0.343</span>, <span class="number">0.099</span>], [<span class="number">0.639</span>, <span class="number">0.161</span>], [<span class="number">0.657</span>, <span class="number">0.198</span>], [<span class="number">0.360</span>, <span class="number">0.370</span>],</span><br><span class="line">    [<span class="number">0.593</span>, <span class="number">0.042</span>], [<span class="number">0.719</span>, <span class="number">0.103</span>], [<span class="number">0.359</span>, <span class="number">0.188</span>], [<span class="number">0.339</span>, <span class="number">0.241</span>], [<span class="number">0.282</span>, <span class="number">0.257</span>],</span><br><span class="line">    [<span class="number">0.748</span>, <span class="number">0.232</span>], [<span class="number">0.714</span>, <span class="number">0.346</span>], [<span class="number">0.483</span>, <span class="number">0.312</span>], [<span class="number">0.478</span>, <span class="number">0.437</span>], [<span class="number">0.525</span>, <span class="number">0.369</span>],</span><br><span class="line">    [<span class="number">0.751</span>, <span class="number">0.489</span>], [<span class="number">0.532</span>, <span class="number">0.472</span>], [<span class="number">0.473</span>, <span class="number">0.376</span>], [<span class="number">0.725</span>, <span class="number">0.445</span>], [<span class="number">0.446</span>, <span class="number">0.459</span>]</span><br><span class="line">]  <span class="comment"># 数据集，1~30，0索引不使用</span></span><br></pre></td></tr></table></figure>

<p>main.py:主函数部分</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> Gauss</span><br><span class="line"><span class="keyword">import</span> Data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;高斯混合聚类 的结果&quot;</span>, end=<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">    res = Gauss.gauss(Data.D)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">len</span>(res[i]), end=<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(res[i])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Guass.py:高斯混合聚类算法部分</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multi_gauss_distri_p</span>(<span class="params">sigma, mu, n, x</span>):</span><br><span class="line">    <span class="comment"># 计算多元高斯分布下取得x的概率，n是维度</span></span><br><span class="line">    vec_mu = np.array(mu)</span><br><span class="line">    vec_x = np.array(x)</span><br><span class="line">    t1 = vec_x - vec_mu</span><br><span class="line">    <span class="comment">#    t1 = t1.T</span></span><br><span class="line">    det_sigma = np.linalg.det(sigma)</span><br><span class="line">    <span class="keyword">if</span>(det_sigma &lt; <span class="number">1e-10</span>):  <span class="comment"># 当行列式过小时，添加一个较小的正则化项</span></span><br><span class="line">        sigma += np.eye(sigma.shape[<span class="number">0</span>]) * <span class="number">1e-6</span>  <span class="comment"># 添加正则化，避免奇异矩阵</span></span><br><span class="line">    t2 = np.linalg.inv(sigma)</span><br><span class="line">    t3 = vec_x - vec_mu</span><br><span class="line">    t3 = t3.T  <span class="comment"># 西瓜书上的公式里没有转置的向量默认是列向量</span></span><br><span class="line">    log_p = -<span class="number">0.5</span> * (np.dot(np.dot(t1, t2), t3) + np.linalg.slogdet(sigma)[<span class="number">1</span>] + n * np.log(<span class="number">2</span> * np.pi))</span><br><span class="line">    p = np.exp(log_p)</span><br><span class="line"><span class="comment">#    p = 1 / ((2 * np.pi) ** (n / 2) * np.linalg.det(sigma) ** (1 / 2)) * np.e ** (-0.5 * np.dot(np.dot(t1, t2), t3)</span></span><br><span class="line"><span class="comment">#    print(&quot;p:&quot;, p, end=&quot;\n&quot;)</span></span><br><span class="line">    <span class="keyword">if</span> p &lt; <span class="number">1e-10</span>:</span><br><span class="line">        p = <span class="number">1e-6</span>  <span class="comment"># 避免数值问题</span></span><br><span class="line">    <span class="keyword">return</span> p</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">new_mu_i</span>(<span class="params">D, lamda_, i</span>):</span><br><span class="line">    up_sum = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    down_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">31</span>):</span><br><span class="line">        up_sum[<span class="number">0</span>] += lamda_[j][i] * D[j][<span class="number">0</span>]</span><br><span class="line">        up_sum[<span class="number">1</span>] += lamda_[j][i] * D[j][<span class="number">1</span>]</span><br><span class="line">        down_sum += lamda_[j][i]</span><br><span class="line">    new_mu = [up_sum[<span class="number">0</span>] / down_sum, up_sum[<span class="number">1</span>] / down_sum]</span><br><span class="line">    <span class="keyword">return</span> new_mu</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">new_sigma_i</span>(<span class="params">D, mu, lamda_, i</span>):</span><br><span class="line">    vec_x = np.array(D[i])</span><br><span class="line">    vec_mu = np.array(mu)</span><br><span class="line">    up_sum = np.array([[<span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]])</span><br><span class="line">    down_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">31</span>):</span><br><span class="line">        t = vec_x - vec_mu</span><br><span class="line">        up_sum += lamda_[j][i] * (np.outer(t.T, t))</span><br><span class="line">        down_sum += lamda_[j][i]</span><br><span class="line">    new_sigma = up_sum / down_sum</span><br><span class="line">    <span class="keyword">return</span> new_sigma</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">new_alpha_i</span>(<span class="params">lambda_, i</span>):</span><br><span class="line">    up_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">31</span>):</span><br><span class="line">        up_sum += lambda_[j][i]</span><br><span class="line">    new_alpha = up_sum / <span class="number">30</span></span><br><span class="line">    <span class="keyword">return</span> new_alpha</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gauss</span>(<span class="params">D: [[<span class="built_in">float</span>]]</span>):</span><br><span class="line">    <span class="comment">#  定义混合高斯分布参数</span></span><br><span class="line">    sigma = []  <span class="comment"># 协方差矩阵</span></span><br><span class="line">    alpha = []  <span class="comment"># 混合权重</span></span><br><span class="line">    mu = []  <span class="comment"># 期望</span></span><br><span class="line">    <span class="comment">#  初始化参数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">        sigma.append(np.array([[<span class="number">0.1</span>, <span class="number">0.0</span>], [<span class="number">0.0</span>, <span class="number">0.1</span>]]))</span><br><span class="line">        alpha.append(<span class="number">1</span>/<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            mu.append(D[<span class="number">6</span>])</span><br><span class="line">        <span class="keyword">elif</span> i == <span class="number">1</span>:</span><br><span class="line">            mu.append(D[<span class="number">22</span>])</span><br><span class="line">        <span class="keyword">elif</span> i == <span class="number">2</span>:</span><br><span class="line">            mu.append(D[<span class="number">27</span>])</span><br><span class="line">    <span class="comment">#  定义xj属于第i种多元高斯分布的概率</span></span><br><span class="line">    lambda_ = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">31</span>):</span><br><span class="line">        lambda_.append([[], [], []])</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">100</span>):  <span class="comment"># 迭代</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">31</span>):  <span class="comment"># 依次对每个xj计算lambda_ji,i = 0，1，2，对应三种多元高斯分布</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">                t_sum = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">                    t_sum += alpha[l] * multi_gauss_distri_p(sigma[l], mu[l], <span class="number">2</span>, D[j])</span><br><span class="line">                lambda_ji = alpha[i] * multi_gauss_distri_p(sigma[i], mu[i], <span class="number">2</span>, D[j]) / t_sum</span><br><span class="line">                lambda_[j][i] = lambda_ji</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):  <span class="comment"># 更新混合高斯分布参数</span></span><br><span class="line">            mu[i] = new_mu_i(D, lambda_, i)</span><br><span class="line">            sigma[i] = new_sigma_i(D, mu[i], lambda_, i)</span><br><span class="line">            alpha[i] = new_alpha_i(lambda_, i)</span><br><span class="line"></span><br><span class="line">    result = [[], [], []]  <span class="comment"># 最终的划分结果</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">31</span>):  <span class="comment"># 依次对每个xj计算lambda_ji,i = 0，1，2，对应三种多元高斯分布</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">            t_sum = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">                t_sum += alpha[l] * multi_gauss_distri_p(sigma[l], mu[l], <span class="number">2</span>, D[j])</span><br><span class="line">            lambda_ji = alpha[i] * multi_gauss_distri_p(sigma[i], mu[i], <span class="number">2</span>, D[j]) / t_sum</span><br><span class="line">            lambda_[j][i] = lambda_ji</span><br><span class="line"></span><br><span class="line">        flag = <span class="number">0</span></span><br><span class="line">        ll = lambda_[j][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">3</span>):</span><br><span class="line">            <span class="keyword">if</span> ll &lt; lambda_[j][i]:</span><br><span class="line">                flag = i</span><br><span class="line">        result[flag].append(D[j])</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>进行这一部分的时候让我最大的感悟是，在使用计算机进行数据处理的时候，很可能会出现数据的溢出问题，非常大、非常小的数都在计算机的表示范围之外，就会带来问题。例如在进行协方差矩阵更新的时候，有时它的行列式值虽然不是0，但是已经非常小了，计算机会默认其为0。同样，可能作为除数的数也是一样的，如果太小变为0就会发送除以0的错误。<br><strong>这称为数值稳定性问题</strong></p>
<p>因此代码中有一些涉及处理这些问题的地方（主要是在求第i个多元高斯分布中取得xj这个值的概率的时候，一处是正则化、一处是对数化并添加过小的判断），尽管我现在并不清楚这样处理是否有问题。但是抛开这些细节，作为一次上手的练习，整个混合高斯聚类算法是正确的。<br><strong>不过值得一提的是，EM算法的收敛与迭代的次数没有必然的关系，通常应该使用参数变化的情况来决定是否结束迭代</strong></p>
<p>最后的结果如下：</p>
<blockquote>
<p>高斯混合聚类 的结果<br>2<br>[[0.608, 0.318], [0.359, 0.188]]<br>0<br>[]<br>28<br>[[0.697, 0.46], [0.774, 0.376], [0.634, 0.264], [0.556, 0.215], [0.403, 0.237], [0.481, 0.149], [0.437, 0.211], [0.666, 0.091], [0.243, 0.267], [0.245, 0.057], [0.343, 0.099], [0.639, 0.161], [0.657, 0.198], [0.36, 0.37], [0.593, 0.042], [0.719, 0.103], [0.339, 0.241], [0.282, 0.257], [0.748, 0.232], [0.714, 0.346], [0.483, 0.312], [0.478, 0.437], [0.525, 0.369], [0.751, 0.489], [0.532, 0.472], [0.473, 0.376], [0.725, 0.445], [0.446, 0.459]]</p>
</blockquote>
<p>经过我的观察，第3次迭代之后划分结果基本上就稳定了，第1次的时候分得比较均匀。这或许是数据的原因。（也有可能是我对于数值稳定性的处理不好）</p>
<p>如果将正则化项改大一些结果如下：</p>
<blockquote>
<p>高斯混合聚类 的结果<br>12<br>[[0.608, 0.318], [0.556, 0.215], [0.403, 0.237], [0.481, 0.149], [0.437, 0.211], [0.243, 0.267], [0.245, 0.057], [0.343, 0.099], [0.359, 0.188], [0.339, 0.241], [0.282, 0.257], [0.483, 0.312]]<br>10<br>[[0.634, 0.264], [0.666, 0.091], [0.639, 0.161], [0.657, 0.198], [0.593, 0.042], [0.719, 0.103], [0.748, 0.232], [0.714, 0.346], [0.751, 0.489], [0.725, 0.445]]<br>8<br>[[0.697, 0.46], [0.774, 0.376], [0.36, 0.37], [0.478, 0.437], [0.525, 0.369], [0.532, 0.472], [0.473, 0.376], [0.446, 0.459]]</p>
</blockquote>

		</div>

		<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC80NjIyNC8yMjczNQ==">
	<script type="text/javascript">
   (function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
	</script>
<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->
		
	</article>

	<div id="toc">
		
	</div>

</div>

<!-- <div id="paginator"> -->
<!-- 	 -->
<!-- </div> -->


			</div>
		</div>

		<div id="bottom-outer">
			<div id="bottom-inner">
				Site by 阳生 | 
				Powered by <a target="_blank" rel="noopener" href="http://hexo.io">Hexo</a> |
				theme <a target="_blank" rel="noopener" href="https://github.com/fireworks99/hexo-theme-PreciousJoy">PreciousJoy</a>
			</div>
		</div>

		
	</div>





	
	<!-- scripts list from theme config.yml -->
	
	<script src="/js/jquery-3.5.1.min.js"></script>
	
	<script src="/js/PreciousJoy.js"></script>
	
	<script src="/js/highlight.pack.js"></script>
	
	<script src="/js/jquery.fancybox.min.js"></script>
	
	<script src="/js/search.js"></script>
	
	<script src="/js/load.js"></script>
	
	<script src="/js/jquery.mCustomScrollbar.concat.min.js"></script>
	
	<script src="/js/clipboard.min.js"></script>
	
	

	<script>hljs.initHighlightingOnLoad();</script>

</body>
</html>
