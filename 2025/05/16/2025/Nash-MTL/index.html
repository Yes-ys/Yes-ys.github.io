


<!DOCTYPE html>
<html lang="ch">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<title>Nash-MTL（代码实现） [ 代码和诗 ]</title>

	<link rel="shortcut icon" href="/myicon.ico">
	
	
	<!-- stylesheets list from _config.yml -->
	
	<link rel="stylesheet" href="/css/PreciousJoy.css">
	
	<link rel="stylesheet" href="/css/top-bar.css">
	
	<link rel="stylesheet" href="/css/menu-outer.css">
	
	<link rel="stylesheet" href="/css/content-outer.css">
	
	<link rel="stylesheet" href="/css/bottom-outer.css">
	
	<link rel="stylesheet" href="/css/atom-one-dark.css">
	
	<link rel="stylesheet" href="/css/recent-posts-item.css">
	
	<link rel="stylesheet" href="/css/article-sidebar-toc.css">
	
	<link rel="stylesheet" href="/css/jquery.fancybox.min.css">
	
	<link rel="stylesheet" href="/css/search.css">
	
	<link rel="stylesheet" href="/css/toc.css">
	
	<link rel="stylesheet" href="/css/sidebar.css">
	
	<link rel="stylesheet" href="/css/archive.css">
	
	<link rel="stylesheet" href="/css/jquery.mCustomScrollbar.min.css">
	
	<link rel="stylesheet" href="/css/Z-last-cover-others.css">
	
	
	
<meta name="generator" content="Hexo 7.3.0"></head>




<body id="wrapper">

	<div id="">
		
		<div id="top-bar">
			
			<div id="avatar-box">
				<img 
				class="avatar"
				src="/images/my-avatar.jpg" //网站头像
				alt="avatar">
			</div>

			<div id="top-bar-text">
				<div id="top-bar-title">
					阳生。
				</div>
				<div id="top-bar-slogan">
					风毛丛劲节，只上尽头竿。
				</div>
			</div>

		</div>

		<div id="menu-outer">
			<div id="menu-inner">
				
				
				<div class="menu-item">
					<a href="/">Home</a>
				</div>
				
				<div class="menu-item">
					<a href="/about">About</a>
				</div>
				
				<div class="menu-item">
					<a href="/archives">Archives</a>
				</div>
				
				<div class="menu-item">
					<a href="/plans">Plans</a>
				</div>
				

				<div class="menu-item menu-item-search">
					
  <span class="local-search local-search-google local-search-plugin">
      <input type="search" placeholder="站内搜索" id="local-search-input" class="local-search-input-cls" style="">
      <div id="local-search-result" class="local-search-result-cls"></div>
  </span>
	
				</div>

			</div>
		</div>

		<div id="content-outer">
			<div id="content-inner">

				
<div id="details">
	
	<article id="details-post">
		<div id=details-post-item>
			<h1>Nash-MTL（代码实现）</h1>
			<p><code>这篇blog用于记录我阅读Nash-MTL算法基于python的implemention，其中存在大量使用pytorch、tensorflow的地方，这些是我之前几乎没有接触过的东西，所以同时我也会进行相关的记录，进行学习。</code></p>
<h2 id="Readme"><a href="#Readme" class="headerlink" title="Readme"></a>Readme</h2><p><code>整篇blog从Nash-MTL开源项目的Readme文档开始，可以帮助我了解整个项目的文件组织结构。</code></p>
<h3 id="conda"><a href="#conda" class="headerlink" title="conda"></a>conda</h3><p>在Python中，conda 是一个开源的包管理器和环境管理器，它主要用于安装和管理Python包以及创建和管理虚拟环境。</p>
<h4 id="什么是包管理器和环境管理器"><a href="#什么是包管理器和环境管理器" class="headerlink" title="什么是包管理器和环境管理器"></a>什么是包管理器和环境管理器</h4><p>通过查阅资料，我现在通俗的理解是：</p>
<ol>
<li>包&#x2F;库，项目开发过程中用到的封装好的<strong>编程语言（或第三方）预置代码</strong></li>
<li>环境，当前项目所基于的所有包&#x2F;库的总称，是<strong>项目的外部依赖</strong></li>
</ol>
<h4 id="一个困惑"><a href="#一个困惑" class="headerlink" title="一个困惑"></a>一个困惑</h4><p>我之前一直有这样的困惑：为什么Py的各种包不能像C++的库一样，一次下载，可以在任何项目文件中使用；就例如numpy而言，每次我都需要在新的项目目录中通过pip重新下载，然后再导入到文件。</p>
<p>这次我查阅了相关资料，了解到：</p>
<ol>
<li><p>C++和Python本身对包管理“如何依赖”的设计就不同，C++ 没有官方统一的包管理工具，依赖通常由系统包管理器（如 apt、brew）或手动安装到系统目录。这种设计<strong>适合系统级工具</strong>，但容易导致版本冲突；Python 通过虚拟环境（如 venv、conda）为每个项目创建独立的依赖环境，避免全局污染。这是为了解决 Python 生态中<strong>广泛存在</strong>的依赖版本冲突问题。</p>
</li>
<li><p>C++的标准库（如STL）是语言规范的一部分，与编译器深度绑定。因此具有特权地位，它们的实现（如 libstdc++ 或 libc++）随编译器安装，因此无需额外管理。但是第三方的库（如OpenCV）仍需手动管理，其使用方式更接近 Python 的第三方包。所以并非C++不需要额外的包管理就可以一次安装随意使用，而是我目前的学识并没有接触到需要包管理的地方。</p>
</li>
<li><p>Python 标准库与第三方包的平等性，如 os、sys虽然随解释器预装，但第三方包（如 numpy）在导入机制上与标准库完全平等。</p>
</li>
<li><p>Python的虚拟环境使得包不能直接全局复用，但是这并非强制的，如果不使用虚拟环境，第三方包会被安装到全局 site-packages，所有项目共享。但这会导致：不同项目依赖的版本冲突；无权限修改全局环境（例如在服务器上）。因此，<strong>虚拟环境是 Python 社区的最佳实践</strong>，而非技术限制。</p>
</li>
</ol>
<h3 id="项目的文件组织"><a href="#项目的文件组织" class="headerlink" title="项目的文件组织"></a>项目的文件组织</h3><p>整个nash-mtl下包含两个主要dir</p>
<ol>
<li>experiments，其中包含了3个MTL任务文件相关代码</li>
<li>methods，其中包含了可以用于处理MTL任务的算法代码</li>
</ol>
<p>二者的关系，experiments中<code>trainer.py</code>是任务入口，对于每一个任务，我们可以指定一个methods中的算法来解决。</p>
<p>于是，<strong>接下来我打算从toy任务入手，先阅读任务文件代码，再阅读methods中Nash-MTL算法部分的代码</strong></p>
<h2 id="packages：torch"><a href="#packages：torch" class="headerlink" title="packages：torch"></a>packages：torch</h2><h3 id="clamp"><a href="#clamp" class="headerlink" title="clamp"></a>clamp</h3><p>在 PyTorch 中，torch.clamp 是一个非常实用的函数，主要用于对张量中的元素进行截断（clamping），将其限制在一个指定的区间范围内。</p>
<p>原型</p>
<blockquote>
<p>torch.clamp(input, min&#x3D;None, max&#x3D;None) → Tensor</p>
</blockquote>
<p>input<br>类型：Tensor<br>需要进行截断操作的输入张量。</p>
<p>min<br>类型：float 或 None（默认值）<br>指定张量中元素的最小值。小于 min 的元素会被截断为 min 值。<br>如果设置为 None，则表示不限制最小值。</p>
<p>max<br>类型：float 或 None（默认值）<br>指定张量中元素的最大值。大于 max 的元素会被截断为 max 值。<br>如果设置为 None，则表示不限制最大值。</p>
<p>返回值<br>返回一个新的张量，其中元素已经被限制在 ([min, max]) 的范围内。<br>原张量不会被修改（函数是非原地操作），除非使用 torch.clamp_ 以进行原地操作。</p>
<p>参考资料来源：<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/shizheng_Li/article/details/144432030">CSDN</a></p>
<h3 id="stack"><a href="#stack" class="headerlink" title="stack"></a>stack</h3><p>torch.stack() 是一个非常有用的函数，用于沿着指定的维度（dim）将多个张量（Tensor）合并成一个新的张量。它可以把一个包含多个张量的序列（比如一个列表或元组）按照给定的维度拼接成一个新的张量。</p>
<p>原型</p>
<blockquote>
<p>torch.stack(tensors, dim&#x3D;0, out&#x3D;None)→ Tensor</p>
</blockquote>
<p>tensors: 一个包含多个张量的序列（例如列表或元组）。这些张量必须具有相同的形状（即，除了要拼接的维度外，其他所有维度的大小必须相同）。<br>dim: 沿着哪个维度进行拼接。dim 参数决定了新张量的形状以及拼接方向。这个参数的值可以是负数，表示从最后一个维度开始倒数。<br>out: 可选，指定输出张量。如果没有提供，函数会自动生成一个新的张量。</p>
<p>补充：关于dim，假设原始的tensor.size() &#x3D; ([x,y,z])，参与拼接的tensor共有a个，dim&#x3D;0，1，2，3的结果分别为：</p>
<ol>
<li>tensor.size() &#x3D; ([a,x,y,z])</li>
<li>tensor.size() &#x3D; ([x,a,y,z])</li>
<li>tensor.size() &#x3D; ([x,y,a,z])</li>
<li>tensor.size() &#x3D; ([x,y,z,a])</li>
</ol>
<p>相应的tensor的“形状”也会有所改变，但是其中包含的信息却是没有改变的，而dim &#x3D; 0是比较容易理解也是比较常用的</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_40507857/article/details/119854085">CSDN</a></p>
<h3 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h3><p>在给定维度上拼接输入序列 tensors 中的张量。所有张量除了拼接维度外必须具有相同的形状，或者是一个大小为 (0,) 的一维空张量。</p>
<p>原型</p>
<blockquote>
<p>torch.cat(tensors, dim&#x3D;0, *, out&#x3D;None) → Tensor</p>
</blockquote>
<p>tensors (Tensor 序列) – 提供的非空张量除了拼接维度外必须具有相同的形状。</p>
<p>dim (int, 可选) – 拼接张量的维度</p>
<p>out (Tensor, 可选) – 输出张量。</p>
<p>示例</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(2, 3)
&gt;&gt;&gt; x
tensor([[ 0.6580, -1.0969, -0.4614],
    [-0.1034, -0.5790,  0.1497]])
&gt;&gt;&gt; torch.cat((x, x, x), 0)
tensor([[ 0.6580, -1.0969, -0.4614],
    [-0.1034, -0.5790,  0.1497],
    [ 0.6580, -1.0969, -0.4614],
    [-0.1034, -0.5790,  0.1497],
    [ 0.6580, -1.0969, -0.4614],
    [-0.1034, -0.5790,  0.1497]])
&gt;&gt;&gt; torch.cat((x, x, x), 1)
tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,
     -1.0969, -0.4614],
    [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,
     -0.5790,  0.1497]])
</code></pre>
<p>注意其与stack的区别，它不会增加新的维度，而是在指定的维度上将所有的tensor拼接到一起</p>
<h3 id="torch-autograd-grad"><a href="#torch-autograd-grad" class="headerlink" title="torch.autograd.grad()"></a>torch.autograd.grad()</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/279758736">知乎</a></p>
<h2 id="Experiment：toy"><a href="#Experiment：toy" class="headerlink" title="Experiment：toy"></a>Experiment：toy</h2><h3 id="train-py"><a href="#train-py" class="headerlink" title="train.py"></a>train.py</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> wandb</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> ArgumentParser</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> experiments.toy.problem <span class="keyword">import</span> Toy</span><br><span class="line"><span class="keyword">from</span> experiments.toy.utils <span class="keyword">import</span> plot_2d_pareto</span><br><span class="line"><span class="keyword">from</span> experiments.utils <span class="keyword">import</span> (</span><br><span class="line">    common_parser,</span><br><span class="line">    extract_weight_method_parameters_from_args,</span><br><span class="line">    set_logger,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> methods.weight_methods <span class="keyword">import</span> WeightMethods</span><br><span class="line"></span><br><span class="line">set_logger()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">method_type, device, n_iter, scale</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选定MTL参数θ更新方法</span></span><br><span class="line">    weight_methods_parameters = extract_weight_method_parameters_from_args(args)</span><br><span class="line">    n_tasks = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 确定任务函数</span></span><br><span class="line">    F = Toy(scale=scale)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用于描述优化过程的损失函数变化的轨迹</span></span><br><span class="line">    all_traj = <span class="built_in">dict</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the initial positions（初始参数）</span></span><br><span class="line">    inits = [</span><br><span class="line">        torch.Tensor([-<span class="number">8.5</span>, <span class="number">7.5</span>]),</span><br><span class="line">        torch.Tensor([<span class="number">0.0</span>, <span class="number">0.0</span>]),</span><br><span class="line">        torch.Tensor([<span class="number">9.0</span>, <span class="number">9.0</span>]),</span><br><span class="line">        torch.Tensor([-<span class="number">7.5</span>, -<span class="number">0.5</span>]),</span><br><span class="line">        torch.Tensor([<span class="number">9</span>, -<span class="number">1.0</span>]),</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, init <span class="keyword">in</span> <span class="built_in">enumerate</span>(inits):</span><br><span class="line">        <span class="comment"># init对应每个初始参数，i用于索引轨迹存储在all_traj中的位置</span></span><br><span class="line">        traj = []</span><br><span class="line">        x = init.clone()</span><br><span class="line">        x.requires_grad = <span class="literal">True</span></span><br><span class="line">        x = x.to(device)</span><br><span class="line"></span><br><span class="line">        method = WeightMethods(</span><br><span class="line">            method=method_type,</span><br><span class="line">            device=device,</span><br><span class="line">            n_tasks=n_tasks,</span><br><span class="line">            **weight_methods_parameters[method_type],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 优化器初始化，确定了其要同时优化x（任务的参数）、具体method的参数；lr是学习率</span></span><br><span class="line">        optimizer = torch.optim.Adam(</span><br><span class="line">            [</span><br><span class="line">                <span class="built_in">dict</span>(params=[x], lr=<span class="number">1e-3</span>),</span><br><span class="line">                <span class="built_in">dict</span>(params=method.parameters(), lr=args.method_params_lr),</span><br><span class="line">            ],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(n_iter)):</span><br><span class="line">            traj.append(x.cpu().detach().numpy().copy())</span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            f = F(x, <span class="literal">False</span>) <span class="comment"># 计算损失函数</span></span><br><span class="line">            <span class="comment"># 根据选定方法，“反向传播”更新参数，我在复现时用的nashmtl</span></span><br><span class="line">            _ = method.backward( </span><br><span class="line">                losses=f,</span><br><span class="line">                shared_parameters=(x,),</span><br><span class="line">                task_specific_parameters=<span class="literal">None</span>,</span><br><span class="line">                last_shared_parameters=<span class="literal">None</span>,</span><br><span class="line">                representation=<span class="literal">None</span>,</span><br><span class="line">            )</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">        all_traj[i] = <span class="built_in">dict</span>(init=init.cpu().detach().numpy().copy(), traj=np.array(traj))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> all_traj</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    parser = ArgumentParser(</span><br><span class="line">        <span class="string">&quot;Toy example (modification of the one in CAGrad)&quot;</span>, parents=[common_parser]</span><br><span class="line">    )</span><br><span class="line">    parser.set_defaults(n_epochs=<span class="number">35000</span>, method=<span class="string">&quot;nashmtl&quot;</span>, data_path=<span class="literal">None</span>)</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--scale&quot;</span>, default=<span class="number">1e-1</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, <span class="built_in">help</span>=<span class="string">&quot;scale for first loss&quot;</span></span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--out-path&quot;</span>, default=<span class="string">&quot;outputs&quot;</span>, <span class="built_in">type</span>=Path, <span class="built_in">help</span>=<span class="string">&quot;output path&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--wandb_project&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="literal">None</span>, <span class="built_in">help</span>=<span class="string">&quot;Name of Weights &amp; Biases Project.&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--wandb_entity&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="literal">None</span>, <span class="built_in">help</span>=<span class="string">&quot;Name of Weights &amp; Biases Entity.&quot;</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.wandb_project <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        wandb.init(project=args.wandb_project, entity=args.wandb_entity, config=args)</span><br><span class="line"></span><br><span class="line">    out_path = args.out_path</span><br><span class="line">    out_path.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    logging.info(<span class="string">f&quot;Logs and plots are saved in: <span class="subst">&#123;out_path.as_posix()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    all_traj = main(</span><br><span class="line">        method_type=args.method, device=device, n_iter=args.n_epochs, scale=args.scale</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot</span></span><br><span class="line">    ax, fig, legend = plot_2d_pareto(trajectories=all_traj, scale=args.scale)</span><br><span class="line"></span><br><span class="line">    title_map = &#123;</span><br><span class="line">        <span class="string">&quot;nashmtl&quot;</span>: <span class="string">&quot;Nash-MTL(Ours)&quot;</span>,</span><br><span class="line">        <span class="string">&quot;cagrad&quot;</span>: <span class="string">&quot;CAGrad&quot;</span>,</span><br><span class="line">        <span class="string">&quot;mgda&quot;</span>: <span class="string">&quot;MGDA&quot;</span>,</span><br><span class="line">        <span class="string">&quot;pcgrad&quot;</span>: <span class="string">&quot;PCGrad&quot;</span>,</span><br><span class="line">        <span class="string">&quot;ls&quot;</span>: <span class="string">&quot;LS&quot;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">    ax.set_title(title_map[args.method], fontsize=<span class="number">25</span>)</span><br><span class="line">    plt.savefig(</span><br><span class="line">        out_path / <span class="string">f&quot;<span class="subst">&#123;args.method&#125;</span>.png&quot;</span>,</span><br><span class="line">        bbox_extra_artists=(legend,),</span><br><span class="line">        bbox_inches=<span class="string">&quot;tight&quot;</span>,</span><br><span class="line">        facecolor=<span class="string">&quot;white&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    plt.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> wandb.run <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        wandb.log(&#123;<span class="string">&quot;Pareto Front&quot;</span>: wandb.Image((out_path / <span class="string">f&quot;<span class="subst">&#123;args.method&#125;</span>.png&quot;</span>).as_posix())&#125;)</span><br><span class="line"></span><br><span class="line">        wandb.finish()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="problem-py"><a href="#problem-py" class="headerlink" title="problem.py"></a>problem.py</h3><p>该模块代码如下</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#这个模型定义了两个损失函数（或目标函数） f1 和 f2，它们是输入变量 $x = [x_1, x_2]$的非线性函数，并允许计算梯度信息。</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="comment">#nn 是 PyTorch 中用于构建神经网络的模块，包括 Module、常用的层（如 Linear、Conv2d 等）和常用的损失函数等。</span></span><br><span class="line">LOWER = <span class="number">0.000005</span></span><br><span class="line"><span class="comment">#定义一个常量 LOWER，用于在计算中对一些值进行下限裁剪。这个常量用于避免对数计算中的负值或零（log(0)），从而防止梯度爆炸或 NaN 问题。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Toy</span>(nn.Module):</span><br><span class="line">    <span class="comment">#定义了一个名为 Toy 的类，继承自 PyTorch 中的 nn.Module。nn.Module 是所有神经网络模块的基类，提供了很多通用功能，如参数注册、反向传播等。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, scale=<span class="number">1.0</span>, scale_both_losses=<span class="number">1.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Toy, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.centers = torch.Tensor([[-<span class="number">3.0</span>, <span class="number">0</span>], [<span class="number">3.0</span>, <span class="number">0</span>]])</span><br><span class="line">        <span class="variable language_">self</span>.scale = scale</span><br><span class="line">        <span class="variable language_">self</span>.scale_both_losses = scale_both_losses</span><br><span class="line">    <span class="comment"># centers 属性，是一个 2x2 的张量，代表两个中心点的坐标 [-3.0, 0] 和 [3.0, 0]。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, compute_grad=<span class="literal">False</span></span>):</span><br><span class="line">        x1 = x[<span class="number">0</span>]</span><br><span class="line">        x2 = x[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        f1 = torch.clamp((<span class="number">0.5</span> * (-x1 - <span class="number">7</span>) - torch.tanh(-x2)).<span class="built_in">abs</span>(), LOWER).log() + <span class="number">6</span></span><br><span class="line">        f2 = torch.clamp((<span class="number">0.5</span> * (-x1 + <span class="number">3</span>) + torch.tanh(-x2) + <span class="number">2</span>).<span class="built_in">abs</span>(), LOWER).log() + <span class="number">6</span></span><br><span class="line">        c1 = torch.clamp(torch.tanh(x2 * <span class="number">0.5</span>), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        f1_sq = ((-x1 + <span class="number">7</span>).<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.1</span> * (-x2 - <span class="number">8</span>).<span class="built_in">pow</span>(<span class="number">2</span>)) / <span class="number">10</span> - <span class="number">20</span></span><br><span class="line">        f2_sq = ((-x1 - <span class="number">7</span>).<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.1</span> * (-x2 - <span class="number">8</span>).<span class="built_in">pow</span>(<span class="number">2</span>)) / <span class="number">10</span> - <span class="number">20</span></span><br><span class="line">        c2 = torch.clamp(torch.tanh(-x2 * <span class="number">0.5</span>), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        f1 = f1 * c1 + f1_sq * c2</span><br><span class="line">        f1 *= <span class="variable language_">self</span>.scale</span><br><span class="line">        f2 = f2 * c1 + f2_sq * c2</span><br><span class="line"></span><br><span class="line">        f = torch.stack([f1, f2]) * <span class="variable language_">self</span>.scale_both_losses</span><br><span class="line">        <span class="keyword">if</span> compute_grad:</span><br><span class="line">            g11 = torch.autograd.grad(f1, x1, retain_graph=<span class="literal">True</span>)[<span class="number">0</span>].item()</span><br><span class="line">            g12 = torch.autograd.grad(f1, x2, retain_graph=<span class="literal">True</span>)[<span class="number">0</span>].item()</span><br><span class="line">            g21 = torch.autograd.grad(f2, x1, retain_graph=<span class="literal">True</span>)[<span class="number">0</span>].item()</span><br><span class="line">            g22 = torch.autograd.grad(f2, x2, retain_graph=<span class="literal">True</span>)[<span class="number">0</span>].item()</span><br><span class="line">            g = torch.Tensor([[g11, g21], [g12, g22]])</span><br><span class="line">            <span class="keyword">return</span> f, g</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> f</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">batch_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x1 = x[:, <span class="number">0</span>]</span><br><span class="line">        x2 = x[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        f1 = torch.clamp((<span class="number">0.5</span> * (-x1 - <span class="number">7</span>) - torch.tanh(-x2)).<span class="built_in">abs</span>(), LOWER).log() + <span class="number">6</span></span><br><span class="line">        f2 = torch.clamp((<span class="number">0.5</span> * (-x1 + <span class="number">3</span>) + torch.tanh(-x2) + <span class="number">2</span>).<span class="built_in">abs</span>(), LOWER).log() + <span class="number">6</span></span><br><span class="line">        c1 = torch.clamp(torch.tanh(x2 * <span class="number">0.5</span>), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        f1_sq = ((-x1 + <span class="number">7</span>).<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.1</span> * (-x2 - <span class="number">8</span>).<span class="built_in">pow</span>(<span class="number">2</span>)) / <span class="number">10</span> - <span class="number">20</span></span><br><span class="line">        f2_sq = ((-x1 - <span class="number">7</span>).<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.1</span> * (-x2 - <span class="number">8</span>).<span class="built_in">pow</span>(<span class="number">2</span>)) / <span class="number">10</span> - <span class="number">20</span></span><br><span class="line">        c2 = torch.clamp(torch.tanh(-x2 * <span class="number">0.5</span>), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        f1 = f1 * c1 + f1_sq * c2</span><br><span class="line">        f1 *= <span class="variable language_">self</span>.scale</span><br><span class="line">        f2 = f2 * c1 + f2_sq * c2</span><br><span class="line"></span><br><span class="line">        f = torch.cat([f1.view(-<span class="number">1</span>, <span class="number">1</span>), f2.view(-<span class="number">1</span>, <span class="number">1</span>)], -<span class="number">1</span>) * <span class="variable language_">self</span>.scale_both_losses</span><br><span class="line">        <span class="keyword">return</span> f</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Nash-MTL"><a href="#Nash-MTL" class="headerlink" title="Nash-MTL"></a>Nash-MTL</h3><p>该类的目标是通过一个优化问题来计算每个任务的权重（alpha），从而实现加权多任务学习。具体来说，该方法使用的是一个基于纳什均衡的算法来计算任务之间的加权关系。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NashMTL</span>(<span class="title class_ inherited__">WeightMethod</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        n_tasks: <span class="built_in">int</span>, <span class="comment">#任务数量</span></span></span><br><span class="line"><span class="params">        device: torch.device, <span class="comment">#设备类型</span></span></span><br><span class="line"><span class="params">        max_norm: <span class="built_in">float</span> = <span class="number">1.0</span>, <span class="comment">#最大范数，用于梯度裁剪</span></span></span><br><span class="line"><span class="params">        update_weights_every: <span class="built_in">int</span> = <span class="number">1</span>, <span class="comment">#更新任务权重的频率</span></span></span><br><span class="line"><span class="params">        optim_niter=<span class="number">20</span>, <span class="comment">#优化器的迭代次数</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(NashMTL, <span class="variable language_">self</span>).__init__(</span><br><span class="line">            n_tasks=n_tasks,</span><br><span class="line">            device=device,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这是规范的Py成员变量的定义方法（不需要显示声明，直接在__init__中用self.variable赋值即可）</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.optim_niter = optim_niter</span><br><span class="line">        <span class="variable language_">self</span>.update_weights_every = update_weights_every</span><br><span class="line">        <span class="variable language_">self</span>.max_norm = max_norm</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.prvs_alpha_param = <span class="literal">None</span> <span class="comment"># 上次更新的权重参数，赋值为None供后续使用</span></span><br><span class="line">        <span class="variable language_">self</span>.normalization_factor = np.ones((<span class="number">1</span>,)) <span class="comment"># 单一元素为1的素质，标准化处理用</span></span><br><span class="line">        <span class="variable language_">self</span>.init_gtg = <span class="variable language_">self</span>.init_gtg = np.eye(<span class="variable language_">self</span>.n_tasks) <span class="comment"># 对应后面的GTG，这里初始化为了单位矩阵</span></span><br><span class="line">        <span class="variable language_">self</span>.step = <span class="number">0.0</span></span><br><span class="line">        <span class="variable language_">self</span>.prvs_alpha = np.ones(<span class="variable language_">self</span>.n_tasks, dtype=np.float32) <span class="comment"># 上次更新的权重，初始化为全1的数组，初始权重相同</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>此方法用于判断是否满足停止条件。具体而言，它检查以下几种情况：</p>
<ol>
<li>alpha_param 为 None。</li>
<li>当前的 gtg 与 alpha_t 计算结果的范数小于某个阈值。</li>
<li>当前的 alpha_param 与 prvs_alpha_param 之间的范数小于某个阈值。</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_stop_criteria</span>(<span class="params">self, gtg, alpha_t</span>):</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        (<span class="variable language_">self</span>.alpha_param.value <span class="keyword">is</span> <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">or</span> (np.linalg.norm(gtg @ alpha_t - <span class="number">1</span> / (alpha_t + <span class="number">1e-10</span>)) &lt; <span class="number">1e-3</span>)</span><br><span class="line">        <span class="keyword">or</span> (</span><br><span class="line">            np.linalg.norm(<span class="variable language_">self</span>.alpha_param.value - <span class="variable language_">self</span>.prvs_alpha_param.value)</span><br><span class="line">            &lt; <span class="number">1e-6</span></span><br><span class="line">        )</span><br><span class="line">    )  </span><br></pre></td></tr></table></figure>

<p>这个方法是优化的核心，目标是求解 alpha_t，即各个任务的权重。优化过程使用了一个迭代的方法，每次通过求解优化问题来更新 alpha_t，直到满足停止条件为止。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">solve_optimization</span>(<span class="params">self, gtg: np.array</span>):</span><br><span class="line">    <span class="variable language_">self</span>.G_param.value = gtg <span class="comment"># </span></span><br><span class="line">    <span class="variable language_">self</span>.normalization_factor_param.value = <span class="variable language_">self</span>.normalization_factor <span class="comment"># 对应使用的标准化因子，方便计算机处理</span></span><br><span class="line"></span><br><span class="line">    alpha_t = <span class="variable language_">self</span>.prvs_alpha</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里使用了迭代求解优化，warm_start=True让求解器利用前一次迭代的解来加速收敛，是一种求解器的使用方法，区别于论文中CCP方法迭代使用alpha，CCP的迭代是接触prvs_*参数来完成的</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.optim_niter):</span><br><span class="line">        <span class="variable language_">self</span>.alpha_param.value = alpha_t</span><br><span class="line">        <span class="variable language_">self</span>.prvs_alpha_param.value = alpha_t</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>: <span class="comment"># 前面在_init_optim_problem中定义了优化问题，这里进行求解</span></span><br><span class="line">            <span class="variable language_">self</span>.prob.solve(solver=cp.ECOS, warm_start=<span class="literal">True</span>, max_iters=<span class="number">100</span>)</span><br><span class="line">        <span class="keyword">except</span>: <span class="comment"># 求解器更新参数发生溢出时的异常处理</span></span><br><span class="line">            <span class="variable language_">self</span>.alpha_param.value = <span class="variable language_">self</span>.prvs_alpha_param.value</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>._stop_criteria(gtg, alpha_t):</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        alpha_t = <span class="variable language_">self</span>.alpha_param.value</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> alpha_t <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="variable language_">self</span>.prvs_alpha = alpha_t</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.prvs_alpha</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>此方法计算线性化的 phi_alpha，用于优化问题的目标函数中。具体的计算过程涉及到 G_param 和 alpha_param 的线性运算。</p>
<p>这里 prvs_phi_tag 对应的是在点 $\alpha^{(\tau)}$<br>  处的梯度 $\nabla \phi(\alpha^{(\tau)})$<br> ，其中：</p>
<p>$\frac{1}{\alpha^{(\tau)}}$是对 $\log(\alpha_i)$的导数</p>
<p>$\frac{1}{G \alpha^{(\tau)}} G$是对 $\log(\beta_i)$的导数</p>
<p>实际上在这里$\tilde{\phi_\tau}(\alpha) &#x3D; \phi(\alpha^{(\tau)})+\nabla \phi(\alpha^{(\tau)})^T(\alpha-\alpha^{(\tau)})$，只考虑了$\nabla \phi(\alpha^{(\tau)})^T(\alpha-\alpha^{(\tau)})$，因为前面是一个常数项，不影响优化的结果。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_calc_phi_alpha_linearization</span>(<span class="params">self</span>):</span><br><span class="line">    G_prvs_alpha = <span class="variable language_">self</span>.G_param @ <span class="variable language_">self</span>.prvs_alpha_param</span><br><span class="line">    prvs_phi_tag = <span class="number">1</span> / <span class="variable language_">self</span>.prvs_alpha_param + (<span class="number">1</span> / G_prvs_alpha) @ <span class="variable language_">self</span>.G_param</span><br><span class="line">    phi_alpha = prvs_phi_tag @ (<span class="variable language_">self</span>.alpha_param - <span class="variable language_">self</span>.prvs_alpha_param)</span><br><span class="line">    <span class="keyword">return</span> phi_alpha</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>此方法初始化优化问题，具体地说，使用 cvxpy 来定义一个凸优化问题。优化目标是最小化一个由加权任务损失和线性化项构成的目标函数。</p>
<p>参考论文中的公式：</p>
<p>$G^TG\alpha &#x3D; 1&#x2F;\alpha$</p>
<p>$\beta_i(\alpha) &#x3D; g_i^TG\alpha$</p>
<p>$\phi_i(\alpha) &#x3D; log(\alpha_i) + log(\beta_i),\phi(\alpha) &#x3D; \sum_i \phi_i(\alpha)$</p>
<p>$min_\alpha \sum_i\beta_i(\alpha) + \phi(\alpha),s.t. \forall i,-\phi_i(\alpha)&lt;0,\alpha_i&gt;0$</p>
<p>$\tilde{\phi_\tau}(\alpha) &#x3D; \phi(\alpha^{(\tau)})+\nabla \phi(\alpha^{(\tau)})^T(\alpha-\alpha^{(\tau)})$</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_init_optim_problem</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment"># 创建一个形状为 (self.n_tasks,) 的非负变量 alpha_param，代表各任务的权重。</span></span><br><span class="line">    <span class="variable language_">self</span>.alpha_param = cp.Variable(shape=(<span class="variable language_">self</span>.n_tasks,), nonneg=<span class="literal">True</span>)</span><br><span class="line">    <span class="variable language_">self</span>.prvs_alpha_param = cp.Parameter(</span><br><span class="line">        shape=(<span class="variable language_">self</span>.n_tasks,), value=<span class="variable language_">self</span>.prvs_alpha</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 创建一个形状为 (self.n_tasks, self.n_tasks) 的参数 G_param，初始化为梯度协方差矩阵 self.init_gtg。</span></span><br><span class="line">    <span class="variable language_">self</span>.G_param = cp.Parameter(</span><br><span class="line">        shape=(<span class="variable language_">self</span>.n_tasks, <span class="variable language_">self</span>.n_tasks), value=<span class="variable language_">self</span>.init_gtg</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 标准化用，但是从数学意义上来说没有什么意义，或许与计算机的数据处理有关（例如增强代码的鲁棒性、稳定性和可维护性...）</span></span><br><span class="line">    <span class="variable language_">self</span>.normalization_factor_param = cp.Parameter(</span><br><span class="line">        shape=(<span class="number">1</span>,), value=np.array([<span class="number">1.0</span>])</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 线性化项</span></span><br><span class="line">    <span class="variable language_">self</span>.phi_alpha = <span class="variable language_">self</span>._calc_phi_alpha_linearization()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个列向量对应论文中的βi，这里将其组成的矩阵，后续使用sum对应就是目标中的所有βi求和</span></span><br><span class="line">    G_alpha = <span class="variable language_">self</span>.G_param @ <span class="variable language_">self</span>.alpha_param <span class="comment"># gtg alpha</span></span><br><span class="line">    constraint = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.n_tasks):</span><br><span class="line">        constraint.append(</span><br><span class="line">            -cp.log(<span class="variable language_">self</span>.alpha_param[i] * <span class="variable language_">self</span>.normalization_factor_param)</span><br><span class="line">            - cp.log(G_alpha[i])</span><br><span class="line">            &lt;= <span class="number">0</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 优化目标</span></span><br><span class="line">    obj = cp.Minimize(</span><br><span class="line">        cp.<span class="built_in">sum</span>(G_alpha) + <span class="variable language_">self</span>.phi_alpha / <span class="variable language_">self</span>.normalization_factor_param</span><br><span class="line">    )</span><br><span class="line">    <span class="variable language_">self</span>.prob = cp.Problem(obj, constraint)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>此方法计算加权损失。首先计算每个任务的梯度（即 grads），然后计算梯度的协方差矩阵 GTG。使用 solve_optimization 方法求解最优的任务权重 alpha。最终返回加权损失和任务权重。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_weighted_loss</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    losses,</span></span><br><span class="line"><span class="params">    shared_parameters,</span></span><br><span class="line"><span class="params">    **kwargs,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    losses :</span></span><br><span class="line"><span class="string">    shared_parameters : shared parameters</span></span><br><span class="line"><span class="string">    kwargs :</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    extra_outputs = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.step == <span class="number">0</span>: <span class="comment"># 步数为0，初始化优化任务</span></span><br><span class="line">        <span class="variable language_">self</span>._init_optim_problem()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="variable language_">self</span>.step % <span class="variable language_">self</span>.update_weights_every) == <span class="number">0</span>: <span class="comment"># 记录更新步数</span></span><br><span class="line">        <span class="variable language_">self</span>.step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#  对每个任务的损失计算相对于共享参数的梯度，并将梯度展平成一维向量后存储在 grads 字典中。</span></span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i, loss <span class="keyword">in</span> <span class="built_in">enumerate</span>(losses):</span><br><span class="line">            g = <span class="built_in">list</span>(</span><br><span class="line">                torch.autograd.grad(</span><br><span class="line">                    loss,</span><br><span class="line">                    shared_parameters,</span><br><span class="line">                    retain_graph=<span class="literal">True</span>,</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">            grad = torch.cat([torch.flatten(grad) <span class="keyword">for</span> grad <span class="keyword">in</span> g])</span><br><span class="line">            grads[i] = grad</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度矩阵 G: 将所有任务的梯度堆叠成一个矩阵。</span></span><br><span class="line">        G = torch.stack(<span class="built_in">tuple</span>(v <span class="keyword">for</span> v <span class="keyword">in</span> grads.values()))</span><br><span class="line">        <span class="comment"># 梯度协方差矩阵 GTG: 计算梯度矩阵的自乘，得到梯度协方差矩阵。</span></span><br><span class="line">        GTG = torch.mm(G, G.t())</span><br><span class="line">        <span class="comment"># 标准化因子: 计算 GTG 的范数，并用其标准化 GTG。</span></span><br><span class="line">        <span class="variable language_">self</span>.normalization_factor = (</span><br><span class="line">            torch.norm(GTG).detach().cpu().numpy().reshape((<span class="number">1</span>,))</span><br><span class="line">        )</span><br><span class="line">        GTG = GTG / <span class="variable language_">self</span>.normalization_factor.item()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 采用优化的方法，在GTG的基础上求解各个损失函数的权重alpha</span></span><br><span class="line">        alpha = <span class="variable language_">self</span>.solve_optimization(GTG.cpu().detach().numpy())</span><br><span class="line">        alpha = torch.from_numpy(alpha)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="variable language_">self</span>.step += <span class="number">1</span></span><br><span class="line">        alpha = <span class="variable language_">self</span>.prvs_alpha</span><br><span class="line">    <span class="comment"># 这里我猜测是作者考虑到为了加快模型的训练可以，简单地通过在某些步数的时候，不按照正常的流程求权重，而是直接使用上次的权重；但是实际上没有这样做，因为update_weights_every是被设置为1的，无论step为多少，总是会按正常步骤更新alpha</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按照得到的权重alpha计算加权损失函数，供backward()使用</span></span><br><span class="line">    weighted_loss = <span class="built_in">sum</span>([losses[i] * alpha[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(alpha))])</span><br><span class="line">    extra_outputs[<span class="string">&quot;weights&quot;</span>] = alpha</span><br><span class="line">    <span class="keyword">return</span> weighted_loss, extra_outputs</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        losses: torch.Tensor,</span></span><br><span class="line"><span class="params">        shared_parameters: <span class="type">Union</span>[</span></span><br><span class="line"><span class="params">            <span class="type">List</span>[torch.nn.parameter.Parameter], torch.Tensor</span></span><br><span class="line"><span class="params">        ] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        task_specific_parameters: <span class="type">Union</span>[</span></span><br><span class="line"><span class="params">            <span class="type">List</span>[torch.nn.parameter.Parameter], torch.Tensor</span></span><br><span class="line"><span class="params">        ] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        last_shared_parameters: <span class="type">Union</span>[</span></span><br><span class="line"><span class="params">            <span class="type">List</span>[torch.nn.parameter.Parameter], torch.Tensor</span></span><br><span class="line"><span class="params">        ] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        representation: <span class="type">Union</span>[<span class="type">List</span>[torch.nn.parameter.Parameter], torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        **kwargs,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">Tuple</span>[<span class="type">Union</span>[torch.Tensor, <span class="literal">None</span>], <span class="type">Union</span>[<span class="type">Dict</span>, <span class="literal">None</span>]]:</span><br><span class="line">        loss, extra_outputs = <span class="variable language_">self</span>.get_weighted_loss(</span><br><span class="line">            losses=losses, <span class="comment"># 当前各损失函数</span></span><br><span class="line">            shared_parameters=shared_parameters, <span class="comment"># 确定待求梯度的损失函数的变量</span></span><br><span class="line">            **kwargs,</span><br><span class="line">        ) <span class="comment"># 得到加权后的损失函数，额外参数是各损失函数的权重信息</span></span><br><span class="line">        loss.backward() <span class="comment"># 对加权损失函数求梯度，从而确定更新方向，以供优化器更新参数使用（optimize.step()）</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># make sure the solution for shared params has norm &lt;= self.eps</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.max_norm &gt; <span class="number">0</span>:</span><br><span class="line">            torch.nn.utils.clip_grad_norm_(shared_parameters, <span class="variable language_">self</span>.max_norm)</span><br><span class="line"></span><br><span class="line"><span class="comment">#在训练过程中，backward 方法用于执行反向传播。它调用 get_weighted_loss 来计算加权损失，并使用 loss.backward() 进行梯度计算。如果 max_norm 大于 0，则会对梯度进行裁剪。</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, extra_outputs</span><br></pre></td></tr></table></figure>

<h3 id="result"><a href="#result" class="headerlink" title="result"></a>result</h3><p><img src="/../_images/result.png" alt="Nash-MTL" title="Nash-MTL"></p>
<h3 id="My-Test"><a href="#My-Test" class="headerlink" title="My_Test"></a>My_Test</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleLinearModel</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, output_dim</span>):</span><br><span class="line">        <span class="variable language_">self</span>.W = np.random.randn(input_dim, output_dim)</span><br><span class="line">        <span class="variable language_">self</span>.b = np.random.randn(output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> X @ <span class="variable language_">self</span>.W + <span class="variable language_">self</span>.b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">task_loss</span>(<span class="params">model, X, y_true</span>):</span><br><span class="line">    y_pred = model.predict(X)</span><br><span class="line">    <span class="keyword">return</span> np.mean((y_pred - y_true) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义两个任务的数据，这里作为简单的测试，我使用的随机生成</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X = np.random.randn(<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">y_A = X @ np.array([<span class="number">2</span>, -<span class="number">3</span>]) + <span class="number">1</span> + <span class="number">0.1</span> * np.random.randn(<span class="number">100</span>)</span><br><span class="line">y_B = X @ np.array([-<span class="number">1</span>, <span class="number">4</span>]) - <span class="number">2</span> + <span class="number">0.1</span> * np.random.randn(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型</span></span><br><span class="line">model = SimpleLinearModel(input_dim=<span class="number">2</span>, output_dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算每个任务的梯度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradients</span>(<span class="params">model, X, y_A, y_B</span>):</span><br><span class="line">    epsilon = <span class="number">1e-5</span></span><br><span class="line">    initial_loss_A = task_loss(model, X, y_A)</span><br><span class="line">    initial_loss_B = task_loss(model, X, y_B)</span><br><span class="line"></span><br><span class="line">    grad_W_A = np.zeros_like(model.W)</span><br><span class="line">    grad_b_A = np.zeros_like(model.b)</span><br><span class="line">    grad_W_B = np.zeros_like(model.W)</span><br><span class="line">    grad_b_B = np.zeros_like(model.b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(model.W.size):</span><br><span class="line">        model.W.flat[i] += epsilon</span><br><span class="line">        grad_W_A.flat[i] = (task_loss(model, X, y_A) - initial_loss_A) / epsilon</span><br><span class="line">        grad_W_B.flat[i] = (task_loss(model, X, y_B) - initial_loss_B) / epsilon</span><br><span class="line">        model.W.flat[i] -= epsilon</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(model.b.size):</span><br><span class="line">        model.b.flat[i] += epsilon</span><br><span class="line">        grad_b_A.flat[i] = (task_loss(model, X, y_A) - initial_loss_A) / epsilon</span><br><span class="line">        grad_b_B.flat[i] = (task_loss(model, X, y_B) - initial_loss_B) / epsilon</span><br><span class="line">        model.b.flat[i] -= epsilon</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (grad_W_A, grad_b_A), (grad_W_B, grad_b_B)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算Nash Bargaining Solution</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nash_bargaining_solution</span>(<span class="params">grad_A, grad_B</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">objective</span>(<span class="params">x</span>):</span><br><span class="line">        grad_W = x[<span class="number">0</span>] * grad_A[<span class="number">0</span>] + x[<span class="number">1</span>] * grad_B[<span class="number">0</span>]</span><br><span class="line">        grad_b = x[<span class="number">0</span>] * grad_A[<span class="number">1</span>] + x[<span class="number">1</span>] * grad_B[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> - np.<span class="built_in">sum</span>(grad_W ** <span class="number">2</span> + grad_b ** <span class="number">2</span>)  <span class="comment"># Minimize the negative of the combined gradient norms</span></span><br><span class="line"></span><br><span class="line">    constraints = &#123;</span><br><span class="line">        <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;eq&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;fun&#x27;</span>: <span class="keyword">lambda</span> x: np.<span class="built_in">sum</span>(x) - <span class="number">1</span>  <span class="comment"># x_A + x_B = 1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    bounds = [(<span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">1</span>)]</span><br><span class="line">    x0 = [<span class="number">0.5</span>, <span class="number">0.5</span>]</span><br><span class="line"></span><br><span class="line">    result = minimize(objective, x0, bounds=bounds, constraints=constraints)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> result.success:</span><br><span class="line">        <span class="keyword">return</span> result.x</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Optimization failed&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行一次更新</span></span><br><span class="line">(grad_W_A, grad_b_A), (grad_W_B, grad_b_B) = compute_gradients(model, X, y_A, y_B)</span><br><span class="line">nash_weights = nash_bargaining_solution((grad_W_A, grad_b_A), (grad_W_B, grad_b_B))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新模型参数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">model.W -= learning_rate * (nash_weights[<span class="number">0</span>] * grad_W_A + nash_weights[<span class="number">1</span>] * grad_W_B)</span><br><span class="line">model.b -= learning_rate * (nash_weights[<span class="number">0</span>] * grad_b_A + nash_weights[<span class="number">1</span>] * grad_b_B)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Updated W: <span class="subst">&#123;model.W&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Updated b: <span class="subst">&#123;model.b&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Experiment-quantum-chemistry"><a href="#Experiment-quantum-chemistry" class="headerlink" title="Experiment: quantum_chemistry"></a>Experiment: quantum_chemistry</h2><p>整个实验是基于QM9数据集中的原子-分子数据，训练了一个神经网络，用来根据分子的特征（原子个数、原子特征、原子之间的化学键连接情况、化学键的性质特征…），来对分子的性质进行一个预测。</p>
<h3 id="QM9"><a href="#QM9" class="headerlink" title="QM9"></a>QM9</h3><p>QM9数据集：一个广泛使用的化学分子数据集，主要用于机器学习和深度学习领域的化学和材料科学研究。它包含了约 134,000 个小分子的详细信息，每个分子包含的属性和标签数量丰富，是研究分子性质和行为的宝贵资源。</p>
<p>实验中的数据使用PyTorch Geometric框架来对原始数据进行预处理，转换为图格式的数据，得到数据集供我们使用。</p>
<h4 id="数据对象"><a href="#数据对象" class="headerlink" title="数据对象"></a>数据对象</h4><p>在 PyTorch Geometric 中，每个数据对象是 torch_geometric.data.Data 类型的实例</p>
<p>包含以下属性：</p>
<ol>
<li>x：节点特征矩阵（形状为 [num_nodes, num_node_features]）。</li>
<li>edge_index：边索引矩阵（形状为 [2, num_edges]），其中每列表示一条边的起始节点和终止节点。</li>
<li>edge_attr：边特征矩阵（形状为 [num_edges, num_edge_features]，可选）。</li>
<li>y：目标属性或标签（形状为 [num_targets]，可选）。</li>
<li>pos：节点的三维坐标（形状为 [num_nodes, 3]，可选）。</li>
<li>z：节点的原子序数（形状为 [num_nodes]）。</li>
<li>idx：分子索引。</li>
<li>name：分子名称（数据集中的标识）。</li>
</ol>
<p><strong>test的输出信息：Data(x&#x3D;[5, 11], edge_index&#x3D;[2, 8], edge_attr&#x3D;[8, 4], y&#x3D;[1, 11], pos&#x3D;[5, 3], idx&#x3D;[1], name&#x3D;’gdb_1’, z&#x3D;[5])</strong></p>
<h3 id="train-py-1"><a href="#train-py-1" class="headerlink" title="train.py"></a>train.py</h3><h4 id="main"><a href="#main" class="headerlink" title="main"></a>main</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params"></span></span><br><span class="line"><span class="params">    data_path: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">    batch_size: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    device: torch.device,</span></span><br><span class="line"><span class="params">    method: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">    weight_method_params: <span class="built_in">dict</span>,</span></span><br><span class="line"><span class="params">    lr: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">    method_params_lr: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">    n_epochs: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    targets: <span class="built_in">list</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    scale_target: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    main_task: <span class="built_in">int</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="comment"># 模型初始化，训练的是一个网络，其中任务的数量n_tasks对应输出头的个数；num_features是输入数据的特征维度，这里设置为11对应quantum_chemistry数据集中一个分子的相关性质的量化指标有11个；dim确定了隐层的维度；（相关细节在model.py）</span></span><br><span class="line">    <span class="comment"># to(device)可以将模型移动到指定的计算设备，保存相关数据并执行计算操作</span></span><br><span class="line">    dim = <span class="number">64</span></span><br><span class="line">    model = Net(n_tasks=<span class="built_in">len</span>(targets), num_features=<span class="number">11</span>, dim=dim).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据转换和加载（相关细节在utils.py）</span></span><br><span class="line">    transform = T.Compose([MyTransform(targets), Complete(), T.Distance(norm=<span class="literal">False</span>)])</span><br><span class="line">    dataset = QM9(data_path, transform=transform).shuffle()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 拆分数据集</span></span><br><span class="line">    test_dataset = dataset[:<span class="number">10000</span>]</span><br><span class="line">    val_dataset = dataset[<span class="number">10000</span>:<span class="number">20000</span>]</span><br><span class="line">    train_dataset = dataset[<span class="number">20000</span>:]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果需要，进行目标值的归一化</span></span><br><span class="line">    std = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> scale_target:</span><br><span class="line">        mean = train_dataset.data.y[:, targets].mean(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = train_dataset.data.y[:, targets].std(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        dataset.data.y[:, targets] = (dataset.data.y[:, targets] - mean) / std</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建数据加载器</span></span><br><span class="line">    test_loader = DataLoader(</span><br><span class="line">        test_dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line">    val_loader = DataLoader(</span><br><span class="line">        val_dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line">    train_loader = DataLoader(</span><br><span class="line">        train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化多任务学习的权重方法</span></span><br><span class="line">    weight_method = WeightMethods(</span><br><span class="line">        method,</span><br><span class="line">        n_tasks=<span class="built_in">len</span>(targets),</span><br><span class="line">        device=device,</span><br><span class="line">        **weight_method_params[method],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置优化器，并为模型和权重方法参数设置不同的学习率</span></span><br><span class="line">    optimizer = torch.optim.Adam(</span><br><span class="line">        [</span><br><span class="line">            <span class="built_in">dict</span>(params=model.parameters(), lr=lr),</span><br><span class="line">            <span class="built_in">dict</span>(params=weight_method.parameters(), lr=method_params_lr),</span><br><span class="line">        ],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习率调度器</span></span><br><span class="line">    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(</span><br><span class="line">        optimizer, mode=<span class="string">&quot;min&quot;</span>, factor=<span class="number">0.7</span>, patience=<span class="number">5</span>, min_lr=<span class="number">0.00001</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练循环</span></span><br><span class="line">    epoch_iterator = trange(n_epochs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用于保存最佳验证损失、最佳测试损失及其变化量的变量，初始值设为正无穷大。</span></span><br><span class="line">    best_val = np.inf</span><br><span class="line">    best_test = np.inf</span><br><span class="line">    best_test_delta = np.inf</span><br><span class="line">    best_val_delta = np.inf</span><br><span class="line">    <span class="comment"># 用于保存最佳测试结果的变量</span></span><br><span class="line">    best_test_results = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> epoch_iterator:</span><br><span class="line">        lr = scheduler.optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>] <span class="comment"># 从学习率调度器（scheduler）中获取当前学习率</span></span><br><span class="line">        <span class="keyword">for</span> j, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):  <span class="comment"># 遍历训练数据加载器（train_loader）</span></span><br><span class="line">            model.train() </span><br><span class="line"></span><br><span class="line">            data = data.to(device)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">            out, features = model(data, return_representation=<span class="literal">True</span>) <span class="comment"># 模型前向传播，进行估计</span></span><br><span class="line"></span><br><span class="line">            losses = F.mse_loss(out, data.y, reduction=<span class="string">&quot;none&quot;</span>).mean(<span class="number">0</span>) <span class="comment"># 估计结果与真实结果的平方误差作为损失函数</span></span><br><span class="line"></span><br><span class="line">            loss, extra_outputs = weight_method.backward(</span><br><span class="line">                losses=losses,</span><br><span class="line">                shared_parameters=<span class="built_in">list</span>(model.shared_parameters()),</span><br><span class="line">                task_specific_parameters=<span class="built_in">list</span>(model.task_specific_parameters()),</span><br><span class="line">                last_shared_parameters=<span class="built_in">list</span>(model.last_shared_parameters()),</span><br><span class="line">                representation=features,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 在验证集和测试集上评估模型（细节在def evaluate中）</span></span><br><span class="line">        val_loss_dict = evaluate(model, val_loader, std=std, scale_target=scale_target)</span><br><span class="line">        test_loss_dict = evaluate(</span><br><span class="line">            model, test_loader, std=std, scale_target=scale_target</span><br><span class="line">        )</span><br><span class="line">        val_loss = val_loss_dict[<span class="string">&quot;avg_loss&quot;</span>]</span><br><span class="line">        val_delta = val_loss_dict[<span class="string">&quot;delta_m&quot;</span>]</span><br><span class="line">        test_loss = test_loss_dict[<span class="string">&quot;avg_loss&quot;</span>]</span><br><span class="line">        test_delta = test_loss_dict[<span class="string">&quot;delta_m&quot;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据验证集性能更新最佳结果</span></span><br><span class="line">        <span class="keyword">if</span> method == <span class="string">&quot;stl&quot;</span>: <span class="comment"># 单任务学习</span></span><br><span class="line">            best_val_criteria = val_loss_dict[<span class="string">&quot;avg_task_losses&quot;</span>][main_task] &lt;= best_val</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 多任务学习</span></span><br><span class="line">            best_val_criteria = val_delta &lt;= best_val_delta <span class="comment"># 判断是否需要更新最佳结果，验证集上delta_m是否小于最好的delta_m</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> best_val_criteria:</span><br><span class="line">            best_val = val_loss</span><br><span class="line">            best_test = test_loss</span><br><span class="line">            best_test_results = test_loss_dict</span><br><span class="line">            best_val_delta = val_delta</span><br><span class="line">            best_test_delta = test_delta</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每个epoch记录日志</span></span><br><span class="line">        epoch_iterator.set_description(</span><br><span class="line">            <span class="string">f&quot;epoch <span class="subst">&#123;epoch&#125;</span> | lr=<span class="subst">&#123;lr&#125;</span> | train loss <span class="subst">&#123;losses.mean().item():<span class="number">.3</span>f&#125;</span> | val loss: <span class="subst">&#123;val_loss:<span class="number">.3</span>f&#125;</span> | &quot;</span></span><br><span class="line">            <span class="string">f&quot;test loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | best test loss <span class="subst">&#123;best_test:<span class="number">.3</span>f&#125;</span> | best_test_delta <span class="subst">&#123;best_test_delta:<span class="number">.3</span>f&#125;</span>&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> wandb.run <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            wandb.log(&#123;<span class="string">&quot;Learning Rate&quot;</span>: lr&#125;, step=epoch)</span><br><span class="line">            wandb.log(&#123;<span class="string">&quot;Train Loss&quot;</span>: losses.mean().item()&#125;, step=epoch)</span><br><span class="line">            wandb.log(&#123;<span class="string">&quot;Val Loss&quot;</span>: val_loss&#125;, step=epoch)</span><br><span class="line">            wandb.log(&#123;<span class="string">&quot;Val Delta&quot;</span>: val_delta&#125;, step=epoch)</span><br><span class="line">            wandb.log(&#123;<span class="string">&quot;Test Loss&quot;</span>: test_loss&#125;, step=epoch)</span><br><span class="line">            wandb.log(&#123;<span class="string">&quot;Test Delta&quot;</span>: test_delta&#125;, step=epoch)</span><br><span class="line">            wandb.log(&#123;<span class="string">&quot;Best Test Loss&quot;</span>: best_test&#125;, step=epoch)</span><br><span class="line">            wandb.log(&#123;<span class="string">&quot;Best Test Delta&quot;</span>: best_test_delta&#125;, step=epoch)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据验证集性能调整学习率</span></span><br><span class="line">        scheduler.step(</span><br><span class="line">            val_loss_dict[<span class="string">&quot;avg_task_losses&quot;</span>][main_task]</span><br><span class="line">            <span class="keyword">if</span> method == <span class="string">&quot;stl&quot;</span></span><br><span class="line">            <span class="keyword">else</span> val_delta</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 脚本入口点</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 参数解析器设置</span></span><br><span class="line">    parser = ArgumentParser(<span class="string">&quot;QM9&quot;</span>, parents=[common_parser])</span><br><span class="line">    parser.set_defaults(</span><br><span class="line">        data_path=<span class="string">&quot;dataset&quot;</span>,</span><br><span class="line">        lr=<span class="number">1e-3</span>,</span><br><span class="line">        n_epochs=<span class="number">300</span>,</span><br><span class="line">        batch_size=<span class="number">120</span>,</span><br><span class="line">        method=<span class="string">&quot;nashmtl&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--scale-y&quot;</span>, default=<span class="literal">True</span>, <span class="built_in">type</span>=str2bool)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--wandb_project&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="literal">None</span>, <span class="built_in">help</span>=<span class="string">&quot;Name of Weights &amp; Biases Project.&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--wandb_entity&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="literal">None</span>, <span class="built_in">help</span>=<span class="string">&quot;Name of Weights &amp; Biases Entity.&quot;</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置随机种子以确保可重复性</span></span><br><span class="line">    set_seed(args.seed)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果指定了项目，则初始化Weights &amp; Biases</span></span><br><span class="line">    <span class="keyword">if</span> args.wandb_project <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        wandb.init(project=args.wandb_project, entity=args.wandb_entity, config=args)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取权重方法的参数</span></span><br><span class="line">    weight_method_params = extract_weight_method_parameters_from_args(args)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取计算设备（CPU或GPU）</span></span><br><span class="line">    device = get_device(gpus=args.gpu)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 运行主训练和评估函数</span></span><br><span class="line">    main(</span><br><span class="line">        data_path=args.data_path,</span><br><span class="line">        batch_size=args.batch_size,</span><br><span class="line">        device=device,</span><br><span class="line">        method=args.method,</span><br><span class="line">        weight_method_params=weight_method_params,</span><br><span class="line">        lr=args.lr,</span><br><span class="line">        method_params_lr=args.method_params_lr,</span><br><span class="line">        n_epochs=args.n_epochs,</span><br><span class="line">        targets=targets,</span><br><span class="line">        scale_target=args.scale_y,</span><br><span class="line">        main_task=args.main_task,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 完成Weights &amp; Biases运行</span></span><br><span class="line">    <span class="keyword">if</span> wandb.run <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        wandb.finish()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="evaluate"><a href="#evaluate" class="headerlink" title="evaluate"></a>evaluate</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">model, loader, std, scale_target</span>): <span class="comment"># 评估模型；数据加载其；标准差；bool值</span></span><br><span class="line">    model.<span class="built_in">eval</span>() <span class="comment"># 模型切换为评估模式</span></span><br><span class="line">    data_size = <span class="number">0.0</span></span><br><span class="line">    task_losses = <span class="number">0.0</span> <span class="comment"># 用来累计各个任务的总损失</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):</span><br><span class="line">        data = data.to(device)</span><br><span class="line">        out = model(data)</span><br><span class="line">        <span class="keyword">if</span> scale_target:</span><br><span class="line">            task_losses += F.l1_loss(</span><br><span class="line">                out * std.to(device), data.y * std.to(device), reduction=<span class="string">&quot;none&quot;</span></span><br><span class="line">            ).<span class="built_in">sum</span>(</span><br><span class="line">                <span class="number">0</span></span><br><span class="line">            )  <span class="comment"># 平均绝对误差（MAE），考虑了先按标准差进行放缩</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            task_losses += F.l1_loss(out, data.y, reduction=<span class="string">&quot;none&quot;</span>).<span class="built_in">sum</span>(<span class="number">0</span>)  <span class="comment"># 平均绝对误差（MAE）</span></span><br><span class="line">        data_size += <span class="built_in">len</span>(data.y)</span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    avg_task_losses = task_losses / data_size <span class="comment"># 平均任务损失</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将误差从电子伏特（eV）转换为毫电子伏特（meV）</span></span><br><span class="line">    avg_task_losses = avg_task_losses.detach().cpu().numpy()</span><br><span class="line">    avg_task_losses[multiply_indx] *= <span class="number">1000</span> <span class="comment"># multiply_indx定义于utils中，或许是索引了单位是eV的物理化学量</span></span><br><span class="line"></span><br><span class="line">    delta_m = delta_fn(avg_task_losses)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">        avg_loss=avg_task_losses.mean(), <span class="comment"># 所有任务平均损失</span></span><br><span class="line">        avg_task_losses=avg_task_losses, <span class="comment"># 每个任务的平均损失</span></span><br><span class="line">        delta_m=delta_m, <span class="comment"># MTL模型在各个任务上的平均相对误差百分比，基于STL模型的基准表现。这个百分比表示MTL模型相较于STL基准的整体性能提升或退步。</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<h3 id="model-py"><a href="#model-py" class="headerlink" title="model.py"></a>model.py</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 这个类定义了一个用于量子化学任务的神经网络模型，并使用了图神经网络（GNN）的相关模块来处理图结构数据。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_tasks, num_features=<span class="number">11</span>, dim=<span class="number">64</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.n_tasks = n_tasks</span><br><span class="line">        <span class="variable language_">self</span>.dim = dim</span><br><span class="line">        <span class="variable language_">self</span>.lin0 = torch.nn.Linear(num_features, dim) <span class="comment"># 初始化了一个线性层，用来把输入数据维度从num_features转换为dim</span></span><br><span class="line"></span><br><span class="line">        nn = Sequential(Linear(<span class="number">5</span>, <span class="number">128</span>), ReLU(), Linear(<span class="number">128</span>, dim * dim)) <span class="comment"># 序列模块，用来生成卷积核的权重（使用到一个有序容器，其中包含两个线性层和一个ReLU层）</span></span><br><span class="line">        <span class="comment"># 在 GNN 中，卷积核用于在图结构上进行卷积操作。它们会在图的每个节点及其邻居节点之间滑动，聚合和更新节点特征。卷积核在 GNN 中的作用类似于在图像处理中提取局部特征，但其操作对象是节点及其邻居。其形状可以是多维的，如d×d，其中d是节点特征的维度。</span></span><br><span class="line">        <span class="comment"># 卷积核的权重（Weights of Convolutional Kernel）是卷积核中的元素值。它们是模型训练过程中需要学习的参数。卷积核的权重直接影响卷积操作的结果，从而影响特征提取的效果。</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.conv = NNConv(dim, dim, nn, aggr=<span class="string">&quot;mean&quot;</span>) <span class="comment"># NNConv卷积层，输入输出维度都是dim；使用nn生成的权重进行卷积操作；使用平均聚合</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># GRU（门控循环单元）处理图卷积后的数据，qa其是一种循环神经网络（RNN），用于处理序列数据。GRU 可以捕捉长程依赖，避免梯度消失问题。</span></span><br><span class="line">        <span class="variable language_">self</span>.gru = GRU(dim, dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set2Set 是一种图池化方法，用于将图节点的表示聚合成固定大小的图表示。</span></span><br><span class="line">        <span class="variable language_">self</span>.set2set = Set2Set(dim, processing_steps=<span class="number">3</span>)</span><br><span class="line">        <span class="comment"># 这个线性层将 Set2Set 的输出转换为 dim 维度。</span></span><br><span class="line">        <span class="variable language_">self</span>.lin1 = torch.nn.Linear(<span class="number">2</span> * dim, dim)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>._init_task_heads() <span class="comment"># 动态创建与任务数量对应的输出层</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 动态创建多个任务头（输出头），每个任务头是一个线性层，将 dim 维度转换为1维输出，对应的是所预测化学分子的相关性质，具体在utils.py中有定义。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_task_heads</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.n_tasks):</span><br><span class="line">            <span class="built_in">setattr</span>(<span class="variable language_">self</span>, <span class="string">f&quot;head_<span class="subst">&#123;i&#125;</span>&quot;</span>, torch.nn.Linear(<span class="variable language_">self</span>.dim, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># task_specific是一个 ModuleList，包含所有任务头</span></span><br><span class="line">        <span class="variable language_">self</span>.task_specific = torch.nn.ModuleList(</span><br><span class="line">            [<span class="built_in">getattr</span>(<span class="variable language_">self</span>, <span class="string">f&quot;head_<span class="subst">&#123;i&#125;</span>&quot;</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.n_tasks)]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播方法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, data, return_representation=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="comment"># 第一层线性变换和激活，输入是结点特征data.x矩阵</span></span><br><span class="line">        out = F.relu(<span class="variable language_">self</span>.lin0(data.x))</span><br><span class="line">        h = out.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 图卷积和GRU层</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            m = F.relu(<span class="variable language_">self</span>.conv(out, data.edge_index, data.edge_attr))</span><br><span class="line">            out, h = <span class="variable language_">self</span>.gru(m.unsqueeze(<span class="number">0</span>), h)</span><br><span class="line">            out = out.squeeze(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># data.edge_index是边索引矩阵（通常是2×edges_num的张量），决定了图中哪些点之间有信息的传递；data.edgea_attr是边特征矩阵（通常是edges_num×edge_features_num的张量），边的特征在信息传递的过程中可能会被使用</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set2Set池化</span></span><br><span class="line">        out = <span class="variable language_">self</span>.set2set(out, data.batch)</span><br><span class="line">        features = F.relu(<span class="variable language_">self</span>.lin1(out))</span><br><span class="line">        logits = torch.cat(</span><br><span class="line">            [<span class="built_in">getattr</span>(<span class="variable language_">self</span>, <span class="string">f&quot;head_<span class="subst">&#123;i&#125;</span>&quot;</span>)(features) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.n_tasks)], dim=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> return_representation:</span><br><span class="line">            <span class="keyword">return</span> logits, features</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回模型中共享层的参数迭代器</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">shared_parameters</span>(<span class="params">self</span>) -&gt; Iterator[torch.nn.parameter.Parameter]:</span><br><span class="line">        <span class="keyword">return</span> chain(</span><br><span class="line">            <span class="variable language_">self</span>.lin0.parameters(),</span><br><span class="line">            <span class="variable language_">self</span>.conv.parameters(),</span><br><span class="line">            <span class="variable language_">self</span>.gru.parameters(),</span><br><span class="line">            <span class="variable language_">self</span>.set2set.parameters(),</span><br><span class="line">            <span class="variable language_">self</span>.lin1.parameters(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回任务头（最后输出的线性层）的参数迭代器</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">task_specific_parameters</span>(<span class="params">self</span>) -&gt; Iterator[torch.nn.parameter.Parameter]:</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.task_specific.parameters()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回最后一层共享层的参数迭代器</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">last_shared_parameters</span>(<span class="params">self</span>) -&gt; Iterator[torch.nn.parameter.Parameter]:</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.lin1.parameters()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="utils-py"><a href="#utils-py" class="headerlink" title="utils.py"></a>utils.py</h3><h4 id="target"><a href="#target" class="headerlink" title="target"></a>target</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">qm9_target_dict = &#123;</span><br><span class="line">    <span class="number">0</span>: <span class="string">&quot;mu&quot;</span>,</span><br><span class="line">    <span class="number">1</span>: <span class="string">&quot;alpha&quot;</span>,</span><br><span class="line">    <span class="number">2</span>: <span class="string">&quot;homo&quot;</span>,</span><br><span class="line">    <span class="number">3</span>: <span class="string">&quot;lumo&quot;</span>,</span><br><span class="line">    <span class="number">5</span>: <span class="string">&quot;r2&quot;</span>,</span><br><span class="line">    <span class="number">6</span>: <span class="string">&quot;zpve&quot;</span>,</span><br><span class="line">    <span class="number">7</span>: <span class="string">&quot;U0&quot;</span>,</span><br><span class="line">    <span class="number">8</span>: <span class="string">&quot;U&quot;</span>,</span><br><span class="line">    <span class="number">9</span>: <span class="string">&quot;H&quot;</span>,</span><br><span class="line">    <span class="number">10</span>: <span class="string">&quot;G&quot;</span>,</span><br><span class="line">    <span class="number">11</span>: <span class="string">&quot;Cv&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># for \Delta_m calculations</span></span><br><span class="line"><span class="comment"># -------------------------</span></span><br><span class="line"><span class="comment"># DimeNet uses the atomization energy for targets U0, U, H, and G.</span></span><br><span class="line">target_idx = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">11</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Report meV instead of eV.</span></span><br><span class="line">multiply_indx = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line"></span><br><span class="line">n_tasks = <span class="built_in">len</span>(target_idx)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="MyTransform"><a href="#MyTransform" class="headerlink" title="MyTransform"></a>MyTransform</h4><p>MyTransform的主要作用是在加载 QM9 数据集时选择并提取感兴趣的目标属性（y）</p>
<p>其用作QM9类，torch_geometric.datasets.QM9(root, transform&#x3D;None, pre_transform&#x3D;None, pre_filter&#x3D;None)，其中的transform参数，应用于每个数据对象的变换函数或变换序列。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyTransform</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, target: <span class="built_in">list</span> = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> target <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            target = torch.tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">11</span>])  <span class="comment"># removing 4</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            target = torch.tensor(target)</span><br><span class="line">        <span class="variable language_">self</span>.target = target</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, data</span>):</span><br><span class="line">        <span class="comment"># Specify target.</span></span><br><span class="line">        data.y = data.y[:, <span class="variable language_">self</span>.target]</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="Compelete"><a href="#Compelete" class="headerlink" title="Compelete"></a>Compelete</h4><p>Complete 类的主要作用是将原始的图数据转换为完全图，以便更好地捕获全局信息和特征。通过添加所有可能的边，并保留原有的边特征，这种转换为某些图神经网络模型的训练和评估提供了便利。</p>
<p>其用作QM9类，torch_geometric.datasets.QM9(root, transform&#x3D;None, pre_transform&#x3D;None, pre_filter&#x3D;None)，其中的pre_transform参数，应用于预处理数据对象的变换函数，仅在数据集下载和处理时运行一次。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Complete</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, data</span>):</span><br><span class="line">        device = data.edge_index.device</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建所有节点的组合（即完整连接），通过将图补全为完全图，每个节点都与其他所有节点相连。这有助于捕获全局的结构信息，特别是当你需要计算节点之间的距离或集成全局特征时。</span></span><br><span class="line">        row = torch.arange(data.num_nodes, dtype=torch.long, device=device)</span><br><span class="line">        col = torch.arange(data.num_nodes, dtype=torch.long, device=device)</span><br><span class="line"></span><br><span class="line">        row = row.view(-<span class="number">1</span>, <span class="number">1</span>).repeat(<span class="number">1</span>, data.num_nodes).view(-<span class="number">1</span>)</span><br><span class="line">        col = col.repeat(data.num_nodes)</span><br><span class="line">        edge_index = torch.stack([row, col], dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        edge_attr = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> data.edge_attr <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 复制现有边特征到新的完全图中，保留原有的边特征，并将其复制到新的完全图中，确保节点之间的连接信息不丢失。</span></span><br><span class="line">            idx = data.edge_index[<span class="number">0</span>] * data.num_nodes + data.edge_index[<span class="number">1</span>]</span><br><span class="line">            size = <span class="built_in">list</span>(data.edge_attr.size())</span><br><span class="line">            size[<span class="number">0</span>] = data.num_nodes * data.num_nodes</span><br><span class="line">            edge_attr = data.edge_attr.new_zeros(size)</span><br><span class="line">            edge_attr[idx] = data.edge_attr</span><br><span class="line">        <span class="comment"># 移除自环，移除自环以减少噪声，有助于某些模型的训练和性能提升。</span></span><br><span class="line">        edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)</span><br><span class="line">        data.edge_attr = edge_attr</span><br><span class="line">        data.edge_index = edge_index</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="delta-fn"><a href="#delta-fn" class="headerlink" title="delta_fn"></a>delta_fn</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># stl results</span></span><br><span class="line">BASE = np.array(</span><br><span class="line">    [</span><br><span class="line">        <span class="number">0.0671</span>,</span><br><span class="line">        <span class="number">0.1814</span>,</span><br><span class="line">        <span class="number">60.576</span>,</span><br><span class="line">        <span class="number">53.915</span>,</span><br><span class="line">        <span class="number">0.5027</span>,</span><br><span class="line">        <span class="number">4.539</span>,</span><br><span class="line">        <span class="number">58.838</span>,</span><br><span class="line">        <span class="number">64.244</span>,</span><br><span class="line">        <span class="number">63.852</span>,</span><br><span class="line">        <span class="number">66.223</span>,</span><br><span class="line">        <span class="number">0.07212</span>,</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">SIGN = np.array([<span class="number">0</span>] * n_tasks)</span><br><span class="line">KK = np.ones(n_tasks) * -<span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">delta_fn</span>(<span class="params">a</span>):</span><br><span class="line">    <span class="keyword">return</span> (KK ** SIGN * (a - BASE) / BASE).mean() * <span class="number">100.0</span>  <span class="comment"># *100 for percentage</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h3><ol>
<li>MR的计算？</li>
<li>训练的结束？（论文中提到了根据验证集的评估情况，提前结束训练）</li>
<li>$\Delta_m$%的含义？（MTL 模型在各个任务上的平均相对误差百分比，基于 STL 模型的基准表现。这个百分比表示 MTL 模型相较于 STL 基准的整体性能提升或退步。）</li>
</ol>

		</div>

		<!-- Comments removed -->
		
	</article>

	<div id="toc">
		
	</div>

</div>

<!-- <div id="paginator"> -->
<!-- 	 -->
<!-- </div> -->
<!-- page.mathjax == true修改为true，默认开启-->

    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            processEscapes: true
          }
        });
      </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
            tex2jax: {
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
          });
      </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
              var all = MathJax.Hub.getAllJax(), i;
              for(i=0; i < all.length; i += 1) {
                  all[i].SourceElement().parentNode.className += ' has-jax';
              }
          });
    </script>

    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>



			</div>
		</div>

		<div id="bottom-outer">
			<div id="bottom-inner">
				Site by 阳生 | 
				Powered by <a target="_blank" rel="noopener" href="http://hexo.io">Hexo</a> |
				theme <a target="_blank" rel="noopener" href="https://github.com/fireworks99/hexo-theme-PreciousJoy">PreciousJoy</a>
			</div>
		</div>

		
	</div>





	
	<!-- scripts list from theme config.yml -->
	
	<script src="/js/jquery-3.5.1.min.js"></script>
	
	<script src="/js/PreciousJoy.js"></script>
	
	<script src="/js/highlight.pack.js"></script>
	
	<script src="/js/jquery.fancybox.min.js"></script>
	
	<script src="/js/search.js"></script>
	
	<script src="/js/load.js"></script>
	
	<script src="/js/jquery.mCustomScrollbar.concat.min.js"></script>
	
	<script src="/js/clipboard.min.js"></script>
	
	

	<script>hljs.initHighlightingOnLoad();</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
