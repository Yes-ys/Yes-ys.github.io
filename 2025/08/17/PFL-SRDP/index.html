


<!DOCTYPE html>
<html lang="ch">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<title>PFL-SRDP [ 代码和诗 ]</title>
	
	
	<!-- stylesheets list from _config.yml -->
	
	<link rel="stylesheet" href="/css/PreciousJoy.css">
	
	<link rel="stylesheet" href="/css/top-bar.css">
	
	<link rel="stylesheet" href="/css/menu-outer.css">
	
	<link rel="stylesheet" href="/css/content-outer.css">
	
	<link rel="stylesheet" href="/css/bottom-outer.css">
	
	<link rel="stylesheet" href="/css/atom-one-dark.css">
	
	<link rel="stylesheet" href="/css/recent-posts-item.css">
	
	<link rel="stylesheet" href="/css/article-sidebar-toc.css">
	
	<link rel="stylesheet" href="/css/jquery.fancybox.min.css">
	
	<link rel="stylesheet" href="/css/search.css">
	
	<link rel="stylesheet" href="/css/toc.css">
	
	<link rel="stylesheet" href="/css/sidebar.css">
	
	<link rel="stylesheet" href="/css/archive.css">
	
	<link rel="stylesheet" href="/css/jquery.mCustomScrollbar.min.css">
	
	<link rel="stylesheet" href="/css/Z-last-cover-others.css">
	
	
	
<meta name="generator" content="Hexo 7.3.0"></head>




<body id="wrapper">

	<div id="">
		
		<div id="top-bar">
			
			<div id="avatar-box">
				<img 
				class="avatar"
				src="/images/my-avatar.jpg" //网站头像
				alt="avatar">
			</div>

			<div id="top-bar-text">
				<div id="top-bar-title">
					阳生。
				</div>
				<div id="top-bar-slogan">
					风毛丛劲节，只上尽头竿。
				</div>
			</div>

		</div>

		<div id="menu-outer">
			<div id="menu-inner">
				
				
				<div class="menu-item">
					<a href="/">Home</a>
				</div>
				
				<div class="menu-item">
					<a href="/about">About</a>
				</div>
				
				<div class="menu-item">
					<a href="/archives">Archives</a>
				</div>
				

				<div class="menu-item menu-item-search">
					
  <span class="local-search local-search-google local-search-plugin">
      <input type="search" placeholder="站内搜索" id="local-search-input" class="local-search-input-cls" style="">
      <div id="local-search-result" class="local-search-result-cls"></div>
  </span>
	
				</div>

			</div>
		</div>

		<div id="content-outer">
			<div id="content-inner">

				
<div id="details">
	
	<article id="details-post">
		<div id=details-post-item>
			<h1>PFL-SRDP</h1>
			<p><code>最近准备申请一个大创的项目，研究围绕PFL展开，这篇blog用于记录其中的一些思路</code></p>
<h2 id="PFL相关的基本问题-阶段0"><a href="#PFL相关的基本问题-阶段0" class="headerlink" title="PFL相关的基本问题(阶段0)"></a>PFL相关的基本问题(阶段0)</h2><h3 id="PFL的基本类型"><a href="#PFL的基本类型" class="headerlink" title="PFL的基本类型"></a>PFL的基本类型</h3><p>(1) 学习单一全局模型并进行微调的方法<br>这些方法首先学习一个全局共享模型，然后在每个客户端上进行本地微调。</p>
<h4 id="例子：FedAvg-本地微调"><a href="#例子：FedAvg-本地微调" class="headerlink" title="例子：FedAvg + 本地微调"></a>例子：FedAvg + 本地微调</h4><p>首先，使用联邦平均（Federated Averaging，FedAvg）算法进行全局模型的训练。FedAvg通过在每个客户端本地训练模型，然后将本地模型参数上传到服务器进行平均化，形成全局模型。</p>
<p>步骤：</p>
<ol>
<li>每个客户端在本地数据上训练模型若干轮。</li>
<li>将本地模型参数上传到服务器。</li>
<li>服务器对所有客户端上传的模型参数进行平均化，得到新的全局模型。</li>
<li>将新的全局模型分发给每个客户端。</li>
<li>每个客户端在全局模型基础上进行本地微调，使用本地数据继续训练若干轮。</li>
</ol>
<p><code>第一类要运用Nash-bargaining game来进行聚合的话，基本上与FL的差异不大，因为都是考虑对客户端训练出的参数在服务器上进行聚合。</code></p>
<p>(2) 学习额外个性化模型的方法<br>这些方法在全局模型的基础上，为每个客户端学习一个额外的个性化模型。</p>
<h4 id="例子：FedPer"><a href="#例子：FedPer" class="headerlink" title="例子：FedPer"></a>例子：FedPer</h4><p>FedPer（Federated Personalization）方法提出在全局模型的基础上，为每个客户端学习一个个性化的模型头（或最后一层）。</p>
<p>步骤：</p>
<ol>
<li>使用FedAvg算法训练一个共享的全局模型，但仅共享前几层的参数。</li>
<li>每个客户端保持自己的个性化模型头，该模型头仅在本地数据上训练。</li>
<li>在每轮通信中，仅共享和更新全局模型的共享层参数，不包括个性化头部参数。</li>
</ol>
<p>(3) 通过个性化（本地）聚合学习本地模型的方法<br>这些方法通过个性化聚合来进一步捕捉个性化需求，为每个客户端生成特定的模型。</p>
<h4 id="例子：FedProx"><a href="#例子：FedProx" class="headerlink" title="例子：FedProx"></a>例子：FedProx</h4><p>FedProx（Federated Proximal）方法通过在本地训练过程中添加一个正则项，来限制本地模型偏离全局模型过多，从而实现个性化。</p>
<p>步骤：</p>
<ol>
<li>每个客户端在本地数据上训练模型时，在损失函数中添加一个正则项，限制模型参数与全局模型参数的差异。</li>
<li>正则项形式为：loss + μ&#x2F;2 * ||w - w_global||^2，其中w是本地模型参数，w_global是全局模型参数，μ是正则化系数。</li>
<li>通过这种方法，使得模型在本地数据上训练时，仍然保持一定的全局模型特性。</li>
</ol>
<h2 id="设想的算法"><a href="#设想的算法" class="headerlink" title="设想的算法"></a>设想的算法</h2><p><img src="/../_images/MyAlgorithm.jpg" alt="MyAlgorithm"></p>
<h2 id="研究目的"><a href="#研究目的" class="headerlink" title="研究目的"></a>研究目的</h2><ol>
<li>有关联邦学习的内容</li>
<li>为什么需要个性化联邦学习（非独立同分布…）</li>
<li>个性化联邦学习的常见方法（各种方法对应的参考文献）</li>
<li>关于NashBargainingGame的一些基本情况，其与相关学习算法的结合</li>
<li>最终我们的研究目的</li>
</ol>
<h2 id="研究内容"><a href="#研究内容" class="headerlink" title="研究内容"></a>研究内容</h2><ol>
<li>实现基本的FL算法框架（FedAvg）</li>
<li>考虑第三类PFL，引入Nash-bargaining Game设置个性化聚合方法</li>
<li>考虑引入权重α，进一步改进算法</li>
<li>PFL实验评估（通过权威论文，明确PFL算法量化评价指标）</li>
</ol>
<h2 id="一些参考文献"><a href="#一些参考文献" class="headerlink" title="一些参考文献"></a>一些参考文献</h2><p>Advances and Open Problems in Federated Learning 联邦学习领域的权威综述，其中把个性化联邦学习作为联邦学习下一个分支，有所提及。</p>
<h2 id="琐碎的思路-阶段1"><a href="#琐碎的思路-阶段1" class="headerlink" title="琐碎的思路(阶段1)"></a>琐碎的思路(阶段1)</h2><p>如果我们不重点考虑隐私问题，或许可以采用数据中心分布式学习的背景框架。这种背景下，我们是要在一个大而扁平的数据集上训练模型，每个客户端是单个集群或数据中心中的计算结点。数据分配的特点是任何客户端都可以读取数据集任意部分。</p>
<p>可能的学习路线：</p>
<ol>
<li>机器学习入门（基本的线性模型训练过程、梯度下降法）</li>
<li>神经网络（基本的训练模式，前向传递、后向传递）</li>
<li>多任务学习+Nash Bargaining Game（大概读懂论文）</li>
<li>FL+Nash Bargaining Game（理解这种迁移）</li>
<li>理解我们目前大概的思路</li>
</ol>
<h3 id="论文撰写相关"><a href="#论文撰写相关" class="headerlink" title="论文撰写相关"></a>论文撰写相关</h3><h4 id="个性化联邦学习常用的优化目标"><a href="#个性化联邦学习常用的优化目标" class="headerlink" title="个性化联邦学习常用的优化目标"></a>个性化联邦学习常用的优化目标</h4><p>在个性化联邦学习（PFL）中，正则化项的设计非常关键，它能帮助平衡全局模型和各个客户端的个性化模型之间的关系。常见的正则化项主要有以下几种形式：</p>
<p>参数距离正则化： 这种方法通过限制个性化模型参数和全局模型参数之间的距离，来保证个性化模型不会偏离全局模型太远。常见的形式有：<br>L2 正则化（欧氏距离）：<br>$\mathcal{R}(w_k, w_g) &#x3D; |w_k - w_g|^2_2$</p>
<p>L1 正则化：<br>$\mathcal{R}(w_k, w_g) &#x3D; |w_k - w_g|_1$</p>
<p>混合模型正则化： 这种方法通过将全局模型和本地模型进行线性组合，并对这种组合进行约束：$w_k &#x3D; \lambda w_g + (1 - \lambda)\tilde{w}_k$其中，$\lambda$ 是权重系数，$\tilde{w}_k$是客户端 $k$ 自己训练的模型参数。这种方法可以通过控制 $\lambda$ 的大小来调节全局模型和本地模型的影响。</p>
<p>多任务学习正则化： 这种方法将个性化建模视为多任务学习问题，通过对不同任务（客户端）的模型参数进行正则化：$\mathcal{R}(w_k, w_g) &#x3D; |v_k|^2_2$其中，$w_k &#x3D; w_g + v_k$，$v_k$是客户端 $k$ 的个性化参数。</p>
<p>元学习正则化： 元学习方法通过对元模型参数进行调整，以便在少量本地数据上快速适应：$\mathcal{R}(\theta, \mathcal{D}_k) &#x3D; \mathcal{L}<em>k(\theta - \alpha \nabla</em>{\theta} \mathcal{L}_k(\theta; \mathcal{D}_k); \mathcal{D}_k)$其中，$\theta$ 是元模型参数，$\alpha$ 是学习率。</p>
<p>KL 散度正则化： 这种方法通过衡量个性化模型和全局模型的概率分布之间的差异，来进行正则化：$\mathcal{R}(w_k, w_g) &#x3D; D_{KL}(P(w_k) | P(w_g))$其中，$D_{KL}$是 Kullback-Leibler 散度，$P(w_k)$和 $P(w_g)$ 分别是个性化模型和全局模型的概率分布。</p>
<h3 id="实验实现相关"><a href="#实验实现相关" class="headerlink" title="实验实现相关"></a>实验实现相关</h3><p>在Server端使用经典的FedAvg算法进行权重的聚合</p>
<p>在Client端对Server发送过来的权重，结合自己的本地权重，进行正则化</p>
<p>算法的基本流程：</p>
<ol>
<li>（初始化部分）服务器初始化参数，并发送给客户端，作为本地参数</li>
<li>服务器向客户端发送全局参数</li>
<li>客户端使用数据集的一个子集，训练更新本地参数；并使用本地参数与全局参数进行后正则化处理（NBG），得到个性化参数，更新本地参数</li>
<li>客户端向服务器发送个性化参数，服务器对所有的个性化参数使用FedAvg进行聚合，得到新的全局参数</li>
<li>重复2~4</li>
</ol>
<p><strong>算法的基本流程（改）</strong>：</p>
<ol>
<li>（初始化部分）服务器初始化参数，并发送给客户端，作为本地参数</li>
<li>服务器向客户端发送全局参数</li>
<li>客户端使用数据集的一个子集，训练<strong>更新本地参数、全局参数</strong>（梯度下降步，此处的全局参数由本地维护，与客户端的全局参数无关）；并使用本地参数的更新梯度与全局参数的更新梯度进行Nash Bargaining Game，全局模型效用函数为$u_g &#x3D; \nabla\omega_g^T\Delta\omega$，本地模型效用函数为$u_l &#x3D; \nabla\omega_l^T\Delta\omega$，其中Nash Bargaining Game的通解是$\Delta\omega &#x3D; \alpha1\nabla\omega_l+\alpha2\nabla\omega_g $，其中$G^TG\alpha &#x3D; 1$对应$\alpha$权重矩阵</li>
<li>求解Nash Bargaining Game，得到$\alpha$，用于聚合本地参数与全局参数，更新本地参数，使其包含更多的个性化信息$\omega_l &#x3D; \alpha1\omega_l + \alpha2\omega_g$（类似混合模型的方法）</li>
<li>客户端向服务器发送本地参数$\omega_l$，服务器对所有的个性化参数进行加权聚合（以样本量为权重），得到新的全局参数$\omega_g$</li>
<li>重复2~5</li>
</ol>
<p>改代码的思路：</p>
<ol>
<li>实现一个基本的FedAvg算法</li>
<li>实现基本的正则化（混合模型，后正则化）</li>
<li>更改混合模型正则化为Nash Bargaining Game</li>
</ol>
<p>关于实现基本的正则化的思路：</p>
<p>关键是要为client多引入一份模型参数，作为个性化参数</p>
<ol>
<li>本地参数在初始化的时候和个性化参数一起，用全局参数初始化；后续每次send_model的时候，本地参数不再更新，只更新个性化参数；每次都使用本地参数照常训练，并且在3、的个性化参数更新之后，用个性化参数替代本地参数</li>
<li>多引入一个个性化参数成员变量，在初始化本地参数的时候，一并将个性化参数做相同设置（都设置为全局参数）：1、加入成员变量（注意初始化）；2、修改set_parameters函数（只设置个性化参数，不再设置本地参数）</li>
<li>当本地参数训练完成之后（之前的直接复用原代码即可），用个性化参数与本地参数进行正则化处理，得到新的个性化参数（同样也更新本地参数）：1、添加正则化函数（后续在此之上引入NBG）2、更新本地参数</li>
<li>将个性化参数发送回服务器：1、修改send_model函数，使其返回的是新的个性化参数的成员变量（不用再修改了，因为我们正则化得到个性化参数后也会将其设置为本地参数，send_model会返回本地参数）</li>
</ol>
<p>引入Nash Bargaining Game的思路：</p>
<p>Q：Nash Bargaining Game的源代码实现是多个任务的loss进行聚合后，用weighted_loss，反向传播求梯度，然后进行梯度下降；对应的就等价于论文中的，各个任务的loss先反向传播，得到weighted_loss，再进行聚合；而FL中的Nash Bargaining Game一般是将各个客户端期望的更新参数用于聚合，对应到我的PFLNash中，应该是将local model在local dataset上期望的更新参数&amp;global model在dataset上期望的更新参数进行聚合；期望的更新参数实际上就是loss反向传播求得的梯度乘上学习率，所以期望更新参数的聚合，可以等价于loss求得梯度的聚合（也就是源代码先聚合loss再求梯度），也适用于Nash Bargaining Game的源代码（学习率在梯度下降,即.step()操作的时候引入）</p>
<ol>
<li>使用personalized model进行前向传播，并求交叉熵损失得到loss2（loss1已经有了）</li>
<li>使用loss1与loss2进行聚合得到综合loss，并使用综合loss反向传播求梯度</li>
<li>使用梯度进行梯度下降，得到新的model</li>
</ol>
<p>如何确保聚合后的loss，求梯度下降的时候对于两个模型中结构位对应相同的参数一视同仁。有必要做到一视同仁吗？</p>
<p>在MTL中，各个任务的损失函数不同，对应即T1,T2,T3对应同一套参数omega的loss1、loss2、loss3不同，但是均可对omega求梯度，得到的梯度使用Nash Bargaining Game进行聚合，用来更新omega</p>
<p>而在PFL中，只有一个任务&#x2F;损失函数相同，但是有两套参数omega1、omega2，这两套参数可以视作同一个model的两套参数；loss相同（从函数结构上来说），loss对omega1、omega2求梯度，得到的值不同分为loss1、loss2，梯度不同，对应参数的更新方向不同</p>
<p>MTL中如果直接聚合loss1、loss2、loss3得到loss，然后反向传播，loss是对omega求梯度nabla，再梯度下降，oemga会使用nalba更新自己；而如果PFL中直接聚合loss1、loss2，然后反向传播，其会将omega1、omega2当作两个不同的参数，分别求梯度nabla1、nabla2，然后反向传播，omega1会用nabla1更新自己，omega2会用nabla2更新自己，得到omega1’，omega2’</p>
<p>或许我应该再令omega2 &#x3D; omega2 + (omega2’-omega2)+(omega1’-omega1)，这样来完成更新。</p>
<p>实际上我使用的personalized_model来临时存储global_model，所以应该是omega2 &#x3D; omega1 + (omega1’ - omega1) + (omega2’ - omega2)</p>
<p>因为PFL中loss1、loss2聚合的时候Nash Bargaining Game为它们确定的系数会在实质上影响到nabla1、nabla2的值，而最基础的聚合也可以直接看作对nabla1、nabla2求一个加权和</p>
<p>对比最初我的设想是这样的：<code>实际上我这里并没有共享参数，我的loss1是由model1得到的、loss2是由model2得到的，model1与model2是结构完全相同的模型（尽管具体对应参数的数值可能不相同，但在初始化的时候model2是由model1deep copy得到的）；然后此时我想使用这个NashMTL对loss1、loss2进行聚合，完成之后只需要更新model2。</code>但这应该是错误的，聚合后的loss不可能只更新model2，这意为着将model1、model2中结构位相同的参数当作同一个参数进行处理，但这实际上是不行的。假设这是可以的，我们在对结构位相同的参数1求梯度的时候可能需要用到参数2，在相同的结构位上，model1、model2的参数2值可能是不同的，这个时候就无法确定应该用model1还是model2的参数2的值了。所以这是不行的。</p>
<h4 id="最终的实现思路"><a href="#最终的实现思路" class="headerlink" title="最终的实现思路"></a>最终的实现思路</h4><p>一个client保存了4个model，分别是个性化模型、本地模型，以及两个模型的备份（主要是用来备份参数）</p>
<p>为了使用MTL，我将本地模型的参数omega1、个性化模型的参数omega2，拼接成了一个大的共享参数网络(omega1,omega2)，然后传入个性化模型、本地模型分别求得的loss1、loss2，传入作为losses，(omega1,omega2)作为shared_parameters，进行nash_mtl.backward()。内部的运行逻辑是，将loss1、loss2通过Nash Bargaining Game聚合后得到一个loss，用loss分别对(omega1,omega2)中的各个参数求偏导得到梯度，更新(omega1,omega2)（这样，对应相同结构位的参数并不会视作同一个参数，而是当作不同参数处理）。</p>
<p>而在此之前，我将omega1、omega2分别用两个备份模型进行了保存，当omega1、omega2更新完成后，再依次对个性化模型的各个参数进行操作，更新为：omega2 &#x3D; omega2 + (omega2’-omega2)+(omega1’-omega1)。最后将个性化模型深拷贝给本地模型，完成更新</p>
<h2 id="琐碎的思路-阶段2"><a href="#琐碎的思路-阶段2" class="headerlink" title="琐碎的思路(阶段2)"></a>琐碎的思路(阶段2)</h2><p>当前我已经完成的任务：</p>
<ol>
<li>问题的建模（method部分的论文初稿）</li>
<li>编写算法的代码，在MINST数据集上完成了测试，并参照了一些评价指标</li>
</ol>
<p>接下来我需要做的事情是对PFLNash的优点进行论述，老师给了我以下建议：</p>
<h3 id="考虑PFL中的典型评价方式"><a href="#考虑PFL中的典型评价方式" class="headerlink" title="考虑PFL中的典型评价方式"></a>考虑PFL中的典型评价方式</h3><p>通信（次数、复杂性）、收敛性、最优性、精度下限…，具体的需要我进一步查阅资料</p>
<p>通信次数的比较可以通过横向比较，相同通信次数下的收敛率（Loss趋于一个较低的稳定值的情况）</p>
<p>通信复杂性是和通信次数相关的，通常通信复杂性取决于：</p>
<ol>
<li>模型大小（每次通信发送模型参数的数量）</li>
<li>通信的次数</li>
<li>参与客户端个数</li>
</ol>
<p>复杂性可以定义为 R×N×b（通信次数、参与客户端个数、模型参数个数）</p>
<p>优化的方法：</p>
<ol>
<li>通信协议（采用一些压缩技术对模型参数进行压缩，不太了解…）</li>
<li>增加本地模型训练轮数</li>
<li>让客户端部分参与</li>
</ol>
<p>关于收敛性分析或许可以参考一下这个视频<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Ks1PYGEu5/?spm_id_from=333.1387.upload.video_card.click&vd_source=80df09f481ef5f0671e5e0e35d02e33e">联邦学习收敛性分析</a></p>
<h3 id="参考当前方法中不好的点"><a href="#参考当前方法中不好的点" class="headerlink" title="参考当前方法中不好的点"></a>参考当前方法中不好的点</h3><p>考虑我参考方法中的缺点，描述自己是如何在前人基础上进行改进的</p>
<p>具体的方向应该是去找一些有关通过正则化方法实现PFL的算法中存在的缺陷</p>
<h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><p>最后老师提到了自己的研究生曾经做过的一个工作，通过知识蒸馏提取个性化特征，与全局模型进行融合，得到个性化模型。并且告诉我可以思考自己的方法较这个方法的优点是什么，类似于在第二个方向中给了我一个更加具体的方向。值得留意。</p>
<h3 id="当前阶段阅读的一些参考文献"><a href="#当前阶段阅读的一些参考文献" class="headerlink" title="当前阶段阅读的一些参考文献"></a>当前阶段阅读的一些参考文献</h3><h4 id="FedAvgM"><a href="#FedAvgM" class="headerlink" title="FedAvgM"></a>FedAvgM</h4><p>这篇文章主要做了两个工作</p>
<ol>
<li>在服务器端引入了动量来更新权重, 提升了FedAVG算法对非独立同分布数据的训练效果</li>
<li>基于狄利克雷分布，提出了一种Non-IID数据生成的方法，用来对FedAVGM以及FedAVG进行测试</li>
</ol>
<p>主要的优点在于accuracy的提高</p>
<p>给我的一个启发是它的实现，实验测试了通信次数-准确率的表现，并且以centralized learning作为了一个衡量标准（我是否可以把centralized learning最终的结果作为一个optimal accuracy的表现？）</p>
<h4 id="Per-FedAvg"><a href="#Per-FedAvg" class="headerlink" title="Per-FedAvg"></a>Per-FedAvg</h4><p>这篇文章的主要工作是把FedAvg算法与meta-learning的框架结合起来，实现了个性化的效果，比较注重理论，严格地证明了算法的收敛性。</p>
<p>实验方法，主要是对比了accuracy，对比的算法是FedAvg+本地微调与PerFedAvg算法</p>
<h3 id="我的思路"><a href="#我的思路" class="headerlink" title="我的思路"></a>我的思路</h3><p>目前翻阅了多篇较为经典的论文，里面的实验大多数都是聚焦于accuracy这一指标。我大概的想法也是将PFLNash的accuracy与一些basline（混合模型正则化、Fedprox…）做对比。此外，还可以从loss-通信轮次的角度来对比，loss收敛时的通信轮次，来说明通信方法的优点。最后，我可能需要仔细学习一下收敛性证明方法的理论，来对PFLNash是否可以从数学层面进行严格的收敛性证明进行一个评估。</p>
<p>具体来说，于是接下来我有两个主要工作：</p>
<ol>
<li>学习FedAvg算法的收敛性证明，考虑是否可以应用到PFLNash中</li>
<li>实验角度，主要是两个实验，一个是accuracy的，一个是loss的，对比混合模型正则化、Fedprox…</li>
</ol>
<p>最后选定了基线模型之后，还可以考虑一些其它的优点，比如避免了引入一些需要人为设置且变动影响不是很清晰的超参数…</p>
<h4 id="关于收敛性证明"><a href="#关于收敛性证明" class="headerlink" title="关于收敛性证明"></a>关于收敛性证明</h4><p>在三大假设的基础之上，集中数据集中的GD收敛性是可以得到保证的；经典的FedAvg算法实际上是一种分布式数据集上的SGD。于是FedAvg与GD的差异主要来源于两个方面</p>
<ol>
<li>分布式训练参数带来的误差</li>
<li>分布式参数聚合时的误差</li>
</ol>
<p>我们的证明思路可以是先考虑1、中产生的误差存在一个bound，再证明2、中产生的误差也存在一个bound，并且这两个bound都是可以收敛到0的，那么最终FedAvg的训练效果会收敛到集中化数据集下GD的训练效果，于是收敛性可以得到保证。</p>
<p>现在我参考的资料FedAvg的收敛性已经证明完毕，我要考虑PFLNash的收敛性证明，其中多的一项误差可能是NBG带来的。</p>
<h4 id="关于实验"><a href="#关于实验" class="headerlink" title="关于实验"></a>关于实验</h4><p>首先我要选择一些合适的算法作为我的基线模型；从三种PFL的实现思路，分别找三种经典的算法</p>
<ol>
<li>FL+本地微调（FedAvg）</li>
<li>仅使用全局参数，更新本地模型的部分层参数，本地模型有自己的任务头，仅通过本地数据进行训练（FedPer）</li>
<li>使用全局模型与本地模型进行个性化聚合（例如在本地模型训练的过程中在目标函数中加入正则化项）（FedProx）</li>
</ol>
<p>实验部分设想（三类实验）</p>
<ol>
<li>第一类实验使用PFLNash与一些经典的算法，在平均准确率与损失函数收敛上进行横向对比，分析相关的性能表现、实验收敛性以及通信复杂性；</li>
<li>第二类实验，通过调整一些超参数，对比相应情况下PFLNash的各项表现，进行相应的敏感性分析；</li>
<li>第三类实验，类比实际的场景，测试了不同客户端数量下PFLNash的表现，同时考虑了客户端可能掉线的情况。</li>
</ol>
<p>可选用数据集：<br>AGNews、AmazonReview、Camelyon、Cifar10、Cifar100、Country211、COVIDx、Digit5、DomainNet、EMNIST、FashionMNIST、FEMNIST、Flower102、GTSBR、HAR、iWildCam、kvasir、MNIST、Omniglot、PAMAP2、Shakespeare、SogouNews、StanfordCars、TinyImagenet</p>
<p>可选用模型：<br>MLR、CNN、DNN、ResNet18、ResNet10、ResNet34、AlexNet、GooleNet、MobileNet、LSTM、BiLSTM、fastText、TextCNN、Transformer、AmazonMLP、HARCNN</p>
<p>第一类实验使用数据集</p>
<p><strong>图像识别相关数据集</strong>：</p>
<ol>
<li>MNIST</li>
<li>EMNIST</li>
<li>Cifar10</li>
<li>Cifar100</li>
</ol>
<p><strong>文本分类相关数据集</strong>：</p>
<p>AGNews（新闻文本分类数据集）<br>数据类型：新闻文章标题与描述<br>标签：文章类别标签，四类，World、Sports、Business、Sci&#x2F;Tech<br>模型：TextCNN、fastText</p>
<p><strong>医疗和生物数据</strong>：</p>
<p>COVIDx（新冠肺炎检测图像数据集）<br>数据类型：胸部X光片（CXR）图像<br>标签：COVID-19阳性、COVID-19阴性、正常<br>模型：CNN、ResNet34</p>
<p><strong>传感器数据</strong>：</p>
<p>HAR（只能收集传感器数据的人体活动识别数据集）<br>数据类型：只能收集内置加速度计和陀螺仪采集的数据<br>标签：标注为不同的物理活动，包括：走路、上楼、下楼、坐着、站着、躺着<br>模型：LSTM、HARCNN</p>
<p>考虑选用数据集：<br>AGNews、AmazonReview、Camelyon、Cifar10、Cifar100、Country211、COVIDx、Digit5、DomainNet、EMNIST、FashionMNIST、FEMNIST、Flower102、GTSBR、HAR、iWildCam、kvasir、MNIST、Omniglot、PAMAP2、Shakespeare、SogouNews、StanfordCars、TinyImagenet</p>
<p>注：<strong>可以考虑异质性数据（狄利克雷+非平衡划分）和非异质性数据（平衡划分）</strong></p>
<p>具体任务划分</p>
<ol>
<li>租聘云服务器，完成实验环境搭建</li>
<li>学习使用py处理h5文件，完成训练结果的可视化展示</li>
<li>训练比较效果</li>
<li>完成实验部分论文内容</li>
</ol>
<p>下面记录一下经典论文中的实验信息</p>
<p>FedAvg+本地微调（FedAvgM）：</p>
<p>客户端数量：40<br>通信次数：10000<br>本地训练次数：1~5<br>模型：CNN<br>数据集：CIFAR-10<br>观测指标：acc-round、best_acc-local_epoch</p>
<p>个性化任务头</p>
<p>FedPer</p>
<p>客户端数量：10、10、30<br>通信次数：50、50、20<br>本地训练次数：1<br>模型：ResNet-34、MobileNet-v1<br>数据集：CIFAR-10、CIFAR-100、FLICKR-AES（均有iid划分、non-iid划分）<br>观测指标：acc-rounds（还通过调整网络结构、调整一些参数例如class_num做了对比）</p>
<p>个性化聚合</p>
<p>FedProx：</p>
<p>客户端数量：1000、1000、200、143、772（对应数据集）<br>通信次数：200、100、200、40、800<br>本地训练次数：1<br>模型：LSTM（不同实验外加了许多自己的调整）<br>数据集：Symnthetic、MNIST、FEMNIST、Shakespeare、Sent140<br>观测指标：loss-rounds、acc-rounds</p>
<p>根据我使用的数据集记录实验条件</p>
<p><strong>已经测试过的算法</strong>：</p>
<p>老师要求，再明确一下个性化联邦学习的评价标准是怎样的</p>
<p><strong>FedNash：</strong>mean for best accuracy: 0.995887594242632</p>
<ol>
<li>FedAvg：mean for best accuracy: 0.99011348192059012</li>
<li>FedProx：mean for best accuracy: 0.9911355198267598</li>
<li>FedPer：mean for best accuracy: 0.99323784539187598</li>
<li>FedAS：mean for best accuracy: 0.992348979183749142</li>
<li>FedALA：mean for best accuracy: 0.9945739090701393(*)</li>
<li>Ditto：mean for best accuracy: 0.9917751884852639(*)</li>
<li>FedPAC：mean for best accuracy: 0.9921178889650446(*)</li>
<li>MOON：mean for best accuracy: 0.9611606122915238</li>
<li>FedPep：mean for best accuracy: 0.9936600411240576</li>
<li>FedFomo：mean for best accuracy: 0.9911469042723327</li>
</ol>
<p><strong>Cifar100</strong>：<br>local epoch 1；global epoch：100；<br>client nums：20；joining rotio：100%；<br>class nums：10；Model：CNN（2conv、2fc）；<br>partition：non-iid（dir）；non-blance；<br>learning-rate：0.005 (ldg 0.99)<br>model：CNN</p>
<p><strong>已经测试过的算法</strong>：</p>
<p><strong>PFLNash：</strong>mean for best accuracy: 0.5981476545842217</p>
<ol>
<li>FedAvg: mean for best accuracy: 0.30810234541577824</li>
<li>FedProx：mean for best accuracy: 0.30810234541577824</li>
<li>FedPer：mean for best accuracy: 0.4931369936034115</li>
<li>FedAS：mean for Best accuracy. 0.4542910447761194</li>
<li>FedALA：mean for best accuracy: 0.5360474413646056（*）</li>
<li>Ditto：mean for best accuracy: 0.4754131130063966</li>
<li>FedPAC: mean for best 0.5436433901918977（*）</li>
<li>MOON: mean for best accuracy: 0.25912846481876334</li>
<li>FedPep：mean for best accuracy: 0.5059968017057569</li>
<li>FedFomo：mean for best accuracy: 0.44869402985074625（*）</li>
</ol>
<p><strong>MNIST</strong>：<br>local epoch 1；global epoch：30；<br>client nums：20；joining rotio：100%；<br>class nums：10；Model：CNN（2conv、2fc）；<br>partition：non-iid（dir）；non-blance；<br>learning-rate：0.005 (ldg 0.99)<br>model：CNN</p>
<p><strong>实验选择的对比算法：</strong></p>
<p>FedProx、FedPer、FedAvg（经典的）<br>FedAS、FedALA、Ditto、FedPAC、MOON、FedRep、FedPHP、FedFomo、pFedME</p>
<p><strong>各算法准确率测试标准</strong>：</p>
<p>(AAAI)FedALA：个性化准确率<br>(ICLR)FedPAC：泛化准确率</p>
<h2 id="实验部分论文"><a href="#实验部分论文" class="headerlink" title="实验部分论文"></a>实验部分论文</h2><p><code>这里先统一都用文字阐释，写到overleaf里面的时候再做修改，在需要的地方补充数学符号，显得更加专业</code><br><code>让AI帮忙写的时候先学习一下经典论文的文风</code></p>
<p>我们对PFLNash做了许多测试，在多个数据集上均取得了较好的效果，在这一部分我们对实验的细节情况进行说明。我们一共设置了三类实验，第一类实验使用PFLNash与一些经典的算法，在平均准确率与损失函数收敛上进行了横向对比，分析了相关的性能表现、实验收敛性以及通信复杂性；第二类实验我们通过调整一些超参数，对比相应情况下PFLNash的各项表现，进行了相应的敏感性分析；第三类实验，我们充分类比了实际的场景，测试了不同客户端数量下PFLNash的表现，同时考虑了客户端可能掉线的情况。</p>
<p>PFLNash与经典算法的对比</p>
<p>我们挑选了一些个性化联邦学习的经典算法，包括FedProx，该算法与PFLNash类似，并不直接使用全局参数更新本地参数，而是为本地训练的过程中引入了一个正则化项，来限制本地参数在训练的过程中过远的偏离全局参数，在此基础上尽可能地使用本地数据训练本地参数，从而达成个性化的效果。与该算法相比PFLNash所作的是后正则化，并不在训练的过程中考虑本地参数与全局参数的偏差，而是在本地训练完成之后使用Nash Bargaining Game让本地参数与全局参数协商出一个平衡的结果。FedPer，该算法通过仅让本地参数与全局参数共享网络的前几层参数，而保留个性化任务头的参数，来实现个性化的效果。FedAvg+本地微调，这是为了实现个性化最经典的做法，在全局模型的基础上用本地数据进一步训练。实际上对比的这几种算法对应了三种实现个性化联邦学习的常见策略，即个性化聚合的方法、学习额外个性化模型的方法、全局训练结合本地微调的方法，具有典型的代表作用。</p>
<p>为了评估模型的个性化情况，我们测试的平均准确率是个性化准确率，即每个客户端个性化模型的准确率按照样本数量进行加权平均。我们使用的测试数据集包括MNIST、EMNIST、Cifar100、Cifar10，我们分别考虑服从独立同分布的数据情况以及具有异质性的数据情况。后者是使用狄利克雷方法对数据集进行非平衡的划分，最终使各个客户端持有的数据集不服从独立同分布性，且数据集大小不一。</p>
<p>对于MNIST数据集，我们考虑了20个客户端，每个客户端每次迭代进行E轮本地训练，$E\in[1,5]$。本地训练使用SGD，设置batch_size大小为10，本地学习率设置为0.005。全局模型进行30轮迭代，每轮全局迭代之后对模型进行评估，测试当前的准确率以及损失函数的大小。使用的模型是经典的CNN网络，其具有两个卷积层，两个全连接层，可以用来完成简单的图像分类工作。</p>
<h2 id="会议-期刊"><a href="#会议-期刊" class="headerlink" title="会议&#x2F;期刊"></a>会议&#x2F;期刊</h2><h3 id="会议"><a href="#会议" class="headerlink" title="会议"></a>会议</h3><p><strong>ICDCS</strong> (IEEE International Conference on Distributed Computing Systems) - CCF-B</p>
<p>相关介绍：ICDCS 是分布式计算领域的会议，联邦学习本质上是分布式机器学习，是其核心关注方向之一。近年来关于联邦学习优化、隐私、效率、个性化（如模型个性化、梯度个性化）的论文非常多。</p>
<p>DDL：ICDCS(2026) 2025-12-05;2025-12-12<br>（时间参考：AoE）<br>两到三个月评审时间，不能转投其它会议…</p>
<p><strong>ECML-PKDD</strong> (European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases) - CCF-B</p>
<p>相关介绍：欧洲顶级的机器学习与数据挖掘联合会议，涵盖面非常广。联邦学习和个性化学习是其核心关注领域的热点方向。论文质量要求很高。</p>
<p>DDL:1st Submission Deadline: 30 October 2025 2nd Submission Deadline: 15 January 2026</p>
<p><strong>BDCAT</strong> (IEEE&#x2F;ACM International Conference on Big Data Computing, Applications and Technologies) - CCF-C</p>
<p>相关介绍：关注大数据计算、应用和技术。联邦学习作为处理分布式、隐私敏感大数据的关键技术，是其核心主题。个性化建模也是大数据应用的重要需求。</p>
<p>DDL：DDL (2026预估)： 主会DDL通常在7月左右</p>
<p><strong>ICA3PP</strong> (International Conference on Algorithms and Architectures for Parallel Processing) - CCF-C</p>
<p>相关介绍： 专注于并行处理的算法和架构。联邦学习涉及分布式并行计算，如果你的工作侧重于底层的并行优化算法（如通信压缩、异步更新、个性化聚合的高效实现）或特定的并行架构适配，则很契合。</p>
<p>DDL：DDL (2026预估)： ICA3PP通常在5月或6月截稿</p>
<h3 id="期刊"><a href="#期刊" class="headerlink" title="期刊"></a>期刊</h3><p>IEEE Transactions on Neural Networks and Learning Systems (TNNLS) - CCF-B</p>
<p>相关介绍：专注于神经网络和机器学习系统。对联邦学习（尤其是优化算法、个性化建模）、分布式学习、自适应学习系统等方向非常友好。</p>
<p>Future Generation Computer Systems (FGCS) - CCF-C (但常被对标更高)</p>
<p>相关介绍：虽然是C类，但其在联邦学习社区非常热门，影响因子和分区(JCR Q1&#x2F;Q2)常优于部分B类期刊。强烈关注未来计算系统，联邦学习及其应用（包括个性化）是其核心。</p>
<h2 id="NBG-代码重写"><a href="#NBG-代码重写" class="headerlink" title="NBG 代码重写"></a>NBG 代码重写</h2><p>在实验过程中遇到了一个比较严峻的问题，我在复用MTL-Nash代码的过程，最终将NBG的一般解转换为了一个优化问题，优化问题的结果是一个<strong>更新梯度</strong>的权重矩阵。</p>
<p>出现的问题是：</p>
<p>1）梯度爆炸<br>2）权重矩阵最后始终呈现出1，1的异常值，即最终的更新梯度（同样也是更新向量），会直接相加；<br>3）我通过一系列debug发现原因在NBG转换为优化问题了之后，优化问题无法成功求解，一直用异常处理令alpha_param &#x3D; prevs_alpha_param，导致权重实际上一直处于初始值</p>
<p>我分析代码，注意到 我复用的代码-我起初设想的算法-我最终敲定的算法 三者间皆存在差距，但最有问题的是复用即实验的代码与我最终敲定的算法存在差距</p>
<p>由于在小型模型上，实验代码呈现出了不错的效果，所以起初我没有注意到这个问题，或许是因为对于小型模型异常的更新方式不足以导致梯度爆炸</p>
<p>尝试了许多debug的方法，最终更换了一个优化器，初步解决了问题，不知道有没有其它隐患，我查看了权重alpha的值，看起来表现得是比较正常的</p>
<p>但是对于Cifar100数据集，梯度爆炸的问题仍然存在，先停止debug，先做一些其它种类数据集上的实验</p>
<p><strong>经过一系列debug之后让代码可以按照我的逻辑正确执行，但是取得的效果并不好</strong>！</p>
<h2 id="最终的修改方案"><a href="#最终的修改方案" class="headerlink" title="最终的修改方案"></a>最终的修改方案</h2><p><strong>对原来的算法进行了修改：</strong>记录每次epoch中的得到的聚合权重，累加之，再归一化到[0,1]区间上，最后用于全局模型与本地模型的聚合，得到个性化模型。<br>（原来的算法是对全局模型与本地模型使用本地数据集训练时得出的更新向量进行聚合，然后得到一个个性化模型的个性向量，用来更新维护的个性化模型）</p>
<p>关键修改：</p>
<p>$\omega_p &#x3D; \alpha_1 \omega_g + \alpha_2 \omega_l$</p>
<p>$\omega_p \leftarrow \omega_p + \Delta\omega_p$</p>
<p>$\quad \Delta\omega_p &#x3D; \alpha_1 \Delta\omega_g + \alpha_2 \Delta\omega_l$</p>
<p>为修改做一些理论上的说明：</p>
<h2 id="论文写作"><a href="#论文写作" class="headerlink" title="论文写作"></a>论文写作</h2><p><code>目前整个科研工作取得了较好的进展，算法流程清晰，理论支撑也较为充足，在图像处理领域的测试取得了较好的效果，只剩下在NLP领域的数据集下进行模型性能的验证。于是我觉得是时候开始好好写论文了，这一部分用来记录写论文的一些过程。</code></p>
<p>整个论文写作过程，我觉得应该围绕以下几部分：</p>
<ol>
<li>前期准备，说明三个主要的idea来源，包括1）NBG作为聚合的方法；2）在本地对两个模型进行融合得到个性化模型；3）使用模型更新时，经过NBG后得到的梯度权重，作为聚合权重</li>
<li>Introdution部分，这一部分需要引入我设计算法时的相关理论支撑，包括：1）联邦学习与个性化联联邦学习；2）nash bargainning game相关理论；3）双模型联邦架构（这里需要解释后面两个主要idea的来源，主要可以集成学习中混合模型，结合梯度的权重反应梯度对模型性能增加的贡献或理解成模型本身的贡献度，来进行说明）</li>
<li>Mtheod部分，参考论文草稿，重新用中文自己写一次，再翻译成英文</li>
<li>Experiment部分，略</li>
</ol>
<h3 id="论文修改"><a href="#论文修改" class="headerlink" title="论文修改"></a>论文修改</h3><p>11.20冯老师的修改建议</p>
<ol>
<li>为了说明论文的问题背景引入插图（联邦学习的基本场景、客户端漂移的问题）</li>
<li>相关工作部分如果需要公式的话，则放在正文中</li>
<li>相关工作部分需要再补充一些，目前太少了</li>
<li>双模型架构的时候引入形象的图片</li>
<li>关于NBG的求解从转换到问题的位置，切割成两部分</li>
<li>伪代码改得更加的简洁（分为客户端与服务器两部分）</li>
<li>适当的多编一些子标题</li>
<li>介绍和相关工作尽量不要出现数学公式</li>
<li>introduction和related works都会提到一些别人的工作，但是introduction应该比related work在一个高一点的层面进行描述</li>
</ol>
<p>我自己的修改想法：</p>
<ol>
<li>伪代码部分的格式需要规范</li>
<li>摘要部分需要修改，必须言明自己最后取得的成果（<strong>摘要里面不能使用数学公式与任何符号</strong>）</li>
<li>整理实验的所需数据：各个对比项的表格；不同采样率下的对比；全局准确率</li>
<li>英文校准（待校准：introduction、related work、method）</li>
</ol>
<h3 id="论文草稿"><a href="#论文草稿" class="headerlink" title="论文草稿"></a>论文草稿</h3><p>这里记一下参考文献：</p>
<ol>
<li>FedAvg</li>
<li>Kairouz, P., McMahan, H. B., et al. (2021). “Advances and Open Problems in Federated Learning.</li>
<li>Li, T., Sahu, A. K., Talwalkar, A., &amp; Smith, V. (2020). “Federated Learning: Challenges, Methods, and Future Directions.</li>
<li>FedALA</li>
<li>APFL</li>
<li>FedRANE</li>
<li>FedAdp</li>
<li>NashMTL</li>
<li>FedMeta</li>
<li>FedProx</li>
<li>FedRep</li>
</ol>
<h4 id="一些注意事项"><a href="#一些注意事项" class="headerlink" title="一些注意事项"></a>一些注意事项</h4><p>在集成学习中，两个基模型的梯度可以衡量它们的贡献度，是否有类似的参考文献</p>
<p>Introduction部分可以先简略的说明我们方法较相关方法的优点，然后以口述的方式，脱离数学描述，在一定高度下将我们的方法描述清楚</p>
<p>Related Work部分可以在Introduction部分引出APFL、FedRANE的基础上，描述它们做了什么工作，然后尤其对于APFL要进行对比，凸显出我们的优点（当然FedRANE也可以对比）</p>
<p>论文对于重复的引用应该怎样进行处理？</p>
<p>注意NBG这个缩写的使用，是不是只要在Nash Bargaining Game第一次出现时，后面用括号标注了(NBG)，后续都可以这样简写</p>
<p>草稿里参杂了一些英文，因为我不是很确定对应英文的确切中文翻译是什么</p>
<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>联邦学习（Federated Learning, FL）是一种分布式机器学习方法，旨在保护数据隐私的同时训练模型【1】。其核心思想是将模型训练分散到各个数据持有者的本地设备上，在使用对应数据训练模型的同时避免本地设备泄漏自己的数据，从而在保护各个设备数据隐私的前提下完成模型训练。这种方法特别适用于跨设备、组织的数据协同场景，并广泛应用于移动设备、医疗健康、金融等领域。</p>
<p>传统的联邦学习通常考虑各个客户端的数据满足独立同分布的性质，但实际情况通常是各个客户端端数据具有异质性。在不同客户端间可能会出现，数据非独立同分布，数据数量相差很大，数据类别分布不平衡，具有严重的噪声标签等情况。因此传统的联邦学习算法存在许多问题。由于不同客户端数据分布存在显著差异，不同客户端的本地参数在训练过程中会发生漂移，导致聚合后的全局参数可能无法表征全体客户端的数据分布，最终全局模型表现出无法收敛以及性能较差等问题【2】。此外，在泛化性能下降的同时，由于传统联邦学习算法的全局模型试图在所有参与方的数据上表现均衡，这可能导致出现“平均损失问题”【3】，使得在某些用户上，模型性能较差，无法满足用户的个性化需求。于是，个性化联邦学习算法被提出来用于解决这些问题。</p>
<p>考虑将常见的个性化联邦学习算法分为三类：1）基于全局模型的训练结果进行本地微调，从而实现个性化；2）通过设计一些本地模型更新或训练方法，直接在本地训练个性化模型；3）通过个性化聚合一些已有的模型参数，直接产生出个性化模型。【4】其中第三类方法通常是致力于解决前两类方法中存在的问题。即在全局模型缺乏泛化能力时，可能会为本地模型训练提供缺乏价值的信息。为了更好地捕获有价值的信息来实现个性化的本地模型，通常采用个性化聚合的方法。我们提出的PFLNash算法正属于这类算法。</p>
<p>为了实现全局模型与本地模型的个性化聚合，我们提出了<strong>Adaptive Hybrid Model Aggregation through Nash Bargaining for Personalized Federated Learning</strong>(PFLNash)。PFLNash基于混合模型的思想【5】，提出了一种双模型架构，用于对混合模型权重进行采样，实现了更加可靠的权重自适应调节，并结合Nash Bargaining Game(NBG)完成了权重的高效求解。在PFLNash的框架下，客户端维护其接收的全局模型$\omega_g^i$以及自己的本地模型$\omega_i$，并在每轮本地训练中，使用本地数据集同时更新全局模型和本地模型。同时，本地的两个模型作为player，以它们的梯度信息构造效用函数，执行NBG过程，采样一系列权重信息$\alpha_j^1,\alpha_j^2$。在完成本地更新后，计算权重信息$\alpha_1 &#x3D; \sum\alpha_j^1,\alpha_2 &#x3D; \sum\alpha_j^2$，并将它们归一化到$[0,1]$区间。最终将权重用于参数层的个性化聚合，更新本地参数$\omega_i &#x3D; \alpha_1\omega_i+\alpha_2\omega_g^i$，并将$\omega_i$发送回服务器，完成一次本地训练。关于NBG，它在一定条件下，存在闭式解，同时也能转换为一个容易求解的优化问题【8】，我们可以高效地采样权重信息。而PFLNash利用梯度产生的权重信息完成参数层的聚合，是因为梯度信息可以反映在当前的参数位置，每个模型对整体进步的贡献程度【7】。此外，虽然在采样权重信息的时候全局模型也会被更新，但通常对于个性化联邦学习来说，为了防止客户端漂移问题，local epoch本身就设置得很小（通常是1），所以并不会损失多少全局信息。</p>
<p>总结，我们的工作是：1）提出了一种双模型架构，用于产生梯度信息，并基于梯度信息完成权重采样，实现了权重的自适应调节；2）将Nash Bargainning Game用于从双模型的梯度中计算出权重信息，由于双模型的架构确保了只有两个player，可以对NBG进行高效求解；3）我们从两个不同的领域，挑选了四个数据集，进行了大量的实验，与许多SOTA methods进行了对比，从实验上证明了PFLNash具有较好的性能表现。</p>
<h4 id="Rlated-Works"><a href="#Rlated-Works" class="headerlink" title="Rlated Works"></a>Rlated Works</h4><p>联邦学习</p>
<p>传统联邦学习的标志性算法FedAvg【1】，定义了这一领域的基本算法架构。该框架下，服务器通过将全局模型参数分发给各个数据持有者的本地客户端，由客户端完成本地模型的训练，然后将训练的结果信息，发送给服务器统一进行聚合，重复进行这个过程，最终期望得到一个具有泛化能力的全局模型。然而由于数据的异质性，传统的联邦学习算法常常会面临客户端漂移导致、模型训练无法收敛以及对不同客户端缺乏个性化的问题。</p>
<p>个性化联邦学习（<strong>稍微带一带其他两类方法，可以缩减点前面的，放在后面来；然后主要提一下第三类方法，其中尤其提一下APFL方法</strong>）</p>
<p>当今的个性化联邦学习算法十分多样。首先，最基本的本地微调方法可以通过FedAvg+Local Fine-Tuning来实现。而FedMeta算法基于MAML，首先在联邦学习环境中训练一个易于适应新任务的初始化模型。然后每个客户端用自己的本地数据和少量的训练步骤来对这个初始化模型进行微调，从而得到个性化模型【9】。其次，直接在本地进行个性化模型的训练，例如FedProx在本地损失函数中引入了近端项（Proximal Term），从而在每次本地训练的过程中引入了全局信息，而避免了直接使用全局模型更新本地模型【10】。FedRep则是将模型进行解耦为了公共层与个性化层，前者位于模型的前几层，用于提取通用特征，并在全局范围内更新共享，后者用于对提取的特征进行回归任务或分类，保留在本地进行训练【11】。最后，关于个性化聚合的方法。FedALA算法通过客户端维护一个自适应局部聚合模块（ALA），用于筛选出全局模型中对本地有益的信息，对本地模型进行更新。APFL则是通过定义目标函数（<strong>公式1</strong>），训练一个聚合权重，将全局模型与本地模型聚合得到混合模型。我们的算法也是采用类似的混合模型思想，但我们的权重调节方法更加可靠。</p>
<p>APFL算法的混合权重自适应调整时通过优化问题<strong>公式1</strong>解决的。其中$\omega_i,\omega_g$分别时客户端i的本地模型参数与全局参数，由于该优化问题的损失函数使用的本地数据集，而本地模型通常能够最小化本地损失，粗粒度地聚合后无法进一步让损失函数更小，从而导致权重偏向本地模型。在一些极端情况下，甚至可能出现$\alpha &#x3D; 1$，使得算法退化为本地模型本地训练，全局模型对客户端不起作用的情况。这种情况下，即使本地模型的个性化效果非常好，但全局模型却可能因为各个本地模型在各不相同的数据集下过拟合，而失去较好的泛化能力。尽管APFL算法也支持将混合模型权重视作超参数，直接进行固定，例如设置$\alpha&#x3D;0.8$。但这失去了自适应的效果，较大的$alpha$会导致全局模型中有价值的信息被忽视，而较小的$alpha$又可能让没有价值的信息影响到本地模型。PFLNash则通过Nash Bargaining Game的方法来对权重进行调节，避免了权重调节时的明显偏好，更加的可靠。</p>
<p><strong>公式1</strong>: $\min_\alpha \mathcal{L}_i(D_i,\alpha \omega_i + (1-\alpha)\omega_g)$</p>
<p>参数聚合 via Nash Bargaining Game</p>
<p>Nash Bargaining Game（NBG）旨在为所有的参与者寻找一个公平的协议，使得参与方的利益都尽可能的最大，其解的帕累托最优性可以防止出现对于某一方的偏好，而损害其他方的利益。正因为这种特点，NBG被广泛应用于需要解决不同任务之间的冲突的情况。在多任务学习的场景下，NashMTL通过引入NBG来解决不同任务参数的更新时的聚合问题，并基于CCP方法，将非凸问题转换为凸问题，完成了对NBG的高效求解。而FiarGard for MTL则在其基础上，引入了$\alpha$公平因子，来改进NBG的效用函数，进一步提高了算法的性能。在联邦学习领域，FedRANE将NBG用于各个本地模型的参数更新量聚合$\Delta\omega_1,\Delta\omega_2,…,\Delta\omega_k$，从而得到全局模型的参数更新量$\Delta\omega_g$，用于增强联邦学习算法在非独立同分布的数据集上的表现。而PFLNash是针对本地双模型进行博弈，player只有两个，最终转换为问题时，相较于FedRANE的$k$个player，求解更加高效。</p>
<h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><h5 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h5><p>这一节从形式化的角度考虑我们需要解决的问题。假设我们有$K$个客户端，每个客户端$client_i$持有自己的私有数据集$D_i &#x3D; {x^i_j,y^i_j}_{j&#x3D;1}^{N_i}$，其中$i&#x3D;1,2,…,K$，并且$D_i \sim P_i$。每个客户端持有的数据量$N_i$是不同的，对应数据集服从的分布$P_i$也是不同的。我们的目标是为所有客户端训练一组<strong>个性化模型参数</strong>$\tilde{\omega}_1,\tilde{\omega}_2,…\tilde{\omega}_K$，满足下面的目标函数。</p>
<p><strong>公式2:</strong>${\tilde{\omega}_1,\tilde{\omega}_2,\dots,\tilde{\omega}_K} &#x3D; \operatorname{arg min}\mathcal{G}(\mathcal{L}_1,\mathcal{L}_2,\dots,\mathcal{L}_K)$</p>
<p>其中$\mathcal{L}_i &#x3D; \mathcal{L}(\tilde{\omega}<em>i;D_i),\forall i \in [1,K]$，而$L$是损失函数。$\mathcal{G}(\cdot)$是一种聚合函数，对应$K$个客户端定义为$\mathcal{G}(L_1,L_2,…,L_K) &#x3D; \frac{1}{N}\sum^{K}</em>{i}N_i\mathcal{L_i}$，$N &#x3D; \sum_i^K N_i$。这种聚合方式反映了客户端持有的数据量用于决定其被分配的权重大小。在个性化联邦学习中，考虑全局参数为$\omega_g$，各个模型的本地参数为$\omega_i$，则个性化参数$\tilde{\omega}_i &#x3D; \mathcal{F}(\omega_i,\omega_g)$，其中$\mathcal{F}(\cdot)$反映了个性化模型基于本地模型和全局模型得到。</p>
<h5 id="双模型架构（蒸馏）"><a href="#双模型架构（蒸馏）" class="headerlink" title="双模型架构（蒸馏）"></a>双模型架构（蒸馏）</h5><p>这一节来整体阐释我们的算法结构。PFLNash采用了双模型架构，在训练过程中基于梯度信息对权重进行采样，在每轮本地训练结束后得到聚合权重。</p>
<p>PFLNash算法在每个客户端在本地有两个模型，一个是本地模型$\omega_i$，还有一个是本地维护的全局模型$\omega_g^i$。我们的全局算法结构和传统的联邦学习算法类似。每轮全局训练开始时，由服务器将全局模型$\omega_g$发送到各个客户端，客户端接收后利用全局模型的信息帮助自己进行本地训练，训练完成后返回一个模型给服务器，服务器以客户端的样本数量为依据对各个客户端的模型进行聚合，更新全局模型。不同点在于PFLNash在本地设置的双模型架构。每一轮客户端接收到服务器发送到全局模型后，设置$\omega_g^i &#x3D; \omega_g$，而不直接将全局模型的参数赋值给本地模型。紧接着开始本地训练，训练的过程中使用SGD，对于采集到的样本$b_j$，执行$\omega_i \leftarrow \omega_i - \eta\nabla\mathcal{L}(\omega_i;b_j)$以及$\omega_g^i \leftarrow \omega_g^i - \eta\nabla\mathcal{L}(\omega_g^i;b_j)$，在这个过程中我们将梯度信息保留下来，更新梯度信息集合$\mathcal{W} &#x3D; \mathcal{W} \cup {(l_1^j,l_2^j)}$，其中$l_1^j &#x3D; \nabla\mathcal{L}(\omega_i;b_j),l_2^j &#x3D; \nabla\mathcal{L}(\omega_g^i;b_j)$。定义一个集合算子$\phi(\cdot)$，用于基于梯度信息生成权重信息，在本地训练结束后执行${\alpha_1,\alpha_2} &#x3D; \phi(\mathcal{W})$。然后进行个性化聚合，更新本地模型$\omega_i \leftarrow \alpha_1\omega_i + \alpha_2\omega_g^i$，最终将本地模型发送给客户端。</p>
<h5 id="基于Nash-Bargaining-Game的自适应权重"><a href="#基于Nash-Bargaining-Game的自适应权重" class="headerlink" title="基于Nash Bargaining Game的自适应权重"></a>基于Nash Bargaining Game的自适应权重</h5><p>这一节用于说明权重的生成，即算子$\phi(\cdot)$的实现，以及其如何具有自适应的效果。从上一节的梯度信息集合$\mathcal{W}$可以看出，我们实际上先采样了一系列梯度。我们要做的就是基于这些梯度，进行Nash Bargaining Game(NBG)，得到权重信息。首先我们对NBG的相关参数做定义。令协议空间为$d$维度的球状空间$B_r$，其中$r$是该空间的半径，而维度$d$实际上对应了我们样本中梯度的维度。此外，定义歧义点$disagreement\ point &#x3D; 0$，无法达成一致本就是少有的情况，这样设置可令对应梯度样本不对最终权重产生影响。接下来，假设有一组梯度样本$(l_1,l_2)\in \mathcal{W}$，考虑两个players进行NBG，定义它们的效用函数分别为$u_1(l) &#x3D; l^T l_1,u_2(l) &#x3D; l^Tl_2$。NBG的目的是解决形如<strong>公式3</strong>的问题。</p>
<p><strong>公式3:</strong>$\arg\max_l \log[u_1(l)] + \log[u_2(l)]$</p>
<p>由于$disagreement\ point &#x3D; 0$，根据Nash Bargaining Solution(NBS)的性质：$l^Tl_1,l^Tl_2 &gt; 0$，所以效用函数是随着the norm of $l$单调递增的。那么唯一的最优解一定在协议空间$B_r$的边界上。所以根据帕累托最优性假设，问题的解$l^*$应该满足$||l^*||^2_2 &#x3D; r^2$。于是根据KKT条件，我们可以将<strong>公式3</strong>的目标函数改写为<strong>公式4</strong>。并且可以对目标函数求导，得到限制条件<strong>公式5</strong>。</p>
<p><strong>公式4:</strong>$\arg\max_l \log[u_1(l)] + \log[u_2(1)] - \lambda(||l||^2_2 - r^2)$</p>
<p><strong>公式5:</strong>$\frac{1}{l^T l_1}l_1+\frac{1}{l^T l_2}l_2  &#x3D; 2\lambda l$</p>
<p>由于在最优解处，导数的方向一定是径向的，即$(\frac{1}{l^T l_1}l_1+\frac{1}{l^T l_2}l_2) || \lambda l$。We can expand Eq.(5) for the inconsistent deviation with linear independent assignment，不妨令$\lambda &#x3D; 1$，则得到形如$l^T l_1 &#x3D; \frac{1}{\alpha_1},l^T l_2 &#x3D; \frac{1}{\alpha_2}$的式子。令$G$为$d \times 2$的矩阵，其中第一、二列分别对应$l_1,l_2$。记$\alpha &#x3D; (\alpha_1,\alpha_2)^T$，其中$\alpha1,\alpha2$即梯度样本$(l_1,l_2)$产生的权重结果。为了得到的这个结果，此时我们的目标等价转换为求解$G^TG\alpha &#x3D; \frac{1}{\alpha}$。我们可以通过将其转换为凸优化问题，进行高效求解【8】。定义$\beta_1 &#x3D; l_1^T G \alpha,\beta_2 &#x3D; l_2^T G \alpha$，目标等价于$\alpha&#x3D;(\alpha_1,\alpha_2)^T$满足$\alpha_1 &#x3D; 1&#x2F;\beta_1,\alpha_2 &#x3D; 1&#x2F;\beta_2$，即$\log( \alpha_1 + \beta_1 )&#x3D; 0, \log( \alpha_2+\beta_2) &#x3D; 0$。于是我们记$\psi_1(\alpha) &#x3D; \alpha_1 + \beta_1 \ge 0,\psi_2(\alpha) &#x3D; \alpha_2 + \beta_2 \ge 0$，以及$\psi(\alpha) &#x3D; \psi_1(\alpha) + \psi_2(\alpha)$。考虑如<strong>公式6</strong>的优化问题作为原问题的近似。</p>
<p><strong>公式6:</strong>$\min_\alpha\psi(\alpha) \ \ \ \ \ \ \ s.t. \ \psi_i(\alpha) \ge0,\alpha_i\ge0,i &#x3D; 1,2$</p>
<p>该优化的约束条件是凸且线性的。而在满足$\psi_1(\alpha),\psi_2(\alpha) \ge 0$时，进一步最小化$\beta_1 + \beta_2$，可以得到关于原问题的一个非常准确的估计解，于是引入它们得到原问题的最终优化近似<strong>公式7</strong>。</p>
<p><strong>公式7:</strong>$\min_\alpha\psi(\alpha) + \beta_1 + \beta_2 \ \ \ \ \ \ \ s.t. \ \psi_i(\alpha) \ge0,\alpha_i\ge0,i &#x3D; 1,2$</p>
<p><strong>这里可能还要改一下</strong><br>上面的优化问题是一个凸-凹优化，其中的凹项是$\psi(\alpha)$。我们可以使用CCP方法来对其进行展开，转换为一个近似的凸问题。考虑多次迭代，在每次迭代过程使用一阶近似 $\tilde{\psi}_\tau(\alpha) &#x3D; \psi(\alpha^\tau)^T+\nabla\psi(\alpha^\tau)(\alpha-\alpha^\tau)$ 对 $\psi(\alpha)$进行展开。 根据 CCP（凸凹过程）的既有理论，该方法迭代地将序列 ${\alpha^\tau}_\tau$ 收敛到原始非凸问题（方程 (7)）的一个临界点。最终，我们得到一个可通过序列优化解决的凸优化目标。</p>
<p>最终优化的结果即我们基于一个梯度样本得到的权重信息。对一次本地训练产生的每组梯度$(l_1^j,l_2^j) \in \mathcal{W}$通过上述过程得到权重$\alpha_1^j,\alpha_2^j$。令最终的权重为$\alpha_1 &#x3D; \sum \alpha_1^j,\alpha_2 &#x3D; \sum \alpha_2^j$，做归一化处理$\alpha_1 \leftarrow \alpha_1&#x2F;(\alpha_1+\alpha_2)$，以及$\alpha_2 &#x3D; 1 - \alpha_1$。将整个过程记做算子$\phi$，即${\alpha_1,\alpha_2} &#x3D; \phi(\mathcal{W})$。</p>
<h5 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h5><p>At the beginning of each round of global training, the server sends the global model $\omega_g$ to each client. After receiving it, the clients use the information of the global model to help their local training. After training is completed, a model is returned to the server. The server aggregates the models of each client based on the number of samples each client has, updating the global model. The difference lies in the dual-model architecture set up locally by PFLNash.</p>
<h5 id="Dataset-and-Model"><a href="#Dataset-and-Model" class="headerlink" title="Dataset and Model"></a>Dataset and Model</h5><p>我们使用的数据集来自两个领域，包括MNIST【】, Cifar100【】, SogouNews【】，其中前两个数据集来自于CV(Computer Vision)领域，剩下的一个来自于NLP(Nature Language Process)领域。</p>
<p>MNIST（Modified National Institute of Standards and Technology）数据集是手写数字识别领域的经典数据集。它包含60,000张训练图像和10,000张测试图像，每张图像为28x28像素的灰度图，表示0到9的手写数字。</p>
<p>CIFAR-100是一个用于图像分类任务的数据集，由100个类别组成，每个类别包含600张32x32像素的彩色图像，其中500张用于训练，100张用于测试。</p>
<p>Sogou News Dataset 是由 SogouCA 和 SogouCS 新闻语料库构成的数据集，其拥有5个类别（体育、财经、娱乐、汽车、科技）共计2,909,551 篇文章，每个类别均包含90,000个训练样本和12,000个测试样本。</p>
<p>我们对这些数据集使用狄利克雷划分的方法【】，为每个客户端分配本地数据。在这种方式下，每种类别的数据被划分到一个客户端的概$p~Dir(\beta)$，其中$Dir(\alpha)$是狄利克雷分布。当参数$\alpha \rightarrow 0$时一种类别的数据几乎会全部分配给某一个客户端，其它客户端只能分到少量该类数据。客户端间的数据呈现极度的非独立同分布性。我们在使用中设置$\alpha &#x3D; 0.1$，对应各客户端持有的数据数量以及类型均相差很大，构建了数据异质性的测试环境。</p>
<p>对于MINST与Cifar100，使用的模型是CNN，与McMahan【】使用的相同，包括两个5×5的卷积层，分别是32通道和64通道；以及一个512单元的全连接层，和一个输出层。对于SogouNews，使用的模型是fastText，包括一个嵌入层，一个线性的隐藏层以及输出层。我们根据任务的类别数调整模型的输出层，分别为10, 100, 5对应上面三个数据集。</p>
<p>进行实验的主机环境使用的操作系统是Ubuntu 22.04，使用了一个AMD EPYC 7663 @3.50GHz CPU，一个24GB显存的RTX4090 GPU，以及80GB RAM。我们选择了联邦学习领域的10个SOTA baselines，与PFLNash进行对比，包括FedProx【】、FedPer【】、FedAvg【】、FedAS【】、FedALA【】、Ditto【】、FedPAC【】、MOON【】、FedRep【】、FedPHP【】、FedFomo【】、pFedME【】。其中Ditto【】被我们选做主要的baseline，因为我们的双模型框架与其相似，均在客户端维护了全局模型和本地模型，且会并行地对它们进行训练。</p>
<p>关于实验的设置，我们参考了一些经典的研究。根据pFedMe【】，我们将客户端的数量设置为20并设置客户端的参与率$\rho &#x3D; 1$。根据FedAvg【】，我们将本地模型训练的epoch数量设置为1，并将batch size设置为10。根据FedALA【】，优化器的学习率被我们设置为$0.005$并且以每轮0.99的比例进行衰减。global epoch是200轮。在MNIST与Cifar100的测试上，PFLNash的采样频率$p &#x3D; 1$，在SogouNews的测试上，PFLnash的采样频率$P &#x3D; 3$。</p>
<h5 id="Evaluation-metrics"><a href="#Evaluation-metrics" class="headerlink" title="Evaluation metrics"></a>Evaluation metrics</h5><p>Personalized Accuracy:我们将满足客户端的个性化需求作为我们的首要目标，将PFLNash划入pFL的范畴。因此我们对10个baselines都使用了这种个性化准确率。根据pFedMe【】，该指标对应的是最好的本地模型的平均测试准确率。</p>
<p>Generalization Accuracy:我们将确保全局模型的性能看作我们的次要目标，使用PFLNash与Ditto在泛化准确率上进行了对比。该指标对应的是全局模型在整个测试数据集上的准确率，可以表征全局模型的泛化能力。</p>
<p>Communication Rounds:在有限的通信轮次内，模型能达到的性能是联邦学习中的一个核心问题。因此我们模型的准确率随通信轮次变化的情况，作为我们的一个评估项。</p>
<h3 id="修改"><a href="#修改" class="headerlink" title="修改"></a>修改</h3><p>11.20老师的修改建议</p>
<ol>
<li><p>为了说明论文的问题背景引入插图（联邦学习的基本场景、客户端漂移的问题）</p>
</li>
<li><p>相关工作部分如果需要公式的话，则放在正文中</p>
</li>
<li><p>相关工作部分需要再补充一些，目前太少了</p>
</li>
<li><p>双模型架构的时候引入形象的图片</p>
</li>
<li><p>关于NBG的求解从转换到问题的位置，切割成两部分</p>
</li>
<li><p>伪代码改得更加的简洁（分为客户端与服务器两部分）</p>
</li>
<li><p>适当的多编一些子标题</p>
</li>
<li><p>介绍和相关工作尽量不要出现数学公式</p>
</li>
<li><p>introduction和related works都会提到一些别人的工作，但是introduction应该比related work在一个高一点的层面进行描述</p>
</li>
<li><p>不要使用最外面的框图</p>
</li>
<li><p>添加上传下载的图示（个性化联邦的过程）</p>
</li>
<li><p>突出两个问题</p>
</li>
<li><p>字体相同，大小相同（与正文）；画图的时候放到百分百之百（画布）</p>
</li>
</ol>
<p>我自己的修改想法：</p>
<ol>
<li>伪代码部分的格式需要规范</li>
<li>摘要部分需要修改，必须言明自己最后取得的成果（<strong>摘要里面不能使用数学公式与任何符号</strong>）</li>
<li>整理实验的所需数据：各个对比项的表格；不同采样率下的对比；全局准确率</li>
<li>英文校准</li>
<li>introduction里要落到问题</li>
</ol>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>联邦学习在面对各个客户端的本地数据异质性较强的时候主要面临两个问题。第一个问题是客户端漂移，即全局模型无法收敛导致最终模型性能较差，第二个问题是全局模型与本地模型最优解冲突，即全局模型试图在所有参与方数据上表现平均使得在某些客户端上性能较差。此外这两种问题之间天然地存在一定程度的冲突，现有的许多方法只能重点考虑其中一个问题。在这个论文中，我们提出了PFLNash(Personalized Federated Learning via Nash Bargaining Game)，一种基于Nash Bargaining Game的联邦学习算法，作为同时考虑两种问题方法，目标是实现客户端的个性化训练增强本地模型性能，同时确保全局模型的泛化性能。首先，PFLNash在客户端提出了一种双模型框架，并行地训练本地模型和全局模型，并按照一定的频率采样梯度信息。其次，PFLNash使用了一种启发式的方法，根据梯度信息进行Nash Bargaining Game产生权重信息，用于本地模型与全局模型的聚合，并将聚合后的模型作为新的本地模型，用于下一轮训练以及传递给客户端。最后，因为Nash Bargaining Solution天然地满足帕累托最优性，所以在数据异质性的环境下，这种方法既可以帮助本地模型衡量全局信息的价值从而更好地实现自己的个性化，又可以稳定全局模型的训练，增强全局模型的性能。通过与10个基线在多个非独立同分布的数据集上进行实验对比个性化准确率，以及同主要基线对比泛化准确率，PFLNash均取得了显著的提升，我们验证了该方法的有效性。</p>
<h4 id="graph1"><a href="#graph1" class="headerlink" title="graph1"></a>graph1</h4><h4 id="Introduction-new"><a href="#Introduction-new" class="headerlink" title="Introduction(new)"></a>Introduction(new)</h4><p><strong>思路：</strong>1）背景；2）面对的挑战（两类）；3）解决挑战的方法（两类），重点关注第二类，因为我们的算法也解决第二类（但实际上附带着在第一类挑战上也取得了较好的效果）</p>
<p>联邦学习（Federated Learning, FL）是一种分布式机器学习方法，旨在保护数据隐私的同时训练模型【】。其核心思想是将模型训练分散到各个数据持有者的本地设备上，在使用对应数据训练模型的同时避免本地设备泄漏自己的数据，从而在保护各个设备数据隐私的前提下完成模型训练。这种方法特别适用于跨设备、组织的数据协同场景，并广泛应用于移动设备、医疗健康、金融等领域。</p>
<p>传统的联邦学习通常考虑各个客户端的数据满足独立同分布的性质，但实际情况通常是各个客户端端数据具有异质性，即：数据非独立同分布，数据数量相差很大，数据类别分布不平衡，具有严重的噪声标签等情况。数据异质性下联邦学习面临的挑战主要有两类：1）客户端漂移【】：在本地训练过程中，每个客户端的本地模型会朝着其本地最优解的方向更新，从而“漂移”或偏离全局最优解的方向；2）全局模型与本地模型最优解冲突【】：由于全局模型试图在所有参与方的数据上表现均衡，使得在某些客户端上模型性能较差，无法满足个性化需求。</p>
<p><strong>想办法将两种问题用图片可视化的展示出来</strong>。</p>
<p>为了解决客户端漂移问题，联邦学习通常更加关注稳定全局模型的训练并确保收敛。可以用面向服务器方解决问题还是面向客户端方解决问题的标准，将常见的方法分为三类：1）约束本地模型的更新，防止客户端在本地数据上过度学习。例如FedProx【】和SCAFFOLD【】2）服务器端优化，面对出现客户端漂移的本地模型采用某些策略进行聚合，而不是直接加权平均，例如FedAdam【】。3）同时改进客户端与服务器的策略，例如FedDyn【】和FedRANE【】。但总的来说，这些方法主要关注全局模型性能，通常不具备满足客户端个性化需求的能力。（<strong>相关工作中提到对应的方法之后，可以更细地描述它们的缺陷</strong>）</p>
<p>为了满足个性化需求，联邦学习通常更加关注如何将全局模型的信息更好地用于本地模型的个性化更新。常见的方法主要有三类，包括：1）基于全局模型的训练结果进行本地微调，从而实现个性化，例如FedRep【】和Per-FedAvg【】等；2）通过设计本地模型的更新或训练方法，直接在本地训练个性化模型，例如pFedMe【】；3）通过个性化聚合模型参数，直接产生出个性化模型，例如FedALA【】和FedAMP【】等。前两类方法在全局模型泛化能力较弱时，可能会为本地模型训练提供缺乏价值的信息，从而阻碍本地模型的个性化更新。第三类方法则是为了更好地捕获全局模型中有价值的信息来帮助本地模型更新，它们通常采用个性化聚合的方式。但总的来说，这三种类型的方法更加关注个性化实现，会忽略全局模型的性能。</p>
<p>（启发；可以删减一下；可以再看看一个更好的模板）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Nash Bargaininng Game(NBG)广泛应用于多方参与协商冲突解决方案的情况。在机器学习中</span><br><span class="line"></span><br><span class="line">在多任务学习领域，Nash-MTL【】将Nash Bargaining Game(NBG)用于解决各任务的梯度冲突问题。其中每个任务被视为一个参与协商的player，它们需要就模型参数的更新方向达成一致，这个过程被建模为NBG。对其求解得到的Nash Bargaining Solution天然地满足了帕累托最优性，从而保证了在不偏袒某个任务的前提下，尽可能地让各个任务得到较好的满足。我们受此启发，提出了一种自适应权重计算方法，用于实现个性化聚合。</span><br></pre></td></tr></table></figure>

<p><strong>我们的架构与Ditto的不同点是并行训练时的损失函数，以及客户端返回给服务器的模型</strong>！<br><strong>别乱用个性化模型的描述</strong>！<br><strong>后面描述统一为对权重采样，而非对梯度采样</strong>！</p>
<p>为了让本地模型能够更有效地利用全局模型中有价值的信息进行自己的个性化更新，同时确保全局模型的泛化能力，我们提出了<strong>Personalized Federated Learning via Nash Bargaining Game</strong>(PFLNash)。PFLNash提出了一种本地双模型的框架。首先，在这种框架下客户端会维护一个全局模型的副本以及自己的本地模型，每轮通信之后全局模型参数会对副本进行更新。其次，在进行本地训练时副本模型与本地模型一同训练。然后，过程中产生的梯度信息会被收集起来，用于进行Nash Bargaining Game(NBG)，采样一系列权重信息。最终，PFLNash将采集到的权重样本累积求和并进行归一化处理，得到聚合权重用来将全局模型与本地模型聚合，更新本地模型。</p>
<p>这篇论文的主要贡献列举如下：</p>
<ol>
<li>双模型架构：我们提出了客户端的本地模型以及全局模型副本同步更新的方式，用于产生梯度信息，并基于它们采样得到权重信息。这些权重信息可以衡量全局模型对于本地模型更新的价值，帮助本地模型进行个性化聚合。</li>
<li>自适应权重：我们将梯度信息生成权重的过程建模为NBG，并进行了高效地求解。这种方法产生的自适应权重具有帕累托最优性，可以在实现个性化的同时保证全局模型的泛化能力。（**这里其实可以在实验的时候展开分析一下，因为不会偏好又尽可能的确保各方最好…**）</li>
<li>实验验证：我们从Computer Vision和Nature Language Process领域挑选了相关数据集进行实验，与10个SOTA methods进行了个性化准确率的对比，PFLNash均取得了最好的表现，在Cifar100和SogouNews上分别超过了最好的方法5.70%和0.44%。并且在Cifar100上，PFLNash的个性化准确率超过了基线模型12.99%，泛化准确率超过了基线模型6.62%；</li>
</ol>
<p>（<strong>优势不太明显？问问老师的意见</strong>）<br>（<strong>如果选择了基准模型，是前面就要说，还是实验部分再说？</strong>）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">//这一部分或许可以放到实验分析的时候使用</span><br><span class="line"></span><br><span class="line">PFLNash基于混合模型的思想【5】，提出了一种双模型架构，用于对混合模型权重进行采样，实现了更加可靠的权重自适应调节，并结合Nash Bargaining Game(NBG)完成了权重的高效求解。在PFLNash的框架下，客户端维护其接收的全局模型$\omega_g^i$以及自己的本地模型$\omega_i$，并在每轮本地训练中，使用本地数据集同时更新全局模型和本地模型。同时，本地的两个模型作为player，以它们的梯度信息构造效用函数，执行NBG过程，采样一系列权重信息$\alpha_j^1,\alpha_j^2$。在完成本地更新后，计算权重信息$\alpha_1 = \<span class="built_in">sum</span>\alpha_j^1,\alpha_2 = \<span class="built_in">sum</span>\alpha_j^2$，并将它们归一化到$[0,1]$区间。最终将权重用于参数层的个性化聚合，更新本地参数$\omega_i = \alpha_1\omega_i+\alpha_2\omega_g^i$，并将$\omega_i$发送回服务器，完成一次本地训练。关于NBG，它在一定条件下，存在闭式解，同时也能转换为一个容易求解的优化问题【8】，我们可以高效地采样权重信息。而PFLNash利用梯度产生的权重信息完成参数层的聚合，是因为梯度信息可以反映在当前的参数位置，每个模型对整体进步的贡献程度【7】。此外，虽然在采样权重信息的时候全局模型也会被更新，但通常对于个性化联邦学习来说，为了防止客户端漂移问题，<span class="built_in">local</span> epoch本身就设置得很小（通常是1），所以并不会损失多少全局信息。</span><br><span class="line"></span><br><span class="line">总结，我们的工作是：1）我们提出了一种双模型框架，用于产生梯度信息，并基于梯度信息完成权重采样，实现了权重的自适应调节；2）将Nash Bargainning Game用于从双模型的梯度中计算出权重信息，；3）我们从两个不同的领域，挑选了四个数据集，进行了大量的实验，与许多SOTA methods进行了对比，从实验上证明了PFLNash具有较好的性能表现。</span><br><span class="line"></span><br><span class="line">！！！！！！！！！！自适应权重：我们将梯度信息生成权重的过程建模为NBG，并进行了高效地求解。对应的权重信息满足帕累托最优性，能够给出有利于双方的更新方向（即满足个性化，又解决客户端漂移，还有利于各方长远合作...这样想怪好的）！！！！！！！！！！</span><br></pre></td></tr></table></figure>

<h4 id="related-works-new"><a href="#related-works-new" class="headerlink" title="related works(new)"></a>related works(new)</h4><p>两三句话（再分ABC）；要有信息量，具体，不要太泛泛<br>每个Section下有引导性的话</p>
<p>每一段话都采用总分结构</p>
<h5 id="Federated-Learning"><a href="#Federated-Learning" class="headerlink" title="Federated Learning"></a>Federated Learning</h5><p>在数据异质性的情况下，训练过程会发生客户端漂移，导致全局模型收敛困难、性能下降。现有的研究主要从服务器或客户端着手，对全局模型或本地模型进行一些处理，来解决这个问题。但这些方法通常无法满足客户端的个性化需求。</p>
<p>FedProx【】通过在本地模型的损失函数中引入近端项约束本地模型与全局模型的偏差，并使用超参数$\mu$来控制这个约束的强度。SCAFFOLD【】维护了控制变量，用于在本地更新时校正本地梯度方向使其接近全局梯度方向。FedDF【】通过蒸馏，将本地模型在私有数据上学习到的知识对齐到全局模型的知识空间，缓减客户端漂移。FedAdm【】在服务器端，引入了自适应学习率，可以基于本地模型的更新量动态调整全局模型的更新。FedDyn通过一个动态的正则项，不仅约束本地模型还在服务器端进行梯度校正，从而在理论上保证收敛到全局最优。</p>
<p>但是，这些方法主要关注全局模型的性能，希望学习到泛化的能力，在所有客户端的数据集上表现出较好的平均能力。但这种平均的表现意味着持有数据集比较小众的客户端可能被全局模型所反感，无法关注到客户端的个性化需求。</p>
<h5 id="Personalized-Federated-Learning"><a href="#Personalized-Federated-Learning" class="headerlink" title="Personalized Federated Learning"></a>Personalized Federated Learning</h5><p>为了实现客户端的个性化需求，有许多研究使用了个性化联邦学习。其中一类是个性化聚合的方法，它们更加关注如何辨别全局信息的价值，并根据具体的情况给出个性化的聚合方案。而其他方法通常无法针对性地辨别有价值的信息，使得所有全局信息均会对本地模型产生相当的影响。</p>
<p>Per-FedAvg【】设置服务器端的目标不是最小化全局模型的损失而是最小化客户端使用全局模型进行一步本地更新后的期望损失。这是一种元学习的方法，能够得到一个客户端可以快速进行个性化微调的全局模型。LG-FedAvg【】将模型划分为全局层和本地层，只有全局模型参与服务器端的聚合，本地层由客户端完全保留，不进行上传和聚合。这种方法将来自全局的通用知识和本地的通用知识解耦，让模型的最后几层作为本地个性化部分。FedRep【】要求客户端轮流训练分类器和特征提取器，训练前者时固定全局的特征提取器，训练后者时固定个性化的分类器。这种方法可以训练一个对所有客户端都通用的特征表示，同时让每个客户端都拥有与本地数据适配的个性化分类器。Ditto【】在客户端并行地与优化全局模型和个性化模型，全局模型的优化与FedAvg【】相同，个性化模型优化时在损失函数中添加了与全局模型的距离限制，由超参数$\lambda$控制约束强度。这种方法在满足个性化需求的同时具有较好的公平性和鲁棒性。但是总的来说，这些方法共有的缺点是，所有的全局信息都会对本地模型产生影响，无法针对性地考虑有价值的信息。</p>
<p>就个性化聚合的方法而言，FedAMP【】会在服务器存储每一轮客户端上传的本地模型，并通过注意力机制为每个客户端计算一个个性化的聚合模型，分发给对应的客户端进行训练。这种方法会为每个客户端动态生成一个个性化的训练起点，满足客户端的个性化需求。但是这种方法服务器端的开销较大，因为需要维护多个模型。FedALA【】则是在客户端通过ALA(Adaptive Local Aggregation)模块，对全局模型和本地模型的每一层计算一个聚合权重，实现个性化聚合。这种方法可以让融合后的模型在客户端的数据上损失尽可能小，让本地模型从全局模型中提取有价值的信息。</p>
<p>总的来说，个性化聚合的方法可以更好地利用全局模型中的信息，帮助本地模型实现个性化。我们的方法也属于这种类型。</p>
<h5 id="Nash-Bargaining-Game"><a href="#Nash-Bargaining-Game" class="headerlink" title="Nash Bargaining Game"></a>Nash Bargaining Game</h5><p>NBG旨在为所有的参与者寻找一个公平的协议，使得参与方的利益都尽可能的最大，其解的帕累托最优性可以天然防止由于对某一方的偏好而损害其他方的利益。因此NBG被广泛应用于需要解决冲突的场景。</p>
<p>在多任务学习领域中，Nash-MTL【】将每个任务视为一个参与协商的player，它们需要就模型参数的更新量达成一致，这个过程被建模为NBG，然后将问题的求解转换为了一个优化问题，最终基于CCP【】方法将非凸问题转换为凸问题，完成了对NBG的高效求解。对其求解得到的Nash Bargaining Solution天然地满足了帕累托最优性，从而保证了在不偏袒某个任务的前提下，尽可能地让各个任务得到较好的满足。类似的，FiarGard for MTL【】在Nash-MTL【】的基础上，引入了$\alpha$公平因子，对各个任务的效用函数做了改进，使得协商过程更具有公平性，改进了算法的表现。在联邦学习中，FedRANE【】将NBG用于聚合本地模型参数的更新量引导全局模型向更优的方向收敛，从而解决由于客户端之间数据不一致引起的客户端漂移问题。</p>
<p>但类似的方法通常是用于参数更新量的聚合，而不能直接应用于模型本身的聚合。我们采用了一种启发式的方法，来解决这个问题并取得了较好的效果。</p>
<h4 id="Method-new"><a href="#Method-new" class="headerlink" title="Method(new)"></a>Method(new)</h4><h5 id="Problem-Statement-1"><a href="#Problem-Statement-1" class="headerlink" title="Problem Statement"></a>Problem Statement</h5><p>这一节从形式化的角度考虑我们需要解决的问题。假设我们有$K$个客户端，每个客户端$client_i$持有自己的私有数据集$D_i &#x3D; {x^i_j,y^i_j}_{j&#x3D;1}^{N_i}$，其中$i&#x3D;1,2,…,K$，并且$D_i \sim P_i$。每个客户端持有的数据量$N_i$是不同的，对应数据集服从的分布$P_i$也是不同的。我们的目标是为所有客户端训练一组<strong>个性化模型参数</strong>$\tilde{\omega}_1,\tilde{\omega}_2,…\tilde{\omega}_K$，满足下面的目标函数。</p>
<p><strong>公式2:</strong>${\tilde{\omega}_1,\tilde{\omega}_2,\dots,\tilde{\omega}_K} &#x3D; \operatorname{arg min}\mathcal{G}(\mathcal{L}_1,\mathcal{L}_2,\dots,\mathcal{L}_K)$</p>
<p>其中$\mathcal{L}_i &#x3D; \mathcal{L}(\tilde{\omega}<em>i;D_i),\forall i \in [1,K]$，而$L$是损失函数。$\mathcal{G}(\cdot)$是一种聚合函数，对应$K$个客户端定义为$\mathcal{G}(L_1,L_2,…,L_K) &#x3D; \frac{1}{N}\sum^{K}</em>{i}N_i\mathcal{L_i}$，$N &#x3D; \sum_i^K N_i$。这种聚合方式反映了客户端持有的数据量可以影响其重要程度，服务器对所有本地模型的聚合通常也采用这种形式。对于个性化聚合的方法，考虑全局参数为$\omega_g$，各个模型的本地参数为$\omega_i$，则个性化参数$\tilde{\omega}_i &#x3D; \mathcal{F}_i(\omega_i,\omega_g)$，其中$\mathcal{F}_i(\cdot)$反映了针对不同客户端，使用不同的方式对全局模型和本地模型进行聚合。PFLNash则是采用了一种自适应权重的方法。</p>
<h5 id="PFLNash"><a href="#PFLNash" class="headerlink" title="PFLNash"></a>PFLNash</h5><p>这一节对PFLNash进行整体阐释，分别就服务器与客户端的工作流程进行了说明，并在最后就我们工作中的关键部分进行了说明。</p>
<p>对于服务器而言，每轮全局训练开始时它将全局模型$\omega_g$发送到各个客户端，客户端接收后利用全局模型的信息帮助自己进行本地训练，训练完成后返回本地模型$\omega_i$给服务器，服务器以客户端的样本数量为依据对各个客户端的模型进行聚合更新全局模型，$\omega_g \leftarrow \frac{1}{N}\sum^{K}_{i}N_i\omega_i$。</p>
<p>对于客户端而言，每一轮本地训练开始时。首先它接收到服务器发送的全局模型，设置$\omega_g^i &#x3D; \omega_g$，而不直接将全局模型的参数赋值给本地模型。紧接着开始本地训练，对每一个训练样本$b_j$，执行$\omega_i \leftarrow \omega_i - \eta\nabla\mathcal{L}(\omega_i;b_j)$以及$\omega_g^i \leftarrow \omega_g^i - \eta\nabla\mathcal{L}(\omega_g^i;b_j)$。在这个过程中我们按照采样频率$p$将梯度信息保留下来，更新梯度信息集合$\mathcal{W} &#x3D; \mathcal{W} \cup {(l_1^j,l_2^j)}$，其中$l_1^j &#x3D; \nabla\mathcal{L}(\omega_i;b_j),l_2^j &#x3D; \nabla\mathcal{L}(\omega_g^i;b_j)$。定义用于权重的集合算子$\phi(\cdot)$，在本地训练结束后执行${\alpha_1,\alpha_2} &#x3D; \phi(\mathcal{W})$。最后进行个性化聚合，更新本地模型$\omega_i \leftarrow \alpha_1\omega_i + \alpha_2\omega_g^i$，并将本地模型发送给客户端。</p>
<p>其中$\omega_g^i$和$\omega_i$是双模型架构中的核心所在，我们会在下一节中讨论它们的作用与优点；而集合算子$\phi(\cdot)$则是基于Nash Bargaining Game定义，我们在后续章节中会依次讨论Game的建模以及求解。</p>
<h5 id="双模型架构"><a href="#双模型架构" class="headerlink" title="双模型架构"></a>双模型架构</h5><p>第一个模型是什么？作用是什么？第二个模型是什么？作用是什么？为什么要使用两个模型？基于这两种模型我们使用的是启发式的方法</p>
<p>为了从全局模型和本地模型中生成自适应的权重，PFLNash采用了一种双模型框架。即在每个本地客户端$client_i$维护两个模型，分别是全局模型的副本$\omega_g^i$以及自己的本地模型$\omega_i$用于在训练过程中基于全局信息和本地信息产生梯度信息，并基于梯度信息得出聚合权重。</p>
<p>接下来对双模型框架的细节进行描述。第一个模型$\omega_g^i$用于接收服务器下发的全局模型$\omega_g$，其中的模型参数反映了当前的全局信息。第二个模型$\omega_i$作为本地模型，其模型参数反映了客户端的个性化信息，因为它不会直接按照全局模型的参数进行更新。对于两个模型的具体用处如下所示。</p>
<p>首先，在本地训练的过程中，对于训练样本$b_j$除了执行$\omega_i \leftarrow \omega_i - \eta\nabla\mathcal{L}(\omega_i;b_j)$，PFLNash还会同步执行$\omega_g^i \leftarrow \omega_g^i - \eta\nabla\mathcal{L}(\omega_g^i;b_j)$。这种额外对全局模型进行训练的方式与Ditto【】类似。但是它只是为了保证个性化的同时，同步地帮助全局模型进行训练，而PFLNash则是为了生成自适应的权重，这是本质上的不同。具体来说，在PFLNash中，每使用$p$个训练样本，客户端就会将$\omega_i$和$\omega_g^i$在第$p$个训练样本$b_j$上产生的梯度信息$(l_1^j,l_2^j)$保存在集合$\mathcal{W}$中。其中$p$是控制权重采样频率的超参数。</p>
<p>其次，$\omega_i$和$\omega_g^i$作为Nash Bargaining Game的两个players，它们会用自己产生的梯度信息$l_1^j$和$l_2^j$构建的适当的效用函数，进行协商。协商的结果可以转换为一个对梯度进行聚合的权重$\alpha_1^j,\alpha_2^j$。第三，我们考虑单个模型的梯度信息可以反映其对于聚合后的模型的贡献度。这属于一种启发式的方法，类似的思想在FedAdp【】，FedPG和FedLAG【】中均有体现。例如FedAdp通过计算本地梯度向量与全局梯度向量之间的夹角来衡量节点对全局模型的贡献。</p>
<p>最后，PFLNash在本地训练结束的时候，对基于梯度信息产生的一系列权重做归一化处理得到$\alpha_1,\alpha_2$，对模型进行个性化聚合$\omega_i \leftarrow \alpha_1\omega_i+\alpha_2\omega_g$。聚合后的模型会同时作为个性化模型以及返回客户端的模型。</p>
<p>关于双模型框架的优点。第一，权重$\alpha_1,\alpha_2$作为Nash Bargaining Solution的等价形式，天然地满足帕累托最优性。其不会对全局模型副本$\omega_g^i$或本地模型$\omega_i$出现偏袒，并尽可能地让聚合后的模型性能在本地数据集上优于参与聚合的模型。这意味着客户端可以更好地根据全局信息对其个性化需求的价值作出对应的处理。第二，尽管聚合后的模型是在本地模型上性能更优，但是本地客户端维护的$\omega_g^i$会完全继承全局模型的信息，并对其进行训练，结合帕累托最优性，最终我们将聚合后的模型返回给客户端，并不会与服务器期望得到的模型有太大的差异。相较于大多数个性化联邦学习算法仅仅利用全局信息为个性化服务，PFLNash可以保证较优的全局模型性能。</p>
<p>在接下来的章节中我们会依次探讨Nash Bargaining Game的建模，以及如何求解得到最终的权重。整体的算法流程则放在最后进行说明。</p>
<h5 id="基于Nash-Bargaining-Game的自适应权重-1"><a href="#基于Nash-Bargaining-Game的自适应权重-1" class="headerlink" title="基于Nash Bargaining Game的自适应权重"></a>基于Nash Bargaining Game的自适应权重</h5><p>这一节用于说明权重的生成，即算子$\phi(\cdot)$的实现，以及其如何具有自适应的效果。从上一节的梯度信息集合$\mathcal{W}$可以看出，我们实际上先采样了一系列梯度。我们要做的就是基于这些梯度，进行Nash Bargaining Game(NBG)，得到权重信息。首先我们对NBG的相关参数做定义。令协议空间为$d$维度的球状空间$B_r$，其中$r$是该空间的半径，而维度$d$实际上对应了我们样本中梯度的维度。此外，定义歧义点$disagreement\ point &#x3D; 0$，无法达成一致本就是少有的情况，这样设置可令对应梯度样本不对最终权重产生影响。接下来，假设有一组梯度样本$(l_1,l_2)\in \mathcal{W}$，考虑两个players进行NBG，定义它们的效用函数分别为$u_1(l) &#x3D; l^T l_1,u_2(l) &#x3D; l^Tl_2$。NBG的目的是解决形如<strong>公式3</strong>的问题。</p>
<p><strong>公式3:</strong>$\arg\max_l \log[u_1(l)] + \log[u_2(l)]$</p>
<p>由于$disagreement\ point &#x3D; 0$，根据Nash Bargaining Solution(NBS)的性质：$l^Tl_1,l^Tl_2 &gt; 0$，所以效用函数是随着the norm of $l$单调递增的。那么唯一的最优解一定在协议空间$B_r$的边界上。所以根据帕累托最优性假设，问题的解$l^*$应该满足$||l^*||^2_2 &#x3D; r^2$。于是根据KKT条件，我们可以将<strong>公式3</strong>的目标函数改写为<strong>公式4</strong>。并且可以对目标函数求导，得到限制条件<strong>公式5</strong>。</p>
<p><strong>公式4:</strong>$\arg\max_l \log[u_1(l)] + \log[u_2(1)] - \lambda(||l||^2_2 - r^2)$</p>
<p><strong>公式5:</strong>$\frac{1}{l^T l_1}l_1+\frac{1}{l^T l_2}l_2  &#x3D; 2\lambda l$</p>
<p>由于在最优解处，导数的方向一定是径向的，即$(\frac{1}{l^T l_1}l_1+\frac{1}{l^T l_2}l_2) || \lambda l$。We can expand Eq.(5) for the inconsistent deviation with linear independent assignment，不妨令$\lambda &#x3D; 1$，则得到形如$l^T l_1 &#x3D; \frac{1}{\alpha_1},l^T l_2 &#x3D; \frac{1}{\alpha_2}$的式子。令$G$为$d \times 2$的矩阵，其中第一、二列分别对应$l_1,l_2$。记$\alpha &#x3D; (\alpha_1,\alpha_2)^T$，其中$\alpha1,\alpha2$即梯度样本$(l_1,l_2)$产生的权重结果。为了得到的这个结果，此时我们的目标等价转换为求解<strong>公式6</strong>。</p>
<p><strong>公式6：</strong>$G^TG\alpha &#x3D; \frac{1}{\alpha}$</p>
<h5 id="Sloving-the-Nash-Bargaining-Game"><a href="#Sloving-the-Nash-Bargaining-Game" class="headerlink" title="Sloving the Nash Bargaining Game"></a>Sloving the Nash Bargaining Game</h5><p>在这一部分我们将<strong>公式6</strong>转换为一个凸优化问题，进行高效求解。定义$\beta_1 &#x3D; l_1^T G \alpha,\beta_2 &#x3D; l_2^T G \alpha$，求解目标<strong>公式6</strong>等价于$\alpha_1 &#x3D; 1&#x2F;\beta_1,\alpha_2 &#x3D; 1&#x2F;\beta_2$，其中$\alpha&#x3D;(\alpha_1,\alpha_2)^T$。进一步考虑将原问题转换为$\log( \alpha_1) +\log(\beta_1)&#x3D; 0, \log(\alpha_2)+\log(\beta_2) &#x3D; 0$。于是我们记$\psi_1(\alpha) &#x3D; \log(\alpha_1) + \log(\beta_1),\psi_2(\alpha) &#x3D; \log(\alpha_2) + \log(\beta_2)$，以及$\psi(\alpha) &#x3D; \psi_1(\alpha) + \psi_2(\alpha)$。那么现在的目标求解$\alpha$满足$\psi(\alpha) &#x3D; 0$。在$\psi_1,\psi_2 \ge 0$的情况下，我们可以通过<strong>公式6</strong>求解$\alpha$的近似解。其意义是希望找到$\alpha$，尽量让$\psi(\alpha)$的值接近或等于0。</p>
<p><strong>公式6:</strong>$\min_\alpha\psi(\alpha) \ \ \ \ \ \ \ s.t. \ \psi_i(\alpha) \ge0,\alpha_i\ge0,i &#x3D; 1,2$</p>
<p>该优化的约束条件是凸且线性的。根据【】，在<strong>公式6</strong>的基础上进一步最小化$\beta_1 + \beta_2$，可以得到关于原问题的一个非常接近精确解的近似解。于是引入它们得到一个更好的优化近似，即<strong>公式7</strong>。</p>
<p><strong>公式7:</strong>$\min_\alpha\psi(\alpha) + \beta_1 + \beta_2 \ \ \ \ \ \ \ s.t. \ \psi_i(\alpha) \ge0,\alpha_i\ge0,i &#x3D; 1,2$</p>
<p>但是上面的优化问题实际上是一个凸-凹优化，其中的凹项是$\psi(\alpha)$。我们可以使用CCP方法【】来对其进行展开，转换为一个近似的凸问题。考虑多次迭代，在每次迭代过程使用一阶近似 $\tilde{\psi}_\tau(\alpha) &#x3D; \psi(\alpha^\tau)^T+\nabla\psi(\alpha^\tau)(\alpha-\alpha^\tau)$ 对 $\psi(\alpha)$进行展开并取代它，得到<strong>公式8</strong>。我们对<strong>公式8</strong>，$\tau &#x3D; 1,2,3,\dots$的情况进行迭代式的求解，得到序列${\alpha^\tau}_\tau$。根据 CCP（凸凹过程）的既有理论，收敛到原始非凸问题<strong>公式7</strong>的一个临界点。最终，$\alpha$的求解被转换为了一个凸优化问题。</p>
<p><strong>公式8:</strong>$\min_\alpha\tilde{\psi}_\tau(\alpha) + \beta_1 + \beta_2 \ \ \ \ \ \ \ s.t. \ \psi_i(\alpha) \ge0,\alpha_i\ge0,i &#x3D; 1,2$</p>
<p>最终优化的结果即我们基于一个梯度信息得到的权重样本$\alpha_1,\alpha_2$。对一次本地训练产生的每组梯度$(l_1^j,l_2^j) \in \mathcal{W}$通过上述过程得到权重$\alpha_1^j,\alpha_2^j$。令最终的权重为$\alpha_1 &#x3D; \sum \alpha_1^j,\alpha_2 &#x3D; \sum \alpha_2^j$，做归一化处理$\alpha_1 \leftarrow \alpha_1&#x2F;(\alpha_1+\alpha_2)$，以及$\alpha_2 &#x3D; 1 - \alpha_1$。将整个过程记做算子$\phi$，即${\alpha_1,\alpha_2} &#x3D; \phi(\mathcal{W})$。</p>
<h5 id="algorithm"><a href="#algorithm" class="headerlink" title="algorithm"></a>algorithm</h5><p>略</p>
<h4 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">实验使用的数据集介绍；数据集划分方法</span><br><span class="line"></span><br><span class="line">使用的模型介绍；CNN、fastText</span><br><span class="line"></span><br><span class="line">使用的实验设置：设备、对比算法、全局参数设置</span><br><span class="line"></span><br><span class="line">评估指标介绍：个性化准确率、全局准确率</span><br><span class="line"></span><br><span class="line">不同方法之间准确率对比；不同方法之间通信轮次对比；不同采样率下的效果对比；个性化准确率与全局准确率</span><br><span class="line"></span><br><span class="line">！！！注意自己添加子标题</span><br></pre></td></tr></table></figure>

<p>这一部分介绍算法的实验测试情况。下面会依次描述，测试使用到的数据集以及非独立同分布的数据集划分方法；使用到的模型；实验的设置情况包括我们使用的环境，相关的参数设置以及对比算法的介绍；评估指标的使用。</p>
<h4 id="还需要补充的实验"><a href="#还需要补充的实验" class="headerlink" title="还需要补充的实验"></a>还需要补充的实验</h4><ol>
<li>non-IID下MINIST的测试结果</li>
<li>IID下MNIST FedAS测试结果（测试过程中会出现h5无法输出的问题，将控制台信息保存到txt，本地测试！）<br>（似乎MNIST数据集的测试结果，IID和non-IID有点混）</li>
<li>关于全局模型性能的测试，可能需要使用所有客户端的测试集构成的数据集作为测试集。而不能自己重新划分。(已经完成)</li>
<li>不同采样率下PFLNash在Cifar100上的表现，这个全局准确率和个性化准确率一起与Ditto&#x2F;FedAvg&#x2F;FedProx做全局准确率的对比</li>
</ol>
<p>使用的图与表</p>
<ol>
<li>一张大表，各个算法在IID与non-IID环境下最终的个性化准确率</li>
<li>一张小表，PFLNash在IID与non-IID环境下与其他算法的泛化准确率（使用Cifar100数据集）</li>
<li>一张图，IID与non-IID环境下，泛化准确率-global round的图</li>
<li>六张个性化准确率-global round的图，三个数据集，IID与non-IID，共对应六种情况</li>
<li>一张小表，PFLNash在不同采样率下，在各个数据集上的表现（1、5、10、50）</li>
</ol>
<h5 id="Experimental-Setup"><a href="#Experimental-Setup" class="headerlink" title="Experimental Setup"></a>Experimental Setup</h5><p>我们使用的数据集来自两个领域，包括MNIST【】, Cifar100【】, SogouNews【】，其中前两个数据集来自于CV(Computer Vision)领域，剩下的一个来自于NLP(Nature Language Process)领域。MNIST（Modified National Institute of Standards and Technology）数据集是手写数字识别领域的经典数据集。CIFAR-100是一个用于图像分类任务的数据集，由100个类别组成，每个类别包含600张32x32像素的彩色图像。Sogou News Dataset 是由 SogouCA 和 SogouCS 新闻语料库构成的数据集，其拥有5个类别（体育、财经、娱乐、汽车、科技）共计2,909,551 篇文章，每个类别均包含90,000个训练样本和12,000个测试样本。对于MINST与Cifar100，使用的模型是CNN，与McMahan【】使用的相同，包括两个5×5的卷积层，分别是32通道和64通道；以及一个512单元的全连接层，和一个输出层。对于SogouNews，使用的模型是fastText，包括一个嵌入层，一个线性的隐藏层以及输出层。我们根据任务的类别数调整模型的输出层，分别为10, 100, 5对应上面三个数据集。</p>
<p>我们对这些数据集使用狄利克雷划分的方法【】，为每个客户端分配本地数据。在这种方式下，每种类别的数据被划分到一个客户端的概$p~Dir(\beta)$，其中$Dir(\alpha)$是狄利克雷分布。当参数$\alpha \rightarrow 0$时一种类别的数据几乎会全部分配给某一个客户端，其它客户端只能分到少量该类数据。客户端间的数据呈现极度的非独立同分布性。我们在使用中设置$\alpha &#x3D; 0.1$，对应各客户端持有的数据数量以及类型均相差很大，构建了数据异质性的测试环境。</p>
<p>我们选择了联邦学习领域的10个SOTA baselines，与PFLNash进行对比，包括FedProx【】、FedPer【】、FedAvg【】、FedAS【】、FedALA【】、Ditto【】、FedPAC【】、MOON【】、FedRep【】、FedPHP【】、FedFomo【】、pFedME【】。其中Ditto【】被我们选做主要的baseline，因为我们的双模型框架与其相似，均在客户端维护了全局模型和本地模型，且会并行地对它们进行训练。</p>
<p>关于实验的设置，我们参考了一些经典的研究。根据pFedMe【】，我们将客户端的数量设置为20并设置客户端的参与率$\rho &#x3D; 1$。根据FedAvg【】，我们将本地模型训练的epoch数量设置为1，并将batch size设置为10。根据FedALA【】，优化器的学习率被我们设置为$0.005$并且以每轮0.99的比例进行衰减。global epoch是200轮。在MNIST与Cifar100的测试上，PFLNash的采样频率$p &#x3D; 1$，在SogouNews的测试上，PFLnash的采样频率$P &#x3D; 3$。进行实验的主机环境使用的操作系统是Ubuntu 22.04，使用了一个AMD EPYC 7663 @3.50GHz CPU，一个24GB显存的RTX4090 GPU，以及80GB RAM。</p>
<h5 id="Evaluation-metrics-1"><a href="#Evaluation-metrics-1" class="headerlink" title="Evaluation metrics"></a>Evaluation metrics</h5><p>Personalized Accuracy:我们将满足客户端的个性化需求作为我们的首要目标，将PFLNash划入pFL的范畴。因此我们对10个baselines都使用了这种个性化准确率。根据pFedMe【】，该指标对应的是最好的本地模型的平均测试准确率。</p>
<p>Generalization Accuracy:我们将确保全局模型的性能看作我们的次要目标，使用PFLNash与Ditto在泛化准确率上进行了对比。该指标对应的是全局模型在整个测试数据集上的准确率，可以表征全局模型的泛化能力。</p>
<p>Communication Rounds:在有限的通信轮次内，模型能达到的性能是联邦学习中的一个核心问题。因此我们模型的准确率随通信轮次变化的情况，作为我们的一个评估项。</p>
<h5 id="Experimental-reasults"><a href="#Experimental-reasults" class="headerlink" title="Experimental reasults"></a>Experimental reasults</h5><p><strong>Personalized Accuracy</strong><br>在强异质性（$\beta&#x3D;0.1$）设定下，各基线方法的性能普遍出现显著下滑。以CIFAR-100为例，FedAvg准确率骤降至$23.31%$，这源于其追求单一全局最优的假设在客户端数据分布高度分歧时完全失效，导致全局模型对每个本地任务均不适用。FedProx通过引入近端项缓解客户端漂移，但并未改变其模型同质化的本质，收效有限（$24.66%$）。部分个性化方法如FedPer（$53.82%$）和FedRep通过解耦个性化层与基础层取得一定提升，但这种固定结构的切割可能损失重要特征交互，且无法动态适应异质程度。MOON（$38.91%$）等利用对比学习增强客户端一致性的方法，在极端异构下可能因“负样本”选择问题而干扰本地训练。</p>
<p>与此形成鲜明对比，PFLNash在CIFAR-100上$\beta&#x3D;0.1$时达到了$60.13%$的准确率，大幅领先所有基线。这一优势直接归因于其核心的双模型纳什议价机制。首先，本地模型$\omega_i$与全局副本$\omega_g^i$的同步独立训练，直接在本地数据上生成了表征两者效用的梯度信息。在强异质环境下，全局副本的性能通常会显著差于精调后的本地模型，该梯度差异被系统精准捕获。其次，基于纳什议价解的权重生成函数$\phi(\mathcal{W})$，将聚合过程建模为一个合作博弈。其求解出的权重$(\alpha_1, \alpha_2)$具有帕累托最优性，意味着在该客户端特定数据分布下，找到了本地模型与全局知识贡献的最佳权衡点。在强异质时，算法会自动为性能更优、更适配本地数据的$\omega_i$分配更高的权重（即$\alpha_1$主导），同时仍以最优方式吸纳全局模型中有限的、有价值的信息（通过$\alpha_2$），而非像Ditto等正则化方法那样进行可能次优的硬性约束。</p>
<p>随着异质性减弱（$\beta$增大至$0.4, 0.7$），全局模型本身性能改善，其对客户端的效用提升。此时，PFLNash通过同样的博弈机制，能够自适应地调整权重，增大$\alpha_2$以更充分地利用高质量的全局知识，因此在$\beta&#x3D;0.7$的CIFAR-100上取得了$65.01%$的更高准确率。这种平滑过渡的动态适应性，是固定策略或启发式加权方法（如FedFomo, FedALA）所难以实现的。<br><strong>Generation Accuracy</strong></p>
<p><strong>Global Rounds</strong></p>
<p><strong>Sampling Frequency</strong></p>
<p><strong>ICDCS</strong> (IEEE International Conference on Distributed Computing Systems) - CCF-B</p>
<p>相关介绍：ICDCS 是分布式计算领域的会议，联邦学习本质上是分布式机器学习，是其核心关注方向之一。近年来关于联邦学习优化、隐私、效率、个性化（如模型个性化、梯度个性化）的论文非常多。</p>
<p>DDL：ICDCS(2026) 2025-12-05;2025-12-12<br>（时间参考：AoE）<br>两到三个月评审时间，不能转投其它会议…</p>
<p><strong>ECML-PKDD</strong> (European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases) - CCF-B（再确认一下审稿时间）</p>
<p>A会（实验相关）</p>
<h3 id="图片描述1"><a href="#图片描述1" class="headerlink" title="图片描述1"></a>图片描述1</h3><p>每个客户端持有的不同种类的数据数量差异很大，导致本地训练时每个本地模型会朝着不同的方向更新，像图中的黑色实线图案一样。而理想的情况是服务器持有所有客户端的数据进行模型训练，像图中的黑色虚线与红色图案一样。蓝色图案是5轮本地训练之后服务器聚合得到的模型。就结论来说，黑色图案与红色图案位置的不同代表了全局最优与本地最优的差异；蓝色图案与红色图案位置的不同代表了客户端漂移。</p>
<h2 id="论文精修"><a href="#论文精修" class="headerlink" title="论文精修"></a>论文精修</h2><ol>
<li>向量符号需要加粗表示</li>
<li>引用文献</li>
<li>超出页数，中间要精简一些</li>
<li>更加丰富地插图（流程、实验）</li>
<li>消融实验</li>
</ol>

		</div>

		<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC80NjIyNC8yMjczNQ==">
	<script type="text/javascript">
   (function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
	</script>
<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->
		
	</article>

	<div id="toc">
		
	</div>

</div>

<!-- <div id="paginator"> -->
<!-- 	 -->
<!-- </div> -->


			</div>
		</div>

		<div id="bottom-outer">
			<div id="bottom-inner">
				Site by 阳生 | 
				Powered by <a target="_blank" rel="noopener" href="http://hexo.io">Hexo</a> |
				theme <a target="_blank" rel="noopener" href="https://github.com/fireworks99/hexo-theme-PreciousJoy">PreciousJoy</a>
			</div>
		</div>

		
	</div>





	
	<!-- scripts list from theme config.yml -->
	
	<script src="/js/jquery-3.5.1.min.js"></script>
	
	<script src="/js/PreciousJoy.js"></script>
	
	<script src="/js/highlight.pack.js"></script>
	
	<script src="/js/jquery.fancybox.min.js"></script>
	
	<script src="/js/search.js"></script>
	
	<script src="/js/load.js"></script>
	
	<script src="/js/jquery.mCustomScrollbar.concat.min.js"></script>
	
	<script src="/js/clipboard.min.js"></script>
	
	

	<script>hljs.initHighlightingOnLoad();</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
