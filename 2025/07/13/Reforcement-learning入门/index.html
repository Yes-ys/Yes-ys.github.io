


<!DOCTYPE html>
<html lang="ch">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<title>Reforcement learning入门 [ 代码和诗 ]</title>
	
	
	<!-- stylesheets list from _config.yml -->
	
	<link rel="stylesheet" href="/css/PreciousJoy.css">
	
	<link rel="stylesheet" href="/css/top-bar.css">
	
	<link rel="stylesheet" href="/css/menu-outer.css">
	
	<link rel="stylesheet" href="/css/content-outer.css">
	
	<link rel="stylesheet" href="/css/bottom-outer.css">
	
	<link rel="stylesheet" href="/css/atom-one-dark.css">
	
	<link rel="stylesheet" href="/css/recent-posts-item.css">
	
	<link rel="stylesheet" href="/css/article-sidebar-toc.css">
	
	<link rel="stylesheet" href="/css/jquery.fancybox.min.css">
	
	<link rel="stylesheet" href="/css/search.css">
	
	<link rel="stylesheet" href="/css/toc.css">
	
	<link rel="stylesheet" href="/css/sidebar.css">
	
	<link rel="stylesheet" href="/css/archive.css">
	
	<link rel="stylesheet" href="/css/jquery.mCustomScrollbar.min.css">
	
	<link rel="stylesheet" href="/css/Z-last-cover-others.css">
	
	
	
<meta name="generator" content="Hexo 7.3.0"></head>




<body id="wrapper">

	<div id="">
		
		<div id="top-bar">
			
			<div id="avatar-box">
				<img 
				class="avatar"
				src="/images/my-avatar.jpg" //网站头像
				alt="avatar">
			</div>

			<div id="top-bar-text">
				<div id="top-bar-title">
					阳生。
				</div>
				<div id="top-bar-slogan">
					风毛丛劲节，只上尽头竿。
				</div>
			</div>

		</div>

		<div id="menu-outer">
			<div id="menu-inner">
				
				
				<div class="menu-item">
					<a href="/">Home</a>
				</div>
				
				<div class="menu-item">
					<a href="/about">About</a>
				</div>
				
				<div class="menu-item">
					<a href="/archives">Archives</a>
				</div>
				

				<div class="menu-item menu-item-search">
					
  <span class="local-search local-search-google local-search-plugin">
      <input type="search" placeholder="站内搜索" id="local-search-input" class="local-search-input-cls" style="">
      <div id="local-search-result" class="local-search-result-cls"></div>
  </span>
	
				</div>

			</div>
		</div>

		<div id="content-outer">
			<div id="content-inner">

				
<div id="details">
	
	<article id="details-post">
		<div id=details-post-item>
			<h1>Reforcement learning入门</h1>
			<p><code>这篇blog用于记录我进行强化学习入门时学习到的基础知识</code></p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>一些基本要素：</p>
<ol>
<li>Agent：进行“学习”的主体，会通过学习到的经验与环境交互，并在与环境交互的过程中进一步学习。</li>
<li>Environment：Agent交互的对象，客观存在，例如智能驾驶捕捉到的一张图片</li>
<li>State：Agent观察当前自己所处环境，获取到的局部环境信息</li>
<li>Action：Agent根据State结合自己以往的经验做出的行动，行动会改变Environment（Action可能是离散值，也可能是连续值，处理方法是不同的）</li>
<li>Reward：Agent执行Action改变Environment后，Environment给到Agent的反馈，可以用于反映Action的好坏，同时为Agent累积“经验”</li>
</ol>
<p>进一步，关于Reward的作用：<br>例如将Agent当作一个neural network，其经验就是权重参数θ，坏的Action应该产生一个Reward将θ向“反向”调节，好的Action应该产生一个Reward将θ向“正向”调节。</p>
<h3 id="基本的工作流程"><a href="#基本的工作流程" class="headerlink" title="基本的工作流程"></a>基本的工作流程</h3><p>强化学习算法的基本工作流程就是，根据Environment产生State输入Agent，Agent做出Action，改变Environment，获得Reward，反馈调整Agent，不断重复。直到满足退出条件</p>
<h3 id="关于计算机眼中的环境"><a href="#关于计算机眼中的环境" class="headerlink" title="关于计算机眼中的环境"></a>关于计算机眼中的环境</h3><p>通常情况下，计算机眼中的环境是一张图像。</p>
<p>例如，环境就是各种可能的画面，State就是当前捕捉到的画面，State输入给Agent(NN)，然后由Agent(NN的参数与结构)计算产生Action(决策，或是各种决策的概率)；进一步，也可以理解成State是由画面经过GNN处理后的特征信息…</p>
<h3 id="强化学习的一个关键问题"><a href="#强化学习的一个关键问题" class="headerlink" title="强化学习的一个关键问题"></a>强化学习的一个关键问题</h3><p>通过前面的基本概念，看似只需要训练一个输入为State，输出为Action的神经网络就可以了，但是事实并非如此。</p>
<p>因为本质上，这个神经网络不是一个纯粹的有监督学习，并非用于分类与回归的网络，处理的数据没有标签，对于每一个State处理后得到的Action无法立刻得知True or False（类比分类），没有办法直接地对神经网络的参数进行更新，取而代之的是产生一个Good or Bad的Reward，我们最终的目标总是要去最大化这个Reward。（当然，以一个游戏做类比，在进行一系列Action后，我们最终能够得知游戏的结果是胜利或失败，对应True or False，所以说不是纯粹的有监督学习）。</p>
<p>所以强化学习算法的关键是要有对于每种Action给出合理Reward的方案，并且要有根据Reward去更新模型参数的方案。</p>
<p>关于<strong>强化学习对比深度学习</strong>例如给出老虎的图片做分类</p>
<p>深度学习可以用CNN去提取特征，然后根据当前模型参数计算出分类标签，根据估计的标签和真实标签的差异情况，计算损失，根据损失更新模型参数，就完成了一次学习。</p>
<p>而强化学习对于给出的图片是先由Agent，得出Action，然后有Action与Reward，再去想办法更新模型参数。</p>
<p>所以，<strong>如何训练一个网络，可以根据Action和Reward进行更新</strong>是强化学习最关键的问题。</p>
<h2 id="PPO-Proximal-Policy-Optimization-算法"><a href="#PPO-Proximal-Policy-Optimization-算法" class="headerlink" title="PPO(Proximal Policy Optimization)算法"></a>PPO(Proximal Policy Optimization)算法</h2><h3 id="介绍背景"><a href="#介绍背景" class="headerlink" title="介绍背景"></a>介绍背景</h3><p>此部分根据一个飞船降落的小游戏，介绍PPO算法，这个小游戏是一个下降的飞船，玩家应该根据left or right去操作飞船的左右行动，最终让飞船落地时在两个小旗帜中间，即降落在正确的位置。</p>
<h3 id="再次从深度学习的视角看"><a href="#再次从深度学习的视角看" class="headerlink" title="再次从深度学习的视角看"></a>再次从深度学习的视角看</h3><p>关于神经网络NN（θ）</p>
<ol>
<li>输入：使用NN神经网络，输入是当前游戏的图片</li>
<li>输出：两个Action，左或右</li>
</ol>
<p>如果我们使用深度学习，看起来这是一个二分类的问题，很简单。如果说θ作为权重参数会控制，对于每一张图片的分类结果，左或右，代表应该怎样玩。那么我们最终的目标应该是求出一个合适的θ，可以对每一张图片给出合理的决策，其中可以用到梯度下降等等一些常见的方法。</p>
<p>但是，我们不得不面临一些非常困难的问题，标签（tag）、梯度（gradient）、损失函数（loss function）应该怎样去确定，这是强化学习需要考虑的问题。</p>
<p>因为不是每一步Action都有一个对应的“正确的”tag，告诉我们的决策是否正确，我们每轮训练，只有一系列的Action，以及最终的一个结果，胜利或失败（或许可以称为tag），那么我们应该如何去定义损失函数？<strong>这就是PPO算法考虑的关键问题，即目标函数的定义方法&amp;如何进行求解</strong></p>
<h3 id="PPO算法"><a href="#PPO算法" class="headerlink" title="PPO算法"></a>PPO算法</h3><h4 id="episod"><a href="#episod" class="headerlink" title="episod"></a>episod</h4><p>一个完整的过程，其中具有许多的State，Agent根据每个State做出Action，改变环境在此处即根据Action进入下一个State，直到停止的退出条件即完成了一个episod。</p>
<h4 id="常见的退出条件"><a href="#常见的退出条件" class="headerlink" title="常见的退出条件"></a>常见的退出条件</h4><p>max_step，最大迭代次数，Agent最多只能进行max_step次Action的产生，完成后退出episod；其它退出条件，对于具体的游戏可能需要设置不同的退出条件，例如此处就是飞机落到地面上，episod就完成了。</p>
<h4 id="整个生命周期的奖励"><a href="#整个生命周期的奖励" class="headerlink" title="整个生命周期的奖励"></a>整个生命周期的奖励</h4><p>$R &#x3D; \sum^{T}_{t&#x3D;1} r_t$，如果考虑退出条件为设置max_step &#x3D; 1000，那么对于Action（$a_1, a_2, \dots, a_1000$），有Reward（$r_1, r_2, \dots, r_1000$），求和后即整个episod的Reward(R)。</p>
<p>奖励可以类比于标签（用于衡量每次Action的好坏），在一些简单的实验中我们可以使用openAI的工具包gym，其中按照一些经典的小游戏的游戏规则，会<strong>给定好每种Action的Reward</strong>。</p>
<p>奖励是由当前一步的Action与State共同决定的，预先确定好后，作为游戏规则是不能改变的</p>
<p>（当然奖励的设置仍然是强化学习的关键，只是在这一部分，对于PPO算法的学习，我们更加关注于<strong>神经网络的设置、目标函数的定义、参数更新的方法</strong>）</p>
<h4 id="一次游戏的记录结果"><a href="#一次游戏的记录结果" class="headerlink" title="一次游戏的记录结果"></a>一次游戏的记录结果</h4><p>即每一步的状态与行动（trajectory），$\tau &#x3D; {s_1,a_1,s_2,a_2,\dots,s_T,a_T}$（在此基础上，我们需要考虑如何给出每一步的行动，让总的奖励达到最大，即训练一个网络模型$\pi_\theta(a_t|s_t)$，其输入是状态State，输出是Action，由所有的模型参数来根据输入决定输出，训练参数的目标是要让奖励尽可能的大）</p>
<p>游戏记录的表达式是：</p>
<p>$p_theta(s_1,a_1,\dots,s_T,a_T) &#x3D; p(s_1)\prod^{T}<em>{t&#x3D;1}\pi_\theta(a_t|s_t)p(s</em>{t+1}|s_t,a_t)$</p>
<p>含义是得到游戏记录，初始状态是$s_1$执行action，$a_1$，切换到状态，$s_2$执行action，$a_2$，…切换到状态$s_T$执行action，$a_T$的概率是：初始状态是$s_1$的概率连乘上模型在该状态下给出对应action的概率和根据游戏规则，在当前状态下执行相应action后切换到下一对应状态的概率</p>
<h4 id="Action的产生与作用"><a href="#Action的产生与作用" class="headerlink" title="Action的产生与作用"></a>Action的产生与作用</h4><p>关于Action的产生$p_\theta(a_t|s_t)$，由当前状态确定某一行为的概率，对应Action产生的概率，是模型输出的结果，同时是我们应该关注的问题</p>
<p>关于Action的作用，$p(s_{t+1}|s_t,a_t)$在某一状态下做出某Action，会切换到下一个状态，这是根据游戏的规则确定的，与我们的模型无关</p>
<h4 id="一般的强化学习算法的目标与训练过程"><a href="#一般的强化学习算法的目标与训练过程" class="headerlink" title="一般的强化学习算法的目标与训练过程"></a>一般的强化学习算法的目标与训练过程</h4><p>我们的目标是得到一个合适的模型参数 $\theta^{*} &#x3D; argmax_\theta E_{\tau \sim p_\theta(\tau)}[\sum_t r(s_t,a_t)]$</p>
<p><em>相关理解：</em></p>
<p>其中$p_\theta(\tau)$表示在策略$\pi_\theta$下，产生轨迹$\tau$的概率分布，即$p_\theta(\tau)$是轨迹 $\tau$ 的概率，其依赖于策略的参数$\theta$。</p>
<p>在优化目标 $\theta^{*} &#x3D; \arg\max_\theta E_{\tau \sim p_\theta(\tau)}\left[\sum_t r(s_t, a_t)\right]$中，$E_{\tau \sim p_\theta(\tau)}$表示在策略 $\pi_\theta$下，对所有可能的轨迹 $\tau$ 进行期望（期望值的计算）。具体来说，$\tau \sim p_\theta(\tau)$这个下标的含义是：轨迹 $\tau$ 是根据策略 $\pi_\theta$产生的。也就是说，我们考虑的是在当前策略 $\pi_\theta$下，每个轨迹 $\tau$ 出现的概率，并对其累计奖励 $\sum_t r(s_t, a_t)$进行加权平均（即期望值）。</p>
<p>进一步，根据大数定律</p>
<p>$E_{\tau \sim p_\theta(\tau)} \approx \frac{1}{N}\sum_{i}\sum_{t}r(s_{i,t},a_{i,t})$其中$N$趋近于$∞$，我们将该期望记作$\mathcal{J}_\theta$</p>
<p>或者是，期望的展开</p>
<p>这里直接将$\pi_\theta$记作$\tau$的分布概率，将$\tau$对应的奖励，即刚才的$\sum_t r(s_t, a_t)$记作$r(\tau)$，对期望展开如下:</p>
<p>$\mathcal{J}(\theta) &#x3D; E_{\tau \sim \pi_\theta(\tau)}[r(\tau)] &#x3D; \int \pi_\theta(\tau)r(\tau)d\tau$</p>
<p>在期望展开的基础上，我们计算对于参数的梯度</p>
<p>$\nabla_\theta \mathcal{J}<em>\theta &#x3D; \int \nabla_\theta \pi_\theta (\tau)r(\tau)d\tau &#x3D; \int \pi_\theta(\tau)\nabla_\theta log\pi_\theta(\tau)r(\tau)d\tau &#x3D; E</em>{\tau \sim \pi_\theta(\tau)}[\nabla_\theta log \pi_{\theta}(\tau)r(\tau)]$</p>
<p>最后，再使用大数定律展开</p>
<p>$\nabla_\theta \mathcal{J}(\theta) \approx \frac{1}{N}\sum^{N}<em>{i&#x3D;1}(\sum^{T}</em>{t&#x3D;1}\nabla_\theta log \pi_{\theta}(a_{i,t}|s_{i,t}))(\sum^T_{t&#x3D;1}r(s_{i,t},a_{i,t}))$</p>
<p>注意，这里把得到轨迹$\tau$的概率展开了，把其对应的奖励也展开了</p>
<p>我们训练的过程就是使用<strong>梯度上升</strong>更新参数，对应为：</p>
<p>$\theta$ &lt;- $\theta + \alpha \nabla_\theta \mathcal{J}(\theta)$</p>
<h2 id="蒙特卡洛方法"><a href="#蒙特卡洛方法" class="headerlink" title="蒙特卡洛方法"></a>蒙特卡洛方法</h2><h3 id="Motivating-example"><a href="#Motivating-example" class="headerlink" title="Motivating example"></a>Motivating example</h3><p>Q：我们如何能够不依赖于模型，进行一些预测<br>A：最简单的思想就是，Monte Carlo estimation</p>
<h4 id="一个例子——抛掷硬币"><a href="#一个例子——抛掷硬币" class="headerlink" title="一个例子——抛掷硬币"></a>一个例子——抛掷硬币</h4><p>定义一个变量$X$，如果正面朝上则$X &#x3D; 1$，否则$X &#x3D; -1$，我们现在要考虑的是如何计算期望$E(X)$</p>
<p>显然，当我们拥有一个模型（概率分布）的时候，即$X \sim p(X), p(X&#x3D;1)&#x3D;0.5, P(X&#x3D;-1)&#x3D;0.5$，那么我们可以根据期望的定义$E(X) &#x3D; \sum_x xp(x)$快速地求出均值。</p>
<p>但是当我们没有这个模型的时候呢？即Model-free的情况</p>
<p>Monte Carlo estimation：进行多次实验，用平均数近似期望</p>
<p>$E(X) &#x3D; \frac{1}{n}\sum^n_{i&#x3D;1}x_i$</p>
<p>数学上的支撑就是我们的大数定律（Law of Large Numbers）</p>
<p>我们知道state value、action value的定义本身就是期望expectations，所以后续我们会用Mente Carlo在Model free的条件下去求state value、action value，并得出相应的策略</p>
<h3 id="MC-Basic"><a href="#MC-Basic" class="headerlink" title="MC Basic"></a>MC Basic</h3><p>Key quesition：How to convert the policy iteration algorithm to be model-free</p>
<h4 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h4><p>策略迭代（Policy Iteration）是强化学习中的一种经典算法，用于解决马尔可夫决策过程（MDP）问题。其目标是找到一个最优策略，使得在该策略下的长期累积奖励最大化。策略迭代由两个主要步骤组成：策略评估（Policy Evaluation）和策略提升（Policy Improvement）。</p>
<p>策略迭代的步骤</p>
<p>初始化</p>
<p>初始化一个任意策略 $\pi$。</p>
<p>策略评估（Policy Evaluation）</p>
<p>在策略评估阶段，我们计算当前策略 $\pi$ 的状态价值函数 $V^{\pi}(s)$<br>，即在策略 $\pi$ 下，从状态 $s$ 开始的期望累积奖励。</p>
<p>通过贝尔曼期望方程迭代地更新 $V^{\pi}(s)$<br>：<br>$  V^{\pi}(s) &#x3D; \sum_{a} \pi(a|s) \sum_{s’} P(s’|s,a) [R(s,a,s’) + \gamma V^{\pi}(s’)]$</p>
<p>其中，$P(s’|s,a)$是从状态 $s$ 执行动作 $a$ 转移到状态 $s’$的概率，$R(s,a,s’)$是从状态 $s$ 执行动作 $a$ 并转移到状态 $s’$所得到的奖励， $\gamma$ 是折扣因子。</p>
<p>策略提升（Policy Improvement）</p>
<p>在策略提升阶段，我们使用当前价值函数 $V^{\pi}(s)$来改进策略 $\pi$。<br>通过贪心策略提升来生成一个新的策略 $\pi’$，使得在每个状态下选择使得期望累积奖励最大的动作：</p>
<p>$  \pi’(s) &#x3D; \arg\max_{a} \sum_{s’} P(s’|s,a) [R(s,a,s’) + \gamma V^{\pi}(s’)]<br> $</p>
<p>检查收敛<br>如果新的策略 $\pi’$等于旧的策略 $\pi$（即策略不再变化），则策略迭代过程结束，当前策略即为最优策略。</p>
<p>否则，更新策略 $\pi \ne \pi’$，继续迭代。</p>
<h4 id="key-point"><a href="#key-point" class="headerlink" title="key point"></a>key point</h4><p>在Policy Iteration的策略提升过程中，实际上我们不止考虑一个针对当前state的最优action，而是会考虑一系列action，即最优的策略，可以写作：</p>
<p>$\pi’(s) &#x3D; argmax_\pi \sum_a \pi(a|s) [\sum_r p(r|s,a)r + \gamma \sum_{s’}p(s’|s,a)v_{\pi_k}(s’)] &#x3D; argmax_\pi \sum_a \pi(a|s)q_{\pi_k}(s,a), s \in S$</p>
<p>问题的关键就在于$q_{\pi_k}(s,a)$这个状态动作值函数，而$\pi(a|s)$为状态s下采取动作a的概率，这在mento carlo考虑的场景下是已知的</p>
<p>我们回到状态值函数的定义</p>
<p>$q_{\pi_k}(s,a) &#x3D; E[G_t|S_t &#x3D; s,At &#x3D; a]$</p>
<p>其中$G_t$是状态s下做出动作a后的累积的奖励；可以看到其本质是一个期望，所以可以用mento carlo的方法来估计它</p>
<h4 id="具体求解"><a href="#具体求解" class="headerlink" title="具体求解"></a>具体求解</h4><p>1）从任意状态$(s,a)$出发，根据当前策略$\pi_k$，生成一个episode</p>
<p>2）返回episode的累积奖励$g(s,a)$，这里$g(s,a)$本质上就是状态值函数期望函数中的变量$G_t$的一个采样（sample）</p>
<p>3）进行多轮采样，令$q_{\pi_k}(s,a) &#x3D; \frac{1}{N}\sum^{N}_{i&#x3D;1}g^{(i)}(s,a)$</p>
<p>得到了$q_{\pi_k}(s,a)$之后，我们就可以和policy iteration中的第二步一样，去根据这个状态值函数，更新我们的最优策略。<strong>所以mento carlo与policy iteration两者最大的不同就在于值函数的求解，policy iteration不是model-free的，相关的概率是已知的，可以直接根据期望的定义求解，而mento carlo做不到。</strong></p>
<h2 id="奖励设置"><a href="#奖励设置" class="headerlink" title="奖励设置"></a>奖励设置</h2><p><code>这一部分介绍有关栅格迷宫问题可以使用的奖励设置</code></p>
<h3 id="目标导向"><a href="#目标导向" class="headerlink" title="目标导向"></a>目标导向</h3><p>当智能体成功到达目标位置时，给予一个较大的正奖励；反之，每一步移动没有到达目标点，可以给予一个较小的负奖励。</p>
<p>从而让智能体更快地到达目标位置。</p>
<h3 id="常见的惩罚"><a href="#常见的惩罚" class="headerlink" title="常见的惩罚"></a>常见的惩罚</h3><ol>
<li>智能体试图移动到不可到达的位置，给予负奖励，防止撞墙</li>
<li>智能体重复访问同一位置的时候，给予负奖励，防止重复访问</li>
</ol>
<h3 id="其它设置"><a href="#其它设置" class="headerlink" title="其它设置"></a>其它设置</h3><p>距离奖励，每步靠近目标给予正奖励；远离目标给予负奖励<br>探索奖励，进入没有探索过的区域给予正奖励，但是逐渐收敛</p>

		</div>

		<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC80NjIyNC8yMjczNQ==">
	<script type="text/javascript">
   (function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
	</script>
<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->
		
	</article>

	<div id="toc">
		
	</div>

</div>

<!-- <div id="paginator"> -->
<!-- 	 -->
<!-- </div> -->


			</div>
		</div>

		<div id="bottom-outer">
			<div id="bottom-inner">
				Site by 阳生 | 
				Powered by <a target="_blank" rel="noopener" href="http://hexo.io">Hexo</a> |
				theme <a target="_blank" rel="noopener" href="https://github.com/fireworks99/hexo-theme-PreciousJoy">PreciousJoy</a>
			</div>
		</div>

		
	</div>





	
	<!-- scripts list from theme config.yml -->
	
	<script src="/js/jquery-3.5.1.min.js"></script>
	
	<script src="/js/PreciousJoy.js"></script>
	
	<script src="/js/highlight.pack.js"></script>
	
	<script src="/js/jquery.fancybox.min.js"></script>
	
	<script src="/js/search.js"></script>
	
	<script src="/js/load.js"></script>
	
	<script src="/js/jquery.mCustomScrollbar.concat.min.js"></script>
	
	<script src="/js/clipboard.min.js"></script>
	
	

	<script>hljs.initHighlightingOnLoad();</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
