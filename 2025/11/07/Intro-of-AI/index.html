


<!DOCTYPE html>
<html lang="ch">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<title>Intro_of_AI [ 代码和诗 ]</title>
	
	
	<!-- stylesheets list from _config.yml -->
	
	<link rel="stylesheet" href="/css/PreciousJoy.css">
	
	<link rel="stylesheet" href="/css/top-bar.css">
	
	<link rel="stylesheet" href="/css/menu-outer.css">
	
	<link rel="stylesheet" href="/css/content-outer.css">
	
	<link rel="stylesheet" href="/css/bottom-outer.css">
	
	<link rel="stylesheet" href="/css/atom-one-dark.css">
	
	<link rel="stylesheet" href="/css/recent-posts-item.css">
	
	<link rel="stylesheet" href="/css/article-sidebar-toc.css">
	
	<link rel="stylesheet" href="/css/jquery.fancybox.min.css">
	
	<link rel="stylesheet" href="/css/search.css">
	
	<link rel="stylesheet" href="/css/toc.css">
	
	<link rel="stylesheet" href="/css/sidebar.css">
	
	<link rel="stylesheet" href="/css/archive.css">
	
	<link rel="stylesheet" href="/css/jquery.mCustomScrollbar.min.css">
	
	<link rel="stylesheet" href="/css/Z-last-cover-others.css">
	
	
	
<meta name="generator" content="Hexo 7.3.0"></head>




<body id="wrapper">

	<div id="">
		
		<div id="top-bar">
			
			<div id="avatar-box">
				<img 
				class="avatar"
				src="/images/my-avatar.jpg" //网站头像
				alt="avatar">
			</div>

			<div id="top-bar-text">
				<div id="top-bar-title">
					阳生。
				</div>
				<div id="top-bar-slogan">
					风毛丛劲节，只上尽头竿。
				</div>
			</div>

		</div>

		<div id="menu-outer">
			<div id="menu-inner">
				
				
				<div class="menu-item">
					<a href="/">Home</a>
				</div>
				
				<div class="menu-item">
					<a href="/about">About</a>
				</div>
				
				<div class="menu-item">
					<a href="/archives">Archives</a>
				</div>
				

				<div class="menu-item menu-item-search">
					
  <span class="local-search local-search-google local-search-plugin">
      <input type="search" placeholder="站内搜索" id="local-search-input" class="local-search-input-cls" style="">
      <div id="local-search-result" class="local-search-result-cls"></div>
  </span>
	
				</div>

			</div>
		</div>

		<div id="content-outer">
			<div id="content-inner">

				
<div id="details">
	
	<article id="details-post">
		<div id=details-post-item>
			<h1>Intro_of_AI</h1>
			<h2 id="ch1-绪论"><a href="#ch1-绪论" class="headerlink" title="ch1 绪论"></a>ch1 绪论</h2><h2 id="ch2-搜索"><a href="#ch2-搜索" class="headerlink" title="ch2 搜索"></a>ch2 搜索</h2><h2 id="ch3：对抗搜索及博弈"><a href="#ch3：对抗搜索及博弈" class="headerlink" title="ch3：对抗搜索及博弈"></a>ch3：对抗搜索及博弈</h2><h3 id="棋类介绍及分类"><a href="#棋类介绍及分类" class="headerlink" title="棋类介绍及分类"></a>棋类介绍及分类</h3><p>棋类：完备信息的动态博弈；<br>牌类：不完备信息的动态博弈；</p>
<p>棋类的特点：<br>    可以决出胜负；<br>    规则简单明确；<br>    博弈过程透明、公平；<br>    智力与经验决定胜负；<br>    变化无穷；</p>
<p>一些具体的棋类特点：<br>    象棋：60步不吃子判和；<br>    国际象棋：相对子力；<br>    围棋：规则最简单，计算机博弈难度最大；（19<em>19）；<br>    五子棋：已被证明先手必胜，有禁手；（15</em>15）；<br>    六子棋：先手下一子，之后每手下两子；（19*19）；</p>
<p>棋类的分类：<br>    参与人数划分：<br>        双人：象棋、围棋、五子棋等；<br>        多人：跳棋；<br>    按兵种分：<br>        单一兵种：围棋、五子棋、六子棋；<br>        多兵种：国际象棋、中国象棋；<br>    按着法分：<br>        走子类：开局摆好棋子后，轮流走动棋子；象棋等；<br>        添子类：开局盘面无子，轮流落子；围棋等；<br>        吃子类：对局过程中可以吃掉对方的子；象棋、围棋等；<br>        混合类：在添子的过程中可以吃子、在走子的过程中可以吃子还可以添子；日本将棋等；<br>    （通常情况下下棋的双方轮流走子，但是有的棋可以一次下多步，连续施着称为轮）<br>    按胜负判决分类：<br>        擒获首领：如象棋；<br>        摆成形状：如五子棋；<br>        占领领域：如围棋；<br>        剩余子粒：如黑白棋；<br>        活动剩余：如亚马逊；<br>        到达目标地：如跳棋；</p>
<h3 id="计算机博弈"><a href="#计算机博弈" class="headerlink" title="计算机博弈"></a>计算机博弈</h3><p>将搜索应用于博弈的思想：<br>    1）任何一方搜索时都要考虑对方可能的走步；<br>    2）优秀的搜索，不应该只考虑对方一步，而是考虑若干步；<br>    3）这个过程动态进行，当考虑对方若干步走了一子之后，待对方走子后，要重新进行考虑若干步；<br>    （即若干步的向后考虑只影响当前步的走子）</p>
<p>我们关注的博弈满足如下特点：<br>    1）双人对弈，双方轮流走步；<br>    2）信息完备，双方看到的棋局是一样的；<br>    3）零和博弈，非合作博弈，一方的收益必然意味着另一方的损失；</p>
<p>博弈的基本原理（普遍、基本、其他规律的基础）<br>    1）棋类要素，棋盘、棋子、棋规（着法与胜负规则）；<br>    2）弈棋要素，用着法推演局面，从有利局面中选择着法；<br>    3）局面评估，指标分析，根据棋种不同而具体分析；<br>    （实际上这三个点构成了我们后续博弈树的基础）</p>
<p>弈棋过程的形式化描述：</p>
<ol>
<li>$S_n$：n手之后的局面</li>
<li>$q_{n+1}$：第n+1手</li>
<li>$Q$：棋谱$q$的集合</li>
<li>$Q_odd,Q_evn$：红方与黑方的着手</li>
</ol>
<p>棋局状态展开（计算机基于搜索的博弈过程）？<br>    1）在当前棋局状态上，使用着法生产器，由具体的着法算子生成可能的着法；<br>    2）使用对应的着法，考虑转移后的棋局状态，即博弈树展开；<br>    3）根据具体的搜索策略，在博弈树上做搜索；<br>    4）对搜索到的叶节点做状态评估，最终确定着法？</p>
<h3 id="棋局要素的数据结构"><a href="#棋局要素的数据结构" class="headerlink" title="棋局要素的数据结构"></a>棋局要素的数据结构</h3><p>计算机博弈数据结构主要涉及：<br>    1）对棋盘、棋子、棋局进行编码；<br>    2）对着法、规则、知识等进行编码；<br>    3）编码得到数据元素，将各种数据元素以特定关系构成数据结构；</p>
<p>常见的编码方法：<br>    1）棋盘：棋位编码（eg，数字对应位置）<br>    2）棋子：角色和方（eg，二元组）<br>    3）着法：棋位序列（eg，数字向量）<br>    4）局面：棋位棋子对的向量（eg，1）、2）构成的二元组，形成的向量）<br>    5）对局过程：着法形成的向量<br>    （当然具体的棋要考虑具体的编码方式）</p>
<p>局面（盘面）存储的最好方式：<br>    Zobrist哈希技术：将盘面转换为哈希数，哈希数即key、盘面即value；<br>        1）基本原理<br>            1）每个棋子的位置、颜色（所属方）、角色，转换为64位随机数；<br>            2）将棋盘上的每个棋子对应的随机数异或求和；<br>            3）得到的结果作为64位哈希数，key，作为盘面的索引值（Zborist键值）；<br>        2）好处：<br>            根据哈希数的定义，当盘面变化的时候只需要用当前盘面的哈希数与相应变化棋子的哈希数异或一次，就可以得到新的盘面对应的哈希数；<br>        3）关键点：<br>            1）在转换为64位随机数的时候，我们要使用到哈希算法，本质是将任意长度的二进制值映射为固定长度的较小二进制值，即哈希值；（eg，因为棋子本质上经过编码在计算机中最终是以01的形式存储的，它可能是一段很长的01串，被我们转换为了一个64位的01串）<br>            2）哈希算法要求对应的散列空间，满足：想找到两个不同的输入可以满足有相同的散列值，在计算上是不可能的；<br>        4）注意：<br>            1）我们无法直接将Zobrist的哈希数key直接转换为value，也没必要这样做，因为设计Zobrist表的目的本身是为了指纹而非压缩；通常可以将盘面作为value存储起来，key即哈希数，建立一个“一一映射”，就像我们开头提到的一样；（这样我们能够给基于一个key，快速找到一个盘面，当然如果是之前没有保存过的key-value，是找不到的）<br>            2）Zobrist技术的应用通常是为了：<br>                1）快速识别：判断两个局面是否相同；<br>                2）重复检测：检测是否到达过某个局面；<br>                3）查表加速：在置换表中快速查找局面的评估值；<br>                （而非完全把弈棋过程转换为异或运算，抛弃其他信息；这也是肯定做不到的，因为信息就不对等）</p>
<p>位棋盘（Bit Board）<br>    应用场景：在着法生成是需要对棋局进行评估，评估的时候时常只关心一些棋子的分布，这是可以用比特棋盘（位棋盘）；<br>    使用方式：本质就是棋子状态条件的布尔表示；<br>    形式化表示：$B &#x3D; {b_{i,j}}<em>{m \times n}$，其中$b</em>{i,j} &#x3D; 1,when:s_{i,j} &#x3D; true$，以及$b_{i,j} &#x3D; 0,when:s_{i,j}&#x3D;false$；而$s_{i,j}$就是棋位$(i,j)$的布尔条件；<br>    eg：当关注某些棋位上是否有黑车时，条件就可以是在$(i,j)$上有黑车时，$s_{i,j}&#x3D;true$；</p>
<h3 id="博弈树展开与分析"><a href="#博弈树展开与分析" class="headerlink" title="博弈树展开与分析"></a>博弈树展开与分析</h3><p>博弈树的基本概念：<br>    博弈树：树枝和节点组成的单向无环图；<br>    树枝：对应着法；<br>    节点：对应着法生成的局面；<br>    <strong>博弈树的展开：着法的生成，代表从当前局面的演化和发展，是进行局面分析的基础</strong>；（当前局面作为根节点）；<br>        常见的着法生成策略（展开策略）：<br>            1）选择生成：根据当前局面，生成部分可行的着法，不考虑其他着法；（eg：象棋被将军的情况下要避将）；<br>            2）渐进生成：先生成一些着法，沿着某个着法沿伸下去，直到足以中止，再考虑其他着法；（eg：象棋先考虑吃子，再考虑非吃子）<br>            3）完全生成：一次产生所有的着法；</p>
<p>具体的着法生成方法：<br>    走子类：<br>        1）棋盘扫描法：<br>            在当前局面中逐一考虑可行的落子点；<br>            通常没有实战意义；<br>        2）模板匹配法：<br>            对于有特殊要求的走子类，根据具体的棋子与要求，逐一找到可行的落子点；<br>        3）预置表法：<br>            将全部棋子在所有棋位上的着法预先放在表中，开局自动生成放入内存，弈棋过程中直接查找；<br>            空间换时间的做法；<br>    添子类：<br>        通常比较直观，在盘面上合法的空位处即可落子；<br>        （略）</p>
<p>博弈树展开的分析：<br>    分子因子：<br>        节点所对应的局面，有多少种着法就有多少种分支；eg：国际象棋的平均分支因子大约是35；<br>    节点的层数：<br>        博弈树展开的深度；<br>    （分支因子越大、层数越多，博弈树规模就越庞大）<br>    树的复杂度：<br>        将博弈树完全展开，从当前的初始局面，到分出胜负时，博弈树上的节点树；eg：五子棋大约是225；<br>    状态复杂度：<br>        全部可行局面的数量；eg：五子棋大约是2225；<br>    <strong>（由于博弈树种有大量重复的状态，所以树的复杂度远高于状态复杂度）</strong>？为什么五子棋的不是，是特例吗？</p>
<h3 id="计算机博弈求解的基本搜索方法"><a href="#计算机博弈求解的基本搜索方法" class="headerlink" title="计算机博弈求解的基本搜索方法"></a>计算机博弈求解的基本搜索方法</h3><p>优化搜索与博弈搜索：<br>    优化搜索的特点：<br>        1）单一决策主体，即性能指标函数；<br>        2）明确的目标函数与约束条件，可以使用数学模型描述；<br>        3）主要用于解决基本规划和优化问题；<br>        4）进行单步考虑；<br>    博弈搜索：<br>        1）主要特点：<br>            1）两个非合作主体构成；<br>            2）涉及多步考虑；<br>            3）对弈双方对立，零和，是对决策目标的约束；<br>            4）目标函数难以用数学模型描述；<br>        2）主要方法：<br>            使用博弈树对博弈过程进行描述，在博弈树中搜索当前的最佳着法，并亦步亦趋地进行下去。<br>            （即目标是找到最好的“根着法”，并且这不是一个目标，而是当前的目标）</p>
<p>极大极小搜索算法<br>    基本思想：<br>        1）将当前状态作为初始状态，建立一个深度为h的搜索树，h可以表征“视野“，时间允许范围内能看到的最大深度；<br>        2）对所有叶节点的状态进行评价；<br>        3）叶节点回推到根节点，选择其中最好的一个动作决策；<br>    使用到的术语：<br>        1）节点 or 结点；<br>        2）根节点 or 顶节点；<br>        3）叶节点 or 端节点；<br>        4）MAX着棋，MIN着棋：通常分别对应我方、对方<br>        5）评估函数f：当f&gt;0时对MAX方有利；f&lt;0时对MIN方有利；f&#x3D;0时双方势均力敌；f绝对值越大对一方越有利，另一方越不利；<br>    具体的算法流程：<br>        1）从根节点开始按照所有可能的着法，扩展博弈树；使用的着法视哪一方着棋而定，MAX、MIN交替着棋；<br>        2）根节点深度h&#x3D;0，每次往下扩展深度+1，到达深度阈值后停止扩展，此时的节点为叶节点；<br>        3）对所有的叶节点按照评估规则计算估值f；<br>        4）逐层上推非叶子节点的估值，按照规则：<br>            1）若非叶子节点对应局面由MAX着棋，则选择其儿子节点中最大的估值作为自己的估值<br>            2）若非叶子节点对应局面由MIN着棋，则选择其儿子节点中最小的估值作为自己的估值<br>            （这就是选择对自己最有利的动作决策）；<br>        5）上推到根节点时，根节点按照4）中的确定估值规则，选择最好的估值，对应的着法就是当前MAX or MIN方使用的着法；<br>        （于是就完成了一次最大最小搜索，找到了当前最好的动作决策）</p>
<p><strong>棋局评估</strong>：</p>
<p>棋局评估：若叶子节点不能够给出胜-负-和的结果，那么有利局面需要依靠棋局评估</p>
<p>静态估值函数f定义的基本规则：</p>
<ol>
<li>有利于MAX为正</li>
<li>有利于MIN为负</li>
<li>势均力敌为0</li>
<li>根据势态的优劣进行定义，从而完成对叶节点的价值度量</li>
<li>若f(p) &#x3D; +∞则MAX赢，若f(p) &#x3D; -∞则MIN赢</li>
</ol>
<p>对一个棋局状态s的评估函数通常是考虑不同类型的知识后，对每种考虑因素进行估值，最后进行加权求和得到的。</p>
<p>即：$e(s) &#x3D; \sum w_i f_i(s)$</p>
<p>其中估值函数$f_i$可以对应：</p>
<ol>
<li>子力</li>
<li>位置</li>
<li>空间</li>
<li>机动</li>
<li>拍节</li>
<li>威胁</li>
<li>形状</li>
<li>图案</li>
</ol>
<p>等等。</p>
<p>值得注棋局性能 &#x3D; <strong>知识✖️速度</strong>意的是，考虑的因素越多，花费的时间必然越多，这个时候时间就会成为掣肘。而有时候考虑得少，时间少速度快，搜索的速度更快，深度更深。</p>
<h3 id="alpha-beta剪枝"><a href="#alpha-beta剪枝" class="headerlink" title="alpha-beta剪枝"></a>alpha-beta剪枝</h3><h4 id="剪枝思想"><a href="#剪枝思想" class="headerlink" title="剪枝思想"></a>剪枝思想</h4><p>极小极大搜索算法随着搜索深度的增长，节点数会成指数增长，所以要考虑一种剪枝算法</p>
<p>alpha-beta剪枝算法的思想</p>
<ol>
<li>将向下生成和倒推计算结合起来</li>
<li>根据一定的条件判定，尽早地修剪一些无用的分支</li>
<li>生成到达规定深度的节点时立刻计算其估值，一旦某个非端节点有条件计算其倒推值则立刻计算</li>
</ol>
<h4 id="具体规则"><a href="#具体规则" class="headerlink" title="具体规则"></a>具体规则</h4><p><code>需要进一步补充一下具体的例子</code></p>
<ol>
<li>alpha，MAX节点的估值下界，随着子节点生成alpha只可能上升</li>
<li>beta，MIN节点的估值下界，随着子节点生成beta只可能下降</li>
<li>alpha剪枝，MIN节点的beta 小于等于 <strong>先辈</strong>的MAX节点的alpha；则终止MIN子节点向下的搜索，令其估值为beta</li>
<li>beta剪枝，MAX节点的alpha 大于等于 <strong>先辈</strong>的MIN节点的beta；则终止MAX子节点向下的搜索，令其估值为alpha</li>
</ol>
<p>注意：</p>
<ol>
<li>仅在MAX、MIN两种不同类型的节点间比较，同一类型不能比较</li>
<li>不仅要和直接的父节点比较，要和直系的所有先辈比较</li>
<li>当估值被确定后，才能向上传递</li>
<li>这种剪枝方法的结果和极小极大的方法是一致的</li>
</ol>
<h3 id="负极大值搜索"><a href="#负极大值搜索" class="headerlink" title="负极大值搜索"></a>负极大值搜索</h3><h4 id="负极大值搜索的思想"><a href="#负极大值搜索的思想" class="headerlink" title="负极大值搜索的思想"></a>负极大值搜索的思想</h4><p>父节点的估值是各子节点估值的变号极大值，从而避免奇数层极大、偶数层极小的情况。（实际上这样的话每次只用从子节点中挑选MIN的估值，对MAX节点的子节点，MIN意味着负的越多，取正越大；对MIN节点的子节点，MIN意味着正的越少，取负越大）</p>
<p>并且由于实际上，这样处理估值后，无论MAX、MIN都是做的MIN节点的搜索，找子节点中估值最小的，于是只需要实现beta剪枝</p>
<h3 id="蒙特卡洛树搜索"><a href="#蒙特卡洛树搜索" class="headerlink" title="蒙特卡洛树搜索"></a>蒙特卡洛树搜索</h3><h4 id="Mento-Carlo的基本思想"><a href="#Mento-Carlo的基本思想" class="headerlink" title="Mento Carlo的基本思想"></a>Mento Carlo的基本思想</h4><ol>
<li>从某一棋局出发，随机走棋</li>
<li>走棋必须按照对应的规则，但不需要任何策略</li>
<li>进行非常多次对局后，可以基于统计，给出该棋局的固有胜率和胜率最高的着法</li>
</ol>
<h2 id="ch4：局部搜索"><a href="#ch4：局部搜索" class="headerlink" title="ch4：局部搜索"></a>ch4：局部搜索</h2><p>引入：<br>    无信息or有信息的搜索：<br>        1）问题的解是一个路径（动作序列）；<br>        2）关注的是路径的代价；<br>        3）环境是可观测的、确定、静态、已知的；<br>    局部搜索：<br>        1）问题的解是一个状态；<br>        2）不需要关注到达该状态的过程；<br>        eg：<br>            1）八皇后问题，只关注最终皇后的排列位置；<br>            2）优化问题求解，只关注最终的解是什么；<br>            3）作业空间调度，只关注调度的方案是什么；<br>            4）旅行商问题，寻找一条满足条件的路径；<br>        （注意甄别，问题的解是路径，这里的路径是指搜索过程的状态序列 or 动作序列；当然动作序列就对应了状态序列，只需要按照动作依次进行状态转移即可；而不是指具体的什么路径。虽然旅行商问题中最终也要找到一条路径，但是那条路径实际上是一个状态）<br>    局部搜索算法的思想：<br>        1）目标：在状态空间（所有状态的集合）中搜索一个最大的状态；<br>        2）思想：从单独的一个状态出发，通常只移动到与之相邻的状态，并且不保留解的路径；<br>        （简单来说，维持当前状态，不断改进；）<br>    局部搜索算法的优点：<br>        1）需要很少的内存；<br>        2）可以从很大、无限的状态空间中找到合适的解；</p>
<p>优化问题与状态空间地形图：<br>    优化问题：<br>        1）目标函数的目标：最高峰，全局最大值；<br>        2）代价函数的目标：最低谷，全局最小值；<br>    目标函数对应的状态空间地形图：<br>        1）shoulder；<br>        2）Global maximum；<br>        3）Local maximum；<br>        4）“flat” local maximum；<br>    对应如图：<br>    <img src="/Intro-of-AI/p1.png" alt="状态空间地形图">;</p>
<p>局部搜索算法1：爬山法<br>    思想：随机选择一个位置爬山，每次朝着更高的方向移动，直到到达山顶；<br>    具体的算法：<br>        1）将当前节点的估值与邻居节点进行比较；<br>        2）如果当前节点估值最好，则返回当前节点作为最大值（山峰）<br>        3）否则使用估值更好的邻居节点，替换当前节点；<br>        4）循环这个过程直到退出；<br>    伪代码如图所示：<br>    <img src="/Intro-of-AI/p2.png" alt="爬山搜索"><br>    eg：<br>        8皇后问题：<br>            目标：任何一个皇后都不会攻击到其他的皇后；<br>            耗散值h：存在冲突的皇后对数；<br>            状态转移（决定相邻节点）：<br>                考虑皇后只能在该列上移动，$8\times 8$的棋盘，则有56个后继状态；<br>            局部搜索求解：<br>                每次考虑相邻状态中耗散值最小的，直到找到耗散值为0的状态，即目标状态；<br>    存在的问题：<br>        局部最优解问题，locl maximum or flat local maximum（山脊与高原）；</p>
<p>局部搜索算法2：随机重启爬山法<br>    思想：在到达局部最优的时候，通过随机生成初始状态来引导爬山法搜索；<br>    具体的算法：<br>        注意：<br>            1）实际上是否处于局部最优在某些情况下比较难判断，我们这里假设全局最优已知；<br>            2）如果我们已经知道了全局最优，为什么还要搜索？实际上这里的全局最优已知，指的是全局最优满足的条件是知道的，我们可以判断是否到达了全局最优；<br>            3）eg，八皇后中，耗散值为0的状态一定是全局最优，尽管我们不知道具体的状态是什么；<br>        步骤：<br>            1）设定一个搜索步数阈值；<br>            2）使用爬山法，但是返回的不是当前估值最好的节点，而是必须返回满足全局最优的节点；<br>            3）如果不是全局最优，则继续搜索；<br>            4）超过搜索步数则随机重置初始状态；<br>    完备性：<br>        完备性在概率上接近1，因为理论上总会随机到最优解作为初始状态；</p>
<p>局部搜索算法3：模拟退火算法<br>    物理背景：<br>        1）学习的冶金学中金属加热-冷却的过程，模拟了物理退火的过程；<br>        2）加热、等温、冷却；<br>        3）<br>    基本思想：<br>        1）与前面两种相同，属于一种贪心的算法，只是在搜索过程中引入了随机因素，迭代更新的时候会以一定的概率接受一个比当前解更差的解，因此有希望跳出局部最优，到达全局最优；<br>        如图所示：<img src="/Intro-of-AI/p3.png" alt="模拟退火算法"><br>    具体过程：<br>        1）初始化：初始温度t0，初始状态s0，全局轮次k；<br>        2）状态产生函数，产生新状态<br>        3）状态接受函数，判断是否接受新状态；<br>        4）抽样稳定准则，判断是否退温；<br>            是则进入5）；<br>            否则回到2）；<br>        5）适当退温，更新t，k&#x3D;k+1；<br>        6）算法终止准则，判断是否终止；<br>            是则终止；<br>            &#x2F;&#x2F;或许可以记录过程中的最好状态，最后返回，尽管这里没提到；<br>            否则回到2）；<br>    关键点：<br>        三函数+两准则+初温：<br>            1）代价函数 or 目标函数C<br>                原则：<br>                    1）代价 or 代价函数，更好的解对应越小，坏的解对应越大；（当然可以更改）；<br>                作用：衡量解的好坏；<br>            2）状态产生函数<br>                原则：产生的候选解可以遍布状态空间；<br>                方法：在当前状态的领域结构用一定概率方式产生新状态；<br>            3）状态接受函数：<br>                原则：<br>                    1）固定温度下，接受好的解的概率更大；<br>                    2）随温度下降，接受坏的解的概率下降；<br>                    3）当温度接近0，只能接受更好的解；<br>                方法：<br>                    通常采用$min[1,exp(-\Delta C&#x2F;t)]$；<br>                    1）对于好的新状态，该函数值为1，必定会接受；<br>                    2）对于坏的新状态，t较大时，该函数值在0～1之间，以概率值接受；<br>                    3）当t较小时，对于好的新状态函数值为1，必定会接受；坏的新状态，函数值接近0，基本不会接受；<br>                具体使用：<br>                    if $min[1,exp(\Delta C&#x2F;t)] &gt; rand(0,1)$:<br>                        s &#x3D; si;&#x2F;&#x2F;新状态；<br>            4）抽样稳定准则（内循环终止准则）：<br>                常用方法：<br>                    1）检验目标函数均值是否稳定；<br>                    （可能需要维护一个目标函数均值，在状态变化的时候不不断更新）；<br>                    2）连续若干步的目标值变化较小；<br>                    （可能需要记录状态变化时的目标值变化）<br>                    3）按一定的步数抽样；<br>                    （这种就类似给内循环设置次数？）<br>            5）外循环终止准则：<br>                常用方法：<br>                    1）设置终止温度阈值，低到一定值；<br>                    2）设置外循环迭代次数；<br>                    3）算法搜索到的最优值连续若干步保持不变；<br>                    4）概率分析？<br>            6）初温设置：<br>                常用方法：<br>                    1）均匀抽样一组状态，用状态的目标值C到方差作为初温；<br>                    2）随机产生一组状态，计算两两状态间的目标值差的最大值，根据差值利用一定函数确定初温；<br>                    3）利用经验公式；<br>        物理学背景：<br>            1）在温度T，分子停留在状态r满足玻尔兹曼概率，这意味着同一温度下，分子停留在能量小的状态比停留在能量大的状态概率更大；<br>            <img src="/Intro-of-AI/p4.png" alt="模拟退火物理学背景公式"><br>            2）从这种概率关系下，引出了Metropolis准则——“以一定概率接受新的状态”</p>
<p>局部性搜索算法4：遗传算法</p>
<h2 id="ch5：不确定性推理"><a href="#ch5：不确定性推理" class="headerlink" title="ch5：不确定性推理"></a>ch5：不确定性推理</h2><p>不确定性推理：<br>    含义：<br>        从不确定的初始证据出发，运用不确定的知识，最终推出以某种不确定度成立的结论的过程；<br>    不确定环境：<br>        不确定性推理进行的环境；<br>        包括：<br>            证据不确定：<br>                证据是初始证据或推出的证据；<br>                不确定性：<br>                    歧义性、不完全性、不精确性、随机性、不一致性；<br>            规则不确定：<br>                规则是启发类的知识，描述由已有的证据可以推得的结论；<br>                不确定性：<br>                    证据组合、规则自身、结论；<br>    推理不确定的体现：<br>        知识不确定性的动态积累和传播；<br>    一个关键的问题：<br>        如何计算不确定程度（不确定度）<strong>定量的对不确定性进行描述，这是本章展开的关键</strong>；</p>
<p>贝叶斯决策论：<br>    一种使用概率框架进行决策的不确定推理工具；<br>    作用：<br>        针对分类问题，在所有概率已知的理想情况，贝叶斯决策考虑如何基于这些概率和误判损失来选择最优的类别标记；<br>    问题的形式化描述：<br>        1）$N$种类别标记：$y &#x3D; {c_1,c_2,\dots,c_n}$；<br>        2）误判损失（将$c_j$判为$c_i$）：$\lambda_{ij}$；<br>        3）后验概率：$P(c_i|x)$<br>        （对于一个具体的样本，其属于一个类的概率）<br>        4）条件风险（将样本x分类为$c_i$的期望损失）：<br>            $R(c_i|x) &#x3D; \sum_{j&#x3D;1}^N \lambda{ij} P(c_j|x)$；<br>            （简单理解就是，该样本有概率是1～N中的任意一类，但是现在分类为$c_i$，必然要承担相应的损失）<br>        5）目标：<br>            找到判定准则：<br>                $h:X\rightarrow Y$；<br>            使得总体风险最小：<br>                $argmin_{h} E_x[R(h(x)|x)]$；<br>            （实际上就是最小化对于样本x这一随机变量的随机变量函数——条件风险的期望）<br>    贝叶斯最优分类器：<br>        用来实现贝叶斯决策论目标的工具；<br>        贝叶斯判定准则：<br>            内容：为了最小化总体风险，只需要在每个样本上选择那个能使条件风险$R(c|x)$最小的类别标签；<br>            形式化描述：<br>                即对样本x，其标签应该是：<br>                $h^* &#x3D; argmin_{c\in y} R(c|x)$；<br>        注意：<br>        1）满足贝叶斯判定准则的分类器即<strong>贝叶斯最优分类器</strong>；<br>        2）此时总体风险称为<strong>贝叶斯风险</strong>；<br>        3）贝叶斯风险能够表征模型的最好性能，是机器学习的理论上限；<br>        （<em><em>1 - R(h</em>)是这里的3）的描述，但是应该不太准确？</em><em>）<br>    问题转换：<br>        一个具体的情况：<br>            $\lambda{ij} &#x3D; 0 if:i&#x3D;j;else:lambda{ij} &#x3D; 1$；<br>            （没有误判的时候不存在损失，其他情况损失都是1）<br>        这时条件风险为：<br>            $R(c|x) &#x3D; 1 - P(c|x)$；<br>        此时贝叶斯最优分类器对于样本x：<br>            给出的标签满足：<br>                $h^</em>(x) &#x3D; argmax_{c \in y}P(c|x)$；<br>            注意：<br>                1）简单来说，我们选择的标签是要让后验概率最大的标签；<br>        注意：<br>            1）这种情况下，贝叶斯最优分类器称为<strong>贝叶斯最小化错误率分类器</strong><br>            2）问题的关键被转换到了关注如何选择一个<strong>后验概率</strong>更大的标签上；<br>    后验概率：<br>        定义：<br>            想要清晰的理解它比较困难，这是随着统计学的演进逐渐被明确的概念，属于现代统计学奠基石部分的概念；<br>            于是简单放一个参考在这里<img src="/Intro-of-AI/p5.png" alt="后验概率"><br>        简单理解：<br>            先验概率：<br>                “先”于证据的概率。它是基于历史数据、长期经验或领域知识，在获得当前观测证据之前，对事件发生可能性的一般性估计。它代表了我们的初始信念。<br>            （<strong>先验概率可以直接按照频率估计</strong>）<br>            后验概率：<br>                “后”于证据的概率。它是我们在获得了特定观测数据x之后，利用<strong>贝叶斯定理</strong>对先验概率进行更新后得到的概率。它代表了结合新证据后，我们对事件发生的修正信念。<br>            后验概率是在先验概率与实验数据的基础上，由贝叶斯定理导出的概率；<br>    机器学习的目标：<br>        在现实中后验概率是无法直接得出的，机器学习实际上要做的就是基于有限的样本尽可能准确地估计出后验概率；</p>
<p>估计后验概率从而实现分类的两种策略模型：<br>    判别式模型：<br>        1）给定x，通过直接建模$p(c|x)$来预测c；<br>        eg：决策树、BP神经网络、支持向量机<br>        （分类的结果，对于一个样本，通常就是其属于各个类别的概率，然后取概率最大的）<br>    生成式模型：<br>        1）先对<strong>联合概率分布</strong>P(x,c)进行建模，再由条件概率的定义得到后验概率：$P(c|x) &#x3D; \frac{P(x,c)}{P(x)}$</p>
<p>基于生成式模型的问题转换：<br>    我们知道后验概率：$P(c|x) &#x3D; \frac{P(x,c)}{P(x)}$<br>    贝叶斯定理：$P(c|x) &#x3D; \frac{P(x|c)P(c)}{P(x)}$<br>        1）其中$P(c)$是先验概率，可通过各类别出现的频率估计（大数定理）<br>        2）$P(x)$是证据因子，样本、实现现象，与类标记c无关；<br>        3）$P(x|c)$是类c中出现样本x的<strong>类条件概率</strong>；<br>        （先确定了一个类别，其中出现一个具体的样本的概率）<br>    问题转换：<br>        如何估计<strong>类条件概率</strong>：$P(x|c)$；</p>
<p>极大似然估计：<br>    作用：估计类条件概率的常用策略；<br>    思想：<br>        假定类条件概率具有某种确定的概率分布形式，基于训练样本对概率分布参数进行估计；<br>    问题转换：<br>        假设$P(x|c)$具有确定形式，被参数$\theta_c$唯一确定，我们的任务就是要使用训练集$D$估计参数$\theta_c$；<br>        （概率模型的训练过程就是参数的估计过程）</p>
<p>估计确定类条件概率的参数：<br>    频率主义学派的思想：<br>        参数虽然未知，但是存在客观值，可以通过优化似然函数等准则来确定参数；<br>    具体的做法：<br>        $\hat{\theta}<em>c &#x3D; argmax</em>{\theta_c} LL(\theta_c)$<br>        其中：<br>            1）$LL(\theta_c) &#x3D; \log P(D_c|\theta_c) &#x3D; \sum_{x \in D_c} \log P(x|\theta_c)$<br>            2）训练集中第c类样本的一个子集$D_c$<br>            3）假设集合中的样本是独立的，原本的式子是：<br>                $P(D_c|\theta_c) &#x3D; \prod_{x\in D_C}P(x|\theta_c)$；<br>            4）为了避免连乘溢出，所以改为了对数似然；<br>            5）实际上就是<strong>我们通过实验找到了这么多的样本，那么就应该尽量让参数接近使得这种情况出现的概率较大的情况</strong><br>            （枪响了，兔子死了，很大概率是枪打死了兔子）<br>    eg：<br>        <img src="/Intro-of-AI/p6.png" alt="极大似然估计例子"><br>    贝叶斯学派的思想：<br>        认为参数是未观察到的随机变量、其本身也可由分布，因此可假定参数服从一个先验分布，然后基于观测到的数据计算参数的后验分布。<br>    <strong>最终使用(生成式模型，类条件概率)<strong>：<br>        1）我们现在有很多样本x以及对应的标签c；<br>        2）那么对于标签c的类条件概率P(x|c)可以使用极大似然进行估计；<br>        （</strong>这两步就类似于在对分类器进行训练</strong>）<br>        3）从而我们可以对于一个新来的样本x，按照P(x|c)的参数表达式进行计算；<br>        4）进一步可以估计后验概率P(c|x)；<br>        5）对于新来的样本x，考虑所有标签，分别按照上述步骤计算后验概率，选择后验概率最大的对应的标签；</p>
<p>朴素贝叶斯分类器：<br>    含义：基于类条件概率和属性条件独立性假设的贝叶斯最优分类器；<br>    目标：<br>        1）与贝叶斯最优分类器一样；<br>        2）满足分类结果让后验概率最大（总体风险最小）的分类器；<br>    属性条件独立性假设：<br>        每个属性独立地对分类结果发生影响；<br>    方法：<br>        1）基于类条件概率和贝叶斯定理可以将后验概率展开；<br>        2）使用属性条件独立性假设，将类条件概率展开为属性条件概率；<br>    具体表达式：<br>        $P(c|x) &#x3D; \frac{P(c)}{P(x)} P(x|c)$；<br>        进一步：$\frac{P(c)}{P(x)} \prod_{i&#x3D;1}^d P(x_i|c)$；<br>        其中：<br>            1）后验概率、先验概率、类条件概率，不多赘述；<br>            2）$P(x_i|c)$ 是属性条件概率，即对于该类，出现了一个具体属性的概率；<br>            3）$x$是具体的样本，$x_i$是它的一个属性，一共有d个属性；<br>    最终的分类器：<br>        分类结果对于所有样本x，得到的标签c满足如下表达式的分类器，就是贝叶斯朴素分类器：<br>        $h_{nb}(x) &#x3D; argmax_{c\in y} P(c)\prod_{i&#x3D;1}^d P(x_i|c)$；<br>        注意：<br>            1）去掉了后验概率展开为属性条件概率的式子中的$P(x)$，因为无论是否考虑它，优化都是等价的；<br>    属性条件概率的计算：<br>        1）离散属性：使用频率直接进行估计；<br>            eg：<img src="/Intro-of-AI/p7.png" alt="离散属性条件概率的计算">;<br>        2）连续属性计算：按照分布进行计算；（当然，分布参数又可以按照类条件概率的估计那样，使用极大似然法进行计算）<br>            eg：<img src="/Intro-of-AI/p8.png" alt="连续属性条件概率的计算">;<br>    <strong>最终的使用(朴素贝叶斯分类器)</strong><br>        1）对于已有的样本x，以及其对应的标签c；<br>        2）使用已有所有样本的属性xi，以及对应的标签c，计算：<br>            1）离散属性条件概率（直接按照频率进行计算）；<br>            2）估计连续属性条件概率的概率分布（或许可以用极大似然）；<br>        （<strong>对于朴素贝叶斯分类器的训练</strong>）<br>        3）对于新来的样本x；<br>        4）考虑样本属于类别c<br>            按照前面计算的离散属性条件（属性属于该类别）概率，设置其新样本各离散属性的条件概率（对应相等）；<br>            按照前面估计的连续属性概率分布，将新样本的连续属性带入概率分布表达式，计算出连续属性条件概率；<br>        5）根据计算出的所有属性条件概率，计算类别c对应的后验概率；<br>        6）按照上面3～5的步骤，考虑所有的类别，取后验概率最大的类别作为该样本x的类别；</p>
<p>拉普拉斯修正：<br>    目的：<br>        修正先验概率和离散属性条件概率的计算；<br>    背景：<br>        若某个属性值在训练集中没有与某个类同时出现过，则直接计算会出现问题，比如“敲声&#x3D;清脆”测试例，训练集中没有该样例，因此连乘式计算的概率值为0，无论其他属性上明显像好瓜，分类结果都是“好瓜&#x3D;否”，这显然不合理。<br>    修正方法：<br>        1）类先验概率：$\hat{P(c)} &#x3D; \frac{|D_c|+1}{|D|+N}$；<br>        2）离散属性条件概率：$\hat{P}(x_i|c) &#x3D; \frac{|D_{c,x_i}|+1}{|D|+N_i}$；<br>        其中：<br>        1）$|D_c|$是数据集中类别c对数据数量；<br>        2）$|D|$是所有数据的数量；<br>        3）$N$是所有可能的类别数量；<br>        4）$N_i$是属性i所有可能的取值数量；</p>
<p>贝叶斯网络：<br>    含义：<br>        1）结点间增加了连接强度的因果关系网；<br>        2）贝叶斯网络&#x3D;网络结构+CPT<br>    其中：<br>        1）连接强度对应的是条件概率；<br>            eg：<br>                1）$P(B|A)$：A到B到连接强度；<br>                2）$P(B|AC)$：A、C对B到联合作用；<br>        2）CPT：条件概率表；<br>    一个贝叶斯网络的例子：<br>        <img src="/Intro-of-AI/p9.png" alt="贝叶斯网络"><br>    贝叶斯网络的性质：<br>        条件独立：<br>            含义：有结点A、B和C，若 P(A|BC) &#x3D; P(A|B)，则称A和C在B条件下独立、A在B条件下独立于C，或A和C关于B独立。<br>        几种典型的条件独立情况：<br>            1）串行连接：如图，B确定的时候，A与C关于B条件独立；<br>            <img src="/Intro-of-AI/p10.png" alt="串行连接"><br>            2）分叉连接：如图，给定父节点确定的时候，各子节点条件独立；<br>            <img src="/Intro-of-AI/p11.png" alt="分叉连接"><br>            3）汇集连接：如图，后继节点<strong>不确定</strong>的时候，同一后继的前驱节点间条件独立；（<strong>即使后继节点的再后继的某个结点确定，前驱节点就不独立了</strong>）<br>            <img src="/Intro-of-AI/p12.png" alt="汇集连接"><br>    贝叶斯网络常见的推理模式：<br>        1）因果推理：已知父节点，计算子节点的条件概率；<br>        （在父节点确定了，子节点出现某种状态的概率）<br>        2）诊断推理：已知一个子节点，计算父节点的条件概率；<br>        （子节点确定了，计算父亲节点出现某种状态的概率）<br>        3）辩解推理：已知父节点和子节点，计算其他父节点的条件概率；<br>        （后继节点以及一个前驱节点确定了，计算该后继节点的另一个前驱节点出现某种状态的概率）<br>    注意：<br>        1）<strong>贝叶斯网络不允许包含因果循环</strong>；<br>        2）在这些推理的过程中，可以使用条件概率的一系列性质，来计算出需要推理访问的节点的状态；</p>
<h2 id="机器学习概述"><a href="#机器学习概述" class="headerlink" title="机器学习概述"></a>机器学习概述</h2><p>机器学习：<br>    定义：用机器来模拟人类学习活动，本质是要从大量数据中学习到隐藏的模式，最终要用这种模式对新的样本进行判别和预测；<br>    不同流派：<br>        1）符号主义：用符号、规则和逻辑来表征知识进行推理；<br>        2）贝叶斯派：基于贝叶斯定理；<br>        3）连接主义：神经网络；<br>        4）进化主义：用计算机模拟进化过程；<br>        5）行为类推主义：根据约束条件来优化函数；<br>    目标：<br>        预测事物；</p>
<p>基本概念：<br>    特征：<br>        特征变量：用来描述问题一种性质的属性；<br>        属性值：特征变量的一个具体取值；<br>        特征向量：由多个特征变量组成的，用来描述问题对象的整体性质；<br>        特征空间：变量组成的空间；<br>    数据集合：<br>        样本空间：特征空间中的一组示例；<br>            记法：$D &#x3D; {x_1,x_2,\dots,x_m}$；<br>        标记空间：标签变量or预测变量的取值集合；<br>            记法：$Y$；<br>        样例集合：特征向量（样本）与标签变量对集合；<br>            记法：$D &#x3D; {(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)}$；<br>        数据集：<br>            用于训练的样例构成的集合，是样例集合的子集；<br>        测试集：<br>            用来对学习到的模型进行检测的样例构成的集合，是样例集合的子集；<br>    常见的学习任务：<br>        1）分类；<br>            1）Y是离散值；<br>            2）常见二分类、多分类；<br>        2）回归；<br>            1）Y是连续值；<br>            2）目标是预测；<br>        3）聚类；<br>            1）无监督学习（1、2是有监督），没有Y信息；<br>        4）强化学习；<br>            1）通过与环境交互逐步学习；<br>        5）泛化能力；<br>            1）考量学习结果对新样本的适应能力（本质就是对样本空间的描述能力，ps不是对样例空间）<br>    假设空间：<br>        机器学习的结果是一个<strong>从样本空间到标记空间的映射</strong>，所有可能映射的集合是假设空间；<br>        记法：$H,s.t.f:G\rightarrow H,f\in H$；<br>    版本空间：<br>        假设空间的一个子集，<strong>与训练样例一致</strong>的所有假设的集合；（这里的假设就是映射）；<br>    归纳偏好：<br>        在机器学习算法学习中，可能有多种假设均满足训练样例，但是具体的算法会偏好于其中一个；<br>            1）奥卡姆剃刀：当有多个假设与观察结果一致的时候，选择最简单的那个；<br>            2）NFL定理：总误差与算法无关，不可能找到一个适用于所有场景的算法，具体的问题选择具体的算法</p>
<p>机器学习的基本流程：<br>    1）定义分析目标；<br>    2）收集数据；<br>    3）数据预处理；<br>        数据中可能存在噪声、不一致、异常等问题，需要数据清洗；<br>    4）数据建模；<br>        算法本身没有绝对的好坏，不同算法有各自适用的范围（NFL定理）<br>    5）模型训练；<br>    6）模型评估；<br>    7）模型应用；</p>
<p>模型评估：<br>    目标：评估机器学习结果在多大程度上能够帮助实现业务目标，量化相关评估结果；<br>    常用量化指标：<br>    （注意：<strong>这些指标没有明确说明的，默认是在测试集上的表现</strong>）<br>        错误率：<br>            1）分类错误的样例占样例总数；<br>            2）$E &#x3D; a&#x2F;m$；<br>        精度：<br>            1）$acc &#x3D; 1 - E$；<br>        误差：预测输出与真实情况的差异<br>            1）训练误差（经验误差）：在训练集上的误差；<br>            2）泛化误差：在测试集（新样本）上的误差；<br>    常见问题：<br>        1）过拟合，过度学习了训练样例中的细节特征，导致泛化能力下降；<br>            eg：认为必须边缘有锯齿的才是树叶；<br>        2）欠拟合，没有充分学习到训练样例的特征，导致泛化能力下降；<br>            eg：认为绿色的大树也是树叶；<br>    训练集与测试集的划分：<br>        1）留出法：<br>            1）直接讲数据集D拆分为两个互斥的集合，其中S是训练集，T是测试集；<br>            2）$D &#x3D; S \cap T$；<br>        2）交叉验证：<br>            1）按照一定的分布规律，将数据集D依次排列为互斥子集；<br>            2）几个互斥的子集组成一组，其中有训练集和测试集；<br>        3）自助法；<br>            1）直接从原始的数据集中自助采样样例，生成训练集；<br>            2）注意采样之后会放回，所以一个样本m可能被多次采到；<br>            3）某个样本永远不会被采到的概率是$1&#x2F;e$；（等概率采样）；<br>    参数调优：<br>        一些算法中存在人为设置的超参数，需要手动进行调节；<br>    针对分类问题性能度量：<br>        已知：<br>            1）数据集 $D &#x3D; {(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)}$；<br>            2）机器学习器 $f:y &#x3D; f(x)$；<br>        性能度量：<br>            1）错误率：$E(f,D) &#x3D; \frac{1}{m} \sum \textbf{1}_{f(x_i)\ne y_i}$<br>            2）精度：$acc(f,D) &#x3D; 1 - E(f,D)$；<br>            3）混淆矩阵：<br>                1）从二分类问题出发，样例的标签有正例、反例；<br>                2）根据<strong>预测</strong>的正反情况与<strong>实际</strong>的正反情况，我们给出基本混淆矩阵的元素；<br>                    1）TP：真正，预测正确，为正例；<br>                    2）FN：假反，预测为反，实际为正；<br>                    3）TN：真反，预测正确，为反例；<br>                    4）FP：假真，预测为正，实际为反；<br>                3）元素名称规律：<br>                    1）T&#x2F;F，代表预测的正确&#x2F;错误；<br>                    2）P&#x2F;N，代表预测结果为正&#x2F;反；<br>                    （组合起来当然也可以推断实际结果）；<br>            4）查准率（precision 准确率）：<br>                1）预测为正的，有多少正确；<br>                2）$P &#x3D; TP&#x2F;(TP+FP)$；<br>                3）理解：声称找到的好东西，里面有多少是真的；<br>                4）eg：邮件过滤通常追求查准率，FP越大代表实际上为垃圾邮件但被判断为重要邮件的数量越大，惩罚越大；（追求准确，宁缺毋滥）<br>            5）查全率（recall 召回率）：<br>                1）实际为正的，有多少被我们预测到了；<br>                2）$R &#x3D; TP&#x2F;(TP+FN)$；<br>                3）理解：对于所有的好东西，你找到了多少；<br>                4）eg：疾病诊断通常追求查全率，FN越大代表实际上有病但是被诊断为每有病的数量越大，惩罚越大；（追求全面，宁可错杀）<br>            5）平衡指标：<br>                F1:<br>                    1）查准率和查全率的调和平均；<br>                    2）$F_1 &#x3D; \frac{2PR}{P+R}$；<br>                    3）当P、R均较高时F1会较高；当P、R偏差较大时F1较小，当P、R均较小时F1较小，用来施加惩罚；<br>                $F_\beta$:<br>                    1）查准率与查全率的加权调和平均；<br>                    2）$F_\beta &#x3D; \farc{(1+\beta^2)PR}{\beta^2 P + R}$<br>                    3）通常设置：<br>                        1）$\beta &#x3D; 1$ 平衡；<br>                        2）$\beta &#x3D; 2$ 重视查全率；<br>                        3）$\beta &#x3D; 0.5$ 重视查准率；</p>
<p>混淆矩阵补充：</p>
<table>
<thead>
<tr>
<th>实际 \ 预测</th>
<th>预测为正类</th>
<th>预测为负类</th>
</tr>
</thead>
<tbody><tr>
<td>实际为正类</td>
<td>TP</td>
<td>FN</td>
</tr>
<tr>
<td>实际为负类</td>
<td>FP</td>
<td>TN</td>
</tr>
</tbody></table>
<p><code>重要点：常用评价指标（分类正确率、回归均方误差、acc、auc、二维表格对应几个指标）</code></p>
<h2 id="ch7：监督学习-决策树"><a href="#ch7：监督学习-决策树" class="headerlink" title="ch7：监督学习-决策树"></a>ch7：监督学习-决策树</h2><p><code>重点？决策树的基本思想、算法过程</code></p>
<p>决策树的基本思想：<br>    1）决策树通过把数据样本分配到某个叶子结点来确定数据集中样本所属的分类</p>
<p>决策树的算法过程：<br>    1）从决策树根结点出发，自顶向下移动，<strong>在每个决策结点都会进行一次划分</strong>，通过划分的结果将样本进行分类，导致不同的分支，最后到达个叶子结点，这个过程就是利用决策树进行分类的过程；</p>
<p>决策树的基本结构：<br>    1）决策结点表示在样本的<strong>一个属性上进行的划分</strong>；<br>    2）分支表示对于决策结点进行划分的输出；<br>    3）叶结点代表经过分支到达的分类结果；</p>
<p>ID3算法：<br>    分支处理：<br>        1）分支属性 选择：对决策节点上选择哪一个属性来对数据集进行划分；<br>        2）分类属性 标签 计算信息熵以及影响算法是否终止<br>            选取原则：<br>                1）要求每个分支中样本的类别纯度尽可能高；<br>                2）不要产生样本数量太少的分支；<br>        2）衡量指标：<br>            信息熵：度量样本纯度的指标<br>            <img src="/Intro-of-AI/p17.png" alt="信息熵"><br>            (<strong>公式与作用</strong>)<br>    算法流程：<br>        1）计算当前分支节点信息熵；<br>        2）选择分支属性，得到当前分支节点的子节点（分支节点）；<br>        3）计算各个子分支节点的信息熵加权和，权重是样本的数量；<br>        4）直到得到的分支节点只有一类标签，则得到了一个叶节点，该标签就是这个叶节点对应的标签；<br>        （or 达到了要求的分类条件：深度限制、样本数量较少，如果此时还有多类标签，则取样本数量最多的标签为对应的标签）；<br>    注意：<br>        ID3算法在完成一次根据某个分支属性进行分类之后，在分支节点中（对应自己的样本集合），再次进行划分，<strong>各个分支节点是独立不相互影响</strong>，即再次考虑信息熵增益最大的时候只需要考虑各个分支节点，自己再次划分的时候信息熵增益最大（<strong>此时对应信息熵计算使用的样本集合D就是分支节点所具有的全部样本组成的</strong>）。</p>
<p>扩展算法的基本思想<br>    1）C4.5算法：使用信息增益率代替信息增益作为划分度量，解决ID3算法往往选择取值较多的分支属性的情况；<br>    <img src="/Intro-of-AI/p18.png" alt="C4.5">（注意图中的例子）；<br>    2）CART算法：二分循环切割，无论选择什么分支属性，都<strong>只分出两个分支</strong>，即构建二叉树；如果分支属性取值不止两个的话，就进行一些组合，最后保留两个组合；使用的是<strong>Gini指数</strong>；<br>    <img src="/Intro-of-AI/p19.png" alt="CART">（注意例子）；</p>
<h2 id="ch8：监督学习-支持向量机"><a href="#ch8：监督学习-支持向量机" class="headerlink" title="ch8：监督学习-支持向量机"></a>ch8：监督学习-支持向量机</h2><p><code>重点？</code><br><code>注意这一节不只涉及支持向量机</code></p>
<p>重要点：<br>    线性回归：<br>        最小二乘法存在闭式解<br>    支持向量机：<br>        大间隔<br>        核函数（四种核函数，不知道该用什么的一般用高斯核函数，sigmod核函数使用比较少）<br>    KNN算法：<br>        多数表决，由输入的k个近邻训练实例多数所属的类来决定</p>
<h2 id="ch9：无监督学习-聚类"><a href="#ch9：无监督学习-聚类" class="headerlink" title="ch9：无监督学习-聚类"></a>ch9：无监督学习-聚类</h2><p>无监督学习：<br>    含义：在没有标签（标注）的样本上进行的学习；<br>    最常用的方法：聚类；</p>
<p>聚类：<br>    目标：<br>        1）将数据样本划分为若干个不相交（通常）的“簇”（Cluster）<br>        2）簇内样本的相似度尽可能的高，簇间样本的相似度尽可能的低；<br>        3）簇内样本与中心的距离尽可能的小，簇间的距离尽可能的大；<br>    一些基本概念：<br>        衡量指标：<br>            外部指标：<br>                使用事先指定的聚类模型作为参考来评判聚类结果的好坏；<br>            内部指标：<br>                不依赖于任何外部知识，只使用参与聚类的样本来评判聚类结果的好坏；<br>        簇的属性：<br>            作用：用来对簇的一些性质进行描述；<br>            1）聚类中心：簇中所有样本的均值（质心）；<br>            2）簇大小：簇中含有样本的数量；<br>            3）簇密度：簇中样本的紧密程度；<br>            4）簇描述：簇中样本的业务特征；</p>
<p>聚类的常见外部指标：<br>    情景：对于含有n个样本点的数据集S，其中的两个不同样本点(xi,xj)，假设C是聚类算法给出的簇划分结果，P是外部参考模型给出的簇划分结果。对于(xi,xj)样本点来说，存在以下四种关系：<br>        1）SS，在结果C、P中(xi,xj)在相同的簇中；<br>        2）SD，在结果C中(xi,xj)在相同的簇中，在结果P中不在相同的簇中；<br>        3）DS，在结果C中不在相同的簇中，在结果P中在相同的簇中；<br>        4）DD，在结果C中不在相同的簇中，在结果P中不在相同的簇中；<br>    四种关系的数量关系：<br>        客观事实：对于任意两个不同的样本(xi,xj)，它们一定只能满足SS&#x2F;SD&#x2F;DS&#x2F;DD四种关系中的一种；<br>        假设：对应令四种关系成立的样本数量对为a、b、c、d；<br>        关系：$a+b+c+d &#x3D; C_n^2$（实际上就是样本对的数量）；<br>    在四种关系的数量关系之上建立的外部指标：<br>        1）Rand统计量：<br>            英文术语：R-Statistc<br>            公式：$R &#x3D; \frac{a+d}{a+b+c+d}$；<br>            含义：？<br>        2）F值：<br>            英文术语：F-Measure；<br>            公式：$F &#x3D; \frac{(\beta^2 + 1)PR}{\beta^2P + R}$；<br>            含义：<br>                1）P表示准确率$P &#x3D; \frac{a}{a+b}$；<br>                2）R表示召回率$R &#x3D; \frac{a}{a+c}$；<br>                （注意区分召回率和Rand Statistic）；<br>                3）$\beta$是参数，当其为1的时候对应F1-Measure；<br>        3）Jaccard系数：<br>            英文术语：Jaccard Coefficient；<br>            公式：$\frac{a}{a+b+c}$；<br>        4）FM指数：<br>            英文术语：Fowlkes and Mallows Index；<br>            公式：$\sqrt{\frac{a}{a+b} \frac{a}{a+c}}$；<br>        四种外部指标的意义：<br>            这四个度量指标的值越大，表明聚类结果和参考模型直接划分的结果越吻合，聚类结果越好；</p>
<p>聚类常见的内部指标：<br>    情景：考虑在聚类分析中，对于两个m维的样本xi &#x3D; (xi1,xi2,…,xim)，xj &#x3D; (xj1,xj2,…,xjm)，对于任意两个样本xi，xj，我们可以考虑距离度量；<br>        1）欧几里得距离；<br>        2）曼哈顿距离；<br>        3）切比雪夫距离；<br>        4）明可夫斯基距离；<br>            公式：$dis_{mind} &#x3D; (\sum_{k&#x3D;1}^m |x_{ik} - x_{jk}|^p)^{\frac{1}{p}}$；<br>            注意：<br>                1）这是距离度量的最一般形式；<br>                2）当p &#x3D; 1的时候，即曼哈顿距离；<br>                3）当p &#x3D; 2的时候，即欧几里得距离；<br>                4）当$p \rightarrow \inf$的时候，即切比雪夫记录；<br>    针对距离度量得出的聚类性能度量内部指标：<br>        1）紧密度(CP)：<br>            英文术语：Compactness；<br>            公式：$CP_c &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^n ||x_i - w_c||$；<br>            含义：<br>                1）$w_c$是簇c的聚类中心，可以用样本加和求平均得到；<br>                2）$CP_c$对应的是簇c的紧密度；<br>                3）对于整个聚类结果的紧密度为：<br>                    $CP &#x3D; \frac{1}{n} \sum_i^n CP_i$；<br>                4）这里两个样本的距离使用的曼哈顿距离，也可以用其他距离替换；<br>            意义：<br>                紧密度的值越小，表示簇内样本点的距离越近，簇内样本的相似度越高；<br>        2）分隔度(SP)：<br>            英文术语：Seperation；<br>            公式：$SP &#x3D; \frac{2}{k^2 - k}\sum_{i&#x3D;1}^k\sum_{j&#x3D;i+1}{k} ||c_i - c_j||$；<br>            含义：<br>                1）聚类的结果是k个簇；<br>                2）其中$c_i$对应的是簇i的聚类中心；<br>            意义：<br>                分隔度的值越大，表示各聚类聚类中心相互之间的距离越远，簇间相似度越低，效果越好；<br>        3）戴维森堡丁距离：<br>            作用：衡量任意两个簇的簇内距离和簇间距离之比，求最大值；<br>            意义：DBI的值越小，表示簇内样本之间距离越小，同时簇间距离越大，即簇内相似度高，簇间相似度低，说明聚类结果越好；<br>        4）邓恩指数：<br>            作用：任意两个簇的样本点的最短距离与任意簇中样本点点最大距离之商；<br>            意义：DVI的值越大，表示簇间样本距离越远，簇内样本距离越近，效果越好；<br>        5）意义总结：<br>            1）CP越小越好；<br>            2）SP越大越好；<br>            3）DBI越小越好；<br>            4）DVI越大越好；</p>
<p>聚类方法一：基于划分的方法<br>    特点：<br>        1）基于划分的方法是最简单、常用的聚类算法；<br>        2）将对象划分为簇进行聚类，每个对象属于且仅属于一个簇；<br>        3）划分追求的结果是，簇间相似度低，簇内相似度高；<br>    常用的基于划分的算法：<br>        1）k-means；<br>        2）k-medoids；<br>        3）k-prototype；<br>    k-means算法：<br>        思想：计算样本点与类簇质心的距离，与类簇质心相近的样本点划分为同一类簇；<br>            1）类簇是指还没有完成最终划分之前的各个簇；<br>            2）常常使用簇中所有样本的均值来表征簇；<br>        算法流程：<br>            1）选取k个类簇，用户指定或随机选取指定k个质心；<br>            2）对剩余样本点，计算到各个质心的欧式距离，将其归入距离最小的质心对应的类簇；<br>            3）每次确定一个样本点所属的类簇，都立刻更新该类簇的质心；<br>            4）所有样本点都划分完毕后，每个簇都有了新的质心；<br>            5）根据新的质心，重新对所有的样本进行划分；<br>            6）重复2～5，直到划分结果基本保持不变，返回结果；<br>        Q：<br>            如何确定划分结果基本保持不变？<br>        优点：<br>            运行效率比较高、容易解释适合高维数据聚类；<br>        缺点：<br>            1）容易局部收敛（贪心策略）；<br>            2）对离群点和噪声点非常敏感；<br>            3）初始聚类中心的选取对算法结果影响很大；<br>            4）不适用于，离散的类别数据；<br>                因为最基础的度量指标是基于距离得到的；<br>                需要做哑变量处理；<br>            5）不适用于非图面形状（非球形）的数据集，尤其是长条形的数据集（结果会与初始目标有非常大的差别）；<br>        注意：<br>            1）变量处理：<br>                离散变量要做哑变量处理，将不能定量处理的变量量化；<br>                原始数据要做标准化处理（防止量纲带来的影响）；<br>            2）k值的选择：<br>                即最终结果应该聚为几个类，这是人为确定的；<br>                通常可以：<br>                    1）使用层次聚类法得到初始的k值，以及初始的簇中心；<br>                    2）使用系统演化的方法；<br>    k-means++算法：<br>        思想：为了克服k-means结果依赖于初始类簇的选择，而提出的；<br>        算法流程：<br>            1）从样本集合中随机选择一个样本点ci，作为第1个聚类中心；<br>            2）计算其他样本点x到“已有的，最近的聚类中心的距离”，d(x)；<br>            3）以概率$\frac{d(x)}{\sum_i^n d(x)}$选择x作为新的聚类中心；（概率越大被选中概率越大）；<br>            4）重复2～3，直到选出k个聚类中心；<br>            5）使用这k个聚类中心，进行k-means算法；</p>
<p>聚类方法二：基于层次聚类<br>    BIRCH算法<br>    CURE算法</p>
<p>聚类方法三：基于密度聚类<br>    思想：<br>        将样本中的高密度区域（样本点分布稠密的区域）划分为簇，将簇看作样本空间中被稀疏区域（or 噪声）分隔开的稠密区域；<br>    优点：<br>        基于样本分布的稠密程度进行聚类，可以克服基于划分&#x2F;层次的聚类方法通常智能用于挖掘球状簇的问题；<br>    DBSCAN算法<br>        ？<br>    OPTICS算法<br>        优点：改进了DBSCAN，降低了其对于输入参数的敏感程度；<br>    DENCLUE算法<br>        优点：综合了基于划分、层次、密度的方法；</p>
<p>聚类方法四：基于网格聚类（了解）</p>
<p>聚类方法五：基于模型聚类（了解）</p>
<p><code>聚类模型前三个比较重要（划分、层次、密度），k-means很重要，CF-tree很重要（构建CF-tree，更新CF-tree），BIRTCH方法很重要、CURE算法知道即可、基于密度聚类很重要</code>   </p>
<h2 id="ch10：神经网络模型"><a href="#ch10：神经网络模型" class="headerlink" title="ch10：神经网络模型"></a>ch10：神经网络模型</h2><p><code>前馈神经网络是我们学习的重点，神经网络的结构，怎样使用</code>；<br><code>前向过程，评价指标：回归（最小二乘法）/分类（交叉熵）</code></p>
<p>神经网络：<br>    人工神经网络的思想：<br>        由简单神经元经过相互连接形成网络结构，通过调节各个连接的权重值来改变连接的强度，进而实现感知和判断；<br>    基本分类：<br>        1）前馈神经网络；<br>        2）反馈神经网络；<br>        3）自组织神经网络；<br>    基本概念：<br>        1）激活函数；<br>        2）损失函数；<br>        3）学习率；<br>        etc.</p>
<p>前馈神经网络：<br>    含义：一种单向多层的网络结构；<br>    （信息层开始，逐层向一个方向传递，直到输出层结束）；<br>    特点：<br>        1）前馈过程中不会调整各层的权重参数；<br>        2）反向传播的时候会将误差向后传递调整权重；<br>            eg：BP算法反向传播，理论上可以逼近任何连续函数；<br>    感知器：一种最简单的前馈神经网络；<br>        构成：<br>            1）可以接收n个输入（n维特征向量）；<br>            2）对应有n个权重值$\omega_i$；<br>            3）偏置项阈值b；（防止溢出？）；<br>            4）激活函数f；<br>            （整个过程就是n个输入根据n个权重，加权求和，最后得到一个值，将其输入激活函数，得到输出）<br>            eg：$y &#x3D; f(x\omega^T+b)$；<br>        作用：<br>            1）用于作为神经元，构造更加复杂的神经网络；<br>            2）如果直接使用可以理解为直线划分；<br>            3）单个感知器甚至不能解决最简单的非线性问题（异或问题）；<br>    BP神经网络<br>        含义；一种具体的前馈神经网络，但是权重值只由反向传播学习算法进行调整；<br>        拓扑结构组成：<br>            1）输入层；<br>            2）隐层；<br>            3）输出层；<br>        激活函数：<br>            1）必须满足处处可导的条件；<br>            eg：sigmoid函数：<br>                1）连续可微，单调递增；<br>                2）输出值在0～1之间；<br>                3）$\sigma(x) &#x3D; \frac{1}{1+e^{-x}}$；<br>            2）可能的问题：梯度消失问题；<br>        eg：<img src="/Intro-of-AI/p13.png" alt="BP神经网络的结构"><br>        BP神经网络训练流程：<br>            1）初始化网络权值和神经网络阈值；<br>            2）前向传播<br>            ？？？</p>
<h2 id="ch11：深度学习"><a href="#ch11：深度学习" class="headerlink" title="ch11：深度学习"></a>ch11：深度学习</h2><p><code>什么是卷积，怎么卷（运算），什么是循环，怎么做</code></p>
<p>深度学习与前馈神经网络的区别：<br>    1）传统的BP算法仅有几层网络，需要<strong>手工指定特征</strong>且易出现<strong>局部最优问题</strong>，而深度学习引入了<strong>概率生成模型</strong>，可<strong>自动地从训练集提取特征</strong>，解决了手工特征考虑不周的问题，而且<strong>初始化了神经网络权重</strong>，采用反向传播算法进行训练，与BP算法相比取得了很好的效果。<br>    2）特征工程与特征学习：<br>        1）特征工程由人类专家根据现实任务来设计, <strong>特征提取与识别</strong>是分开的两个阶段（传统）；<br>        2）特征学习通过深度学习自动产生有益于分类的特征, 是一个<strong>端到端</strong>的学习框架（深度学习）；</p>
<p>深度学习基本思想与优缺点<br>    idea来源：<br>        1）多层前馈网络有强大的表示能力（”万有逼近性”）；<br>        2）仅需<strong>一个包含足够多神经元的隐层</strong>, 多层前馈神经网络就能以任意精度逼近任意复杂度的连续函数 [Horniket al., 1989]；<br>    于是增加模型复杂度有两种思路：<br>        1）模型宽度：增加隐层神经元的数目；<br>        2）模型深度：增加隐层数目；<br>    深度学习使用的是方法2），而非方法1），原因是？<br>        老师说：层数的增加可以有更多的抽象，层级式的抽象可以升维，带来更好的表示的方式；<br>        deepseek：参数效率与表示效率，这是最核心的理论原因。深度网络可以用指数级更少的神经元（参数）来表示一个复杂函数，而一个浅层网络可能需要指数级更多的神经元。<br>    复杂模型的挑战：<br>        1）梯度消失的问题：深度网络难以直接用经典算法（例如BP算法）进行训练, 因为误差在多隐层内传播时会出现梯度消失问题（即梯度迅速为0），难以收敛到稳定状态<br>    （<strong>这些是理论无法解决的问题，需要trick</strong>）<br>    训练技巧（trick）：<br>        1）<strong>预训练+微调</strong>：预训练阶段，每次训练时将上<strong>一层隐层结点的输出作为输入, 本层隐结点的输出作为输出，仅训练一层网络</strong>；预训练全部完成后，对整个网络进行<strong>微调训练</strong>，一般采用BP算法；<br>        2）<strong>新型激活函数（LeakyReLU）</strong>：求导更容易，缓解梯度消失；<br>        3）<strong>Dropout</strong>：当训练一个深度神经网络时，我们可以随机丢弃一部分神经元以及其对应的连接边；<br>    基本思想：<br>        idea1）+思路2）<br>    缺点：<br>        挑战<br>    优点：<br>        idea 1）</p>
<p>卷积神经网络的基本思想：<br>    1）启发：卷积神经网络是人工神经网络的一种，由对猫的视觉皮层的研究发展而来，视觉皮层细胞对视觉子空间更敏感，<strong>通过子空间的平铺扫描实现对整个视觉空间的感知。</strong>；</p>
<p>卷积神经网络特点：<br>    1）优势在于具有共享权值的网络结构和局部感知（也称为稀疏连接）的特点，能够降低神经网络的运算复杂度。</p>
<p>卷积神经网络的结构：<br>    1）输入：卷积层和子采样器提取得到的特征；<br>    2）低层：子采样器和卷积层交替得到的；<br>    3）更高层：全连接层；<br>    4）输出：可以是分类器 or 逻辑回归；</p>
<p>卷积层：<br>    1）输入；<br>    2）<strong>卷积核</strong>（如何运算）；<br>    3）步长；<br>    4）确定输出维度；<br>    （<strong>注意相关参数影响，输入维度、步长</strong>）<br>eg：<strong>卷积核</strong><br>输入7,7 -&gt; 输出3,3（<strong>要会算这个例子</strong>）<br>1 2 3 4 5 6 7<br>1 2 3<br>    1 2 3<br>        1 2 3<br><img src="/Intro-of-AI/p14.png" alt="卷积层运算过程"></p>
<p>RNN的基本思想（循环神经网络）<br>    思想：<br>        1）循环神经网络是一种用于处理序列数据的神经网络结构，其基本原理在于网络中存在循环连接，使得网络具有记忆能力，能够捕捉时间序列中的依赖关系。<br>        2）RNN不同于前向神经网络，它的层内、层与层之间的信息可以双向传递，更高效地存储信息，利用更复杂的方法来更新规则，通常用于处理信息序列的任务。</p>
<p>RNN的基本结构<br>    1）RNN主要用来处理序列数据，在传统的神经网络模型中，是从输入层到隐含层再到输出层，每层内的节点之间无连接，<strong>循环神经网络中一个当前神经元的输出与前面的输出也有关</strong>，网络会对前面的信息进行记忆并应用于当前神经元的计算中，<strong>隐藏层之间的节点是有连接的</strong>，并且<strong>隐藏层的输入不仅包含上层的输出还包含上一时刻隐藏层的输出</strong>。<br>    2）公式表示：<br>        $h_t &#x3D; f_w(h_{t-1},x_t)$；<br>        含义：<br>            1）$h_t$：新的目标状态；<br>            2）$h_{t-1}$：前一目标状态；<br>            3）$x_t$：当前输入向量；<br>            4）$f_w$：权重参数函数；<br>    3）图像表示：<br>        <img src="/Intro-of-AI/p15.png" alt="RNN的结构">；</p>
<p>RNN的每次执行会作为下次执行输入的一部分：<br>    一个RNN可认为是同一网络的多次重复执行，每一次执行的结果是下一次执行的输入。<br>    即输入为：$x_t$以及$W_{s_{t-1}}$<br>    <img src="/Intro-of-AI/p16.png" alt="隐层连接展开">；</p>
<p>RNN的特点是循环连接和共享参数：<br>    1）循环连接就是隐层节点之间的连接；<br>    2）共享参数是指无论输入序列有多长，使用的都是同一套结构的权重；<br>        W_hh: 连接上一个隐状态 (h_{t-1}) 到 当前隐状态 (h_t) 的权重。<br>        W_xh: 连接当前输入 (x_t) 到 当前隐状态 (h_t) 的权重。<br>        W_hy: 连接当前隐状态 (h_t) 到 当前输出 (o_t) 的权重（如果有输出的话）。</p>
<p>由于上述特点RNN通常用于处理<strong>序列</strong>式的任务，在图像识别、自然语言处理很好；</p>
<h2 id="ch12：强化学习"><a href="#ch12：强化学习" class="headerlink" title="ch12：强化学习"></a>ch12：强化学习</h2><p><code>最后一节课会划重点</code></p>

		</div>

		<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC80NjIyNC8yMjczNQ==">
	<script type="text/javascript">
   (function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
	</script>
<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->
		
	</article>

	<div id="toc">
		
	</div>

</div>

<!-- <div id="paginator"> -->
<!-- 	 -->
<!-- </div> -->


			</div>
		</div>

		<div id="bottom-outer">
			<div id="bottom-inner">
				Site by 阳生 | 
				Powered by <a target="_blank" rel="noopener" href="http://hexo.io">Hexo</a> |
				theme <a target="_blank" rel="noopener" href="https://github.com/fireworks99/hexo-theme-PreciousJoy">PreciousJoy</a>
			</div>
		</div>

		
	</div>





	
	<!-- scripts list from theme config.yml -->
	
	<script src="/js/jquery-3.5.1.min.js"></script>
	
	<script src="/js/PreciousJoy.js"></script>
	
	<script src="/js/highlight.pack.js"></script>
	
	<script src="/js/jquery.fancybox.min.js"></script>
	
	<script src="/js/search.js"></script>
	
	<script src="/js/load.js"></script>
	
	<script src="/js/jquery.mCustomScrollbar.concat.min.js"></script>
	
	<script src="/js/clipboard.min.js"></script>
	
	

	<script>hljs.initHighlightingOnLoad();</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
